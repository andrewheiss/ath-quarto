---
title: "The ultimate practical guide to multilevel multinomial conjoint analysis with R"
date: 2023-08-12
description: "Learn how to use R, {brms}, {marginaleffects}, and {tidybayes} to analyze discrete choice conjoint data with fully specified hierarchical multilevel multinomial models"
image: index_files/figure-html/plot-amces-minivans-carpool-1.png
twitter-card:
  image: "index_files/figure-html/plot-amces-minivans-carpool-1.png"
open-graph:
  image: "index_files/figure-html/plot-amces-minivans-carpool-1.png"
categories:
  - r
  - tidyverse
  - ggplot
  - statistics
  - brms
  - stan
toc-depth: 4
format:
  html:
    fig-cap-location: bottom
bibliography: references.json
resources:
  - "img/parks-rec-ols.mp4"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.height = 6 * 0.618,
  fig.retina = 3,
  dev = "ragg_png",
  fig.align = "center",
  out.width = "90%",
  collapse = TRUE,
  cache.extra = 1234  # Change number to invalidate cache
)

options(
  digits = 4,
  width = 300,
  dplyr.summarise.inform = FALSE
)

set.seed(1234)
```

```{r tikz-stuff, include=FALSE}
# Necessary for using dvisvgm on macOS
# See https://www.andrewheiss.com/blog/2021/08/27/tikz-knitr-html-svg-fun/
Sys.setenv(LIBGS = "/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53")

font_opts <- list(
  extra.preamble = c(
    "\\usepackage{libertine}",
    "\\usepackage{libertinust1math}"
  ),
  dvisvgm.opts = "--font-format=woff"
)
```

I recently [posted a guide](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/) (mostly for future-me) about how to analyze conjoint survey data with R. I explore two different estimands that social scientists are interested in—causal average marginal component effects (AMCEs) and descriptive marginal means—and show how to find them with R, with both frequentist and Bayesian approaches.

However, that post is a little wrong. It's not *wrong* wrong, but it is a bit oversimplified.

When political scientists, psychologists, economists, and other social scientists analyze conjoint data, they overwhelmingly do it with [ordinary least squares (OLS) regression](https://en.wikipedia.org/wiki/Ordinary_least_squares), or just standard linear regression (`lm(y ~ x)` in R; `reg y x` in Stata). Even if the outcome is binary, they'll use OLS and call it a linear probability model. The main R package for working with conjoint data in a frequentist way ([{cregg}](https://thomasleeper.com/cregg/)) uses OLS and linear probability models. Social scientists (and economists in particular) adore OLS.

```{=html}
<figure>
<video controls loop width="90%" style="display: block; margin: auto;">
  <source src="img/parks-rec-ols.mp4" type="video/mp4">
</video>
<figcaption>Video by <a href="https://twitter.com/theotheredmund/status/1349453230762196992">Edmund Helmer</a></figcaption>
</figure>
```

In my earlier guide, [I showed how to analyze the data with logistic regression](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/#the-overall-model), but even that is still overly simplified. In reality, conjoint choice-based experiments are more complex than what regular old OLS regression—or even logistic regression—can handle (though I'm sure some econometrician somewhere has a proof showing that OLS works just fine for multinomial conjoint data :shrug:).

A recent paper published in *Political Analysis* [@JensenMarbleScheve:2021] does an excellent job explaining the problem with using plain old OLS to estimate AMCEs and marginal means with conjoint data ([access the preprint here](https://williammarble.co/docs/CityLimits-April2020.pdf)). Their main argument boils down to this: OLS throws away too much useful information about (1) the relationships and covariance between the different combinations of feature levels offered to respondents, and (2) individual-specific differences in how respondents react to different feature levels.

Jensen et al. explain three different approaches to analyzing data that has a natural hierarchical structure like conjoint data (where lots of choices related to different "products" are nested within individuals). This also is the same argument from [chapter 15 of *Bayes Rules!*](https://www.bayesrulesbook.com/chapter-15).

1. **Pooled data** (["no room for individuality"](https://www.bayesrulesbook.com/chapter-15#ch15-complete)): The easiest way to deal with individual-specific effects is to not to. Lump each choice by each respondent into one big dataset, run OLS on it, and be done. Believe it or not, linear regression right away.

   However, this erases all individual-level heterogeneity and details about how different feature levels interact with individual characteristics.

2. **No pooling** (["every person for themselves"](https://www.bayesrulesbook.com/chapter-15#no-pooling)): An alternative way to deal with individual-specific effects is to run a separate model for each individual. If 800 people participated in the experiment, run 800 different models. That way you can perfectly model the relationship between each individual's characteristics (political party, age, income, education, etc.) with their choices and preferences. 

   This sounds neat, but has substantial issues. It'll only show an identified causal effect if each respondent saw every combination of conjoint features at least once. In the candidate experiment [from my previous post](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/), there were 8 features with different counts of attributes: 2 × 6 × 6 × 6 × 2 × 6 × 6 × 6 = 186,624 possibilities. Every respondent would need to see each of the nearly 200,000 options. lol.

3. **Partial pooling** (["a welcome middle ground"](https://www.bayesrulesbook.com/chapter-15#partial-pooling-with-hierarchical-models)): Jensen et al.'s solution is to use hierarchical models that allow individual-level characteristics to inform choice-level decisions. As *Bayes Rules!* says:

   > [T]hough each group is *unique*, having been sampled from the same population, all groups are connected and thus might contain valuable information about one another.
   
   In this kind of model, we want individual-level characteristics like age, income, education, etc. to inform the decision to choose specific outcomes and interact with different feature levels in different ways. Not every respondent needs to have seen all 200,000 options—information about respondents with similar characteristics facing similar sets of choices gets shared because of the hierarchical-ness of the model.

::: {.callout-tip}
## Best overview of multilevel models

For the best overviews I've found of how multilevel models work, check out these two resources:

- Chapters 15–17 in [*Bayes Rules!*](https://www.bayesrulesbook.com/) ([especially chapter 17](https://www.bayesrulesbook.com/chapter-17))
- Michael Clark's [*Mixed Models with R* guide](https://m-clark.github.io/mixed-models-with-R/)

[I also have a guide here](https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/), but it's nowhere near as good as those ↑
:::

By using a multilevel hierarchical model, @JensenMarbleScheve:2021 show that we can still find AMCEs and causal effects, just like in my previous guide, but we can take advantage of the far richer heterogeneity that we get from these complex statements. We can make cool statements like this (in an experiment that varied policies related to unions):

> On average, Democrats are $x$ percentage points more likely than demographically similar Republicans to support a plan that includes expanding union power, relative to the status quo.

Using hierarchical models for conjoint experiments in political science is new and exciting and revolutionary and neat. That's the whole point of Jensen et al.'s paper—it's a call to stop using OLS for everything.

I've been working on a conjoint experiment with my coauthors [Marc Dotson](https://marriott.byu.edu/directory/details?id=50683) and [Suparna Chaudhry](https://www.suparnachaudhry.com/). Suparna and I are political scientists and this multilevel stuff in general is still relatively new and wildly underused in the discipline. Marc, though, is a marketing scholar. The marketing world has been using hierarchical models for conjoint experiments for a long time and it's standard practice in that discipline. There's a whole textbook about the hierarchical model approach in marketing [@ChapmanFeit:2019], and these fancy conjoint multilevel models are used widely throughout the marketing industry.

lol at political science, just now discovering this.

---

So, I need to expand [my previous conjoint guide](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/). That's what this post is for.

::: {.callout-note}

## Who this post is for

Here's what I assume you know:

- You're familiar with [R](https://www.r-project.org/) and the [tidyverse](https://www.tidyverse.org/) (particularly [{dplyr}](https://dplyr.tidyverse.org/) and [{ggplot2}](https://ggplot2.tidyverse.org/)).
- You're familiar with linear regression and packages like [{broom}](https://broom.tidymodels.org/) for converting regression results into tidy data frames and [{marginaleffects}](https://vincentarelbundock.github.io/marginaleffects/) for calculating [marginal effects](https://www.andrewheiss.com/blog/2022/05/20/marginalia/).
- You're familiar with [{brms}](https://paul-buerkner.github.io/brms/) for running Bayesian regression models and [{tidybayes}](https://mjskay.github.io/tidybayes/) and [{ggdist}](https://mjskay.github.io/ggdist/) for manipulating and plotting posterior draws. You *don't* need to know how to write stuff in raw Stan.

:::

I'll do three things in this guide:

1. **Chocolate**: Analyze a simple choice-based conjoint experiment where respondents only saw one set of options. This is so I can (1) explore the {mlogit} package, and (2) figure out how to work with multinomial models and predictions both frequentistly and Bayesianly.
2. **Minivans**: Analyze a more complex choice-based conjoint experiment where respondents saw three randomly selected options fifteen times. I do this with both {mlogit} and {brms} to figure out how to work with true multinomial outcomes both frequentistly and Bayesianly.
3. **Minivans again**: Analyze the same complex choice-based experiment but incorporate respondent-level characteristics into the estimates using a hierarchical or multilevel model. I only do this with {brms} because in reality I have no interest in working with {mlogit} (I only use it here as a sort of baseline for my {brms} explorations).

Throughout this example, I'll use data from two different simulated conjoint choice experiments. You can download these files and follow along:

- [{{< fa table >}} `choco_candy.csv`](data/choco_candy.csv): This is simulated data for a hypothetical experiment about consumer preferences for candy features. It comes from p. 292 in @Kuhfeld:2010, a [SAS technical note](http://support.sas.com/techsup/technote/mr2010f.pdf) about how to run multinomial models in SAS. Instead of copying/pasting from the PDF, [I found a version of it here](https://github.com/sangwoo-statistics/dataset), cleaned up by Sangwoo Kim (who also has [a YouTube tutorial](https://www.youtube.com/watch?v=ra8Y2FjRqOE) using the same candy data)
- [{{< fa table >}} `rintro-chapter13conjoint.csv`](data/choco_candy.csv): This is simultated data for a hypothetical experiment about consumer preferences for minivan features. It comes from chapter 13 in @ChapmanFeit:2019 and is available from the [book's resources page](http://r-marketing.r-forge.r-project.org/) (or [directly from their data folder](http://r-marketing.r-forge.r-project.org/data/))

Additionally, in [Part 3](#part-3-minivans-repeated-questions-full-hierarchical-multinomial-logit), I fit a huge Stan model with {brms} that takes ≈30 minutes to run on my fast laptop. If you want to follow along and not melt your CPU for half an hour, you can download an .rds file of that fitted model that I stuck in [an OSF project](https://osf.io/3y7es/). The code for `brm()` later in this guide will load the .rds file automatically instead of rerunning the model as long as you put it in a folder named "models" in your working directory. This code uses the [{osfr} package](https://docs.ropensci.org/osfr/) to download [the .rds file from OSF](https://osf.io/zp6eh) automatically and places it where it needs to go:

```{r get-rds-osf, eval=FALSE}
library(osfr)  # Interact with OSF via R

# Make a "models" folder if it doesn't exist already
if (!file.exists("models")) { dir.create("models") }

# Download model_minivans_mega_mlm_brms.rds from OSF
osf_retrieve_file("https://osf.io/zp6eh") |>
  osf_download(path = "models", conflicts = "overwrite", progress = TRUE)
```


Let's load some libraries, create some helper functions, load the data, and get started!

```{r libraries, warning=FALSE, message=FALSE}
library(tidyverse)        # ggplot, dplyr, and friends
library(broom)            # Convert model objects to tidy data frames
library(parameters)       # Show model results as nice tables
library(survey)           # Panel-ish frequentist regression models
library(mlogit)           # Frequentist multinomial regression models
library(dfidx)            # Structure data for {mlogit} models
library(scales)           # Nicer labeling functions
library(marginaleffects)  # Calculate marginal effects
library(ggforce)          # For facet_col()
library(brms)             # The best formula-based interface to Stan
library(tidybayes)        # Manipulate Stan results in tidy ways
library(ggdist)           # Fancy distribution plots
library(patchwork)        # Combine ggplot plots
library(rcartocolor)      # Color palettes from CARTOColors (https://carto.com/carto-colors/)

# Custom ggplot theme to make pretty plots
# Get the font at https://github.com/intel/clear-sans
theme_nice <- function() {
  theme_minimal(base_family = "Clear Sans") +
    theme(panel.grid.minor = element_blank(),
          plot.title = element_text(family = "Clear Sans", face = "bold"),
          axis.title.x = element_text(hjust = 0),
          axis.title.y = element_text(hjust = 1),
          strip.text = element_text(family = "Clear Sans", face = "bold",
                                    size = rel(0.75), hjust = 0),
          strip.background = element_rect(fill = "grey90", color = NA))
}

theme_set(theme_nice())

clrs <- carto_pal(name = "Prism")

# Functions for formatting things as percentage points
label_pp <- label_number(accuracy = 1, scale = 100, 
                         suffix = " pp.", style_negative = "minus")

label_amce <- label_number(accuracy = 0.1, scale = 100, suffix = " pp.", 
                           style_negative = "minus", style_positive = "plus")
```

```{r load-clean-data, warning=FALSE, message=FALSE}
chocolate <- read_csv("data/choco_candy.csv") %>% 
  mutate(
    dark = case_match(dark, 0 ~ "Milk", 1 ~ "Dark"),
    dark = factor(dark, levels = c("Milk", "Dark")),
    soft = case_match(soft, 0 ~ "Chewy", 1 ~ "Soft"),
    soft = factor(soft, levels = c("Chewy", "Soft")),
    nuts = case_match(nuts, 0 ~ "No nuts", 1 ~ "Nuts"),
    nuts = factor(nuts, levels = c("No nuts", "Nuts"))
  )

minivans <- read_csv("data/rintro-chapter13conjoint.csv") %>% 
  mutate(
    across(c(seat, cargo, price), factor),
    carpool = factor(carpool, levels = c("no", "yes")),
    eng = factor(eng, levels = c("gas", "hyb", "elec"))
  )
```


\ 

# Part 1: Candy; single question; basic multinomial logit

## The setup

In this experiment, respondents are asked to choose which of these kinds of candies they'd want to buy. Respondents only see this question one time and all possible options are presented simultaneously.

:::: {.callout-tip}

### Example survey question

|           |                A                |                B                |                C                |                D                |                E                |                F                |                G                |                H                |
|--------|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
| Chocolate |              Milk               |              Milk               |              Milk               |              Milk               |              Dark               |              Dark               |              Dark               |              Dark               |
| Center    |              Chewy              |              Chewy              |              Soft               |              Soft               |              Chewy              |              Chewy              |              Soft               |              Soft               |
| Nuts      |             No nuts             |              Nuts               |             No nuts             |              Nuts               |             No nuts             |              Nuts               |             No nuts             |              Nuts               |
| Choice    | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> | <input type="radio" name="ex1"> |

::::

## The data

The data for this kind of experiment looks like this, with one row for each possible alternative (so eight rows per person, or `subj`), with the alternative that was selected marked as 1 in `choice`. Here, Subject 1 chose option E (dark, chewy, no nuts). There were 10 respondents, with 8 rows each, so there are 10 × 8 = 80 rows.

```{r show-chocolate}
chocolate
```


## The model

Respondents were shown eight different options and asked to select one. While this seems like a binary yes/no choice that could work with just regular plain old logistic regression, we want to account for the features and levels in all the unchosen categories too. To do this, we can use [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), where the outcome variable is an unordered categorical variable with more than two categories. In this case we have eight different possible outcomes: alternatives A through H.


### Original SAS model as a baseline

::: {.callout-warning}
#### lol SAS

I know nothing about SAS. I have never opened SAS in my life. It is a mystery to me.

I copied these results directly from p. 297 in SAS's massive ["Discrete Choice" technical note](http://support.sas.com/techsup/technote/mr2010f.pdf) [@Kuhfeld:2010].

I only have this SAS output here as a baseline reference for what the actual correct coefficients are supposed to be.
:::

SAS apparently fits these models with proportional hazard survival-style models, which feels weird, but there's probably a mathematical or statistical reason for it. You use PROC PHREG to do it:

```default
proc phreg data=chocs outest=betas;
   strata subj set;
   model c*c(2) = dark soft nuts / ties=breslow;
   run;
```

It gives these results:

```default
                   Choice of Chocolate Candies

                       The PHREG Procedure

              Multinomial Logit Parameter Estimates
              
                      Parameter     Standard
                 DF    Estimate        Error  Chi-Square   Pr > ChiSq
Dark Chocolate   1      1.38629      0.79057      3.0749       0.0795
Soft Center      1     -2.19722      1.05409      4.3450       0.0371
With Nuts        1      0.84730      0.69007      1.5076       0.2195
```

### Survival model

Ew, enough SAS. Let's do this with R instead.

We can recreate the same proportional hazards model with `coxph()` from the {survival} package. Again, this feels weird and not like an intended purpose of survival models and not like multinomial logit at all—in my mind it is neither (1) multinomial nor (2) logit, but whatever. People far smarter than me invented these things, so I'll just trust them.

```{r model-chocolate-survival, message=FALSE}
model_chocolate_survival <- coxph(
  Surv(subj, choice) ~ dark + soft + nuts, 
  data = chocolate, 
  ties = "breslow"  # This is what SAS uses
)

model_parameters(model_chocolate_survival, digits = 4, p_digits = 4)
```

The coefficients, standard errors, and p-values are identical to the SAS output! The only difference is the statistic: in SAS they use a chi-square statistic, while `survival:coxph()` uses a z statistic. There's probably a way to make `coxph()` use a chi-square statistic, but I don't care about that. I never use survival models and I'm only doing this to replicate the SAS output and it just doesn't matter.

### Poisson model

An alternative way to fit a multinomial logit model without resorting to survival models is to actually (mis?)use another model family. We can use a Poisson model, even though `choice` isn't technically count data, because of obscure stats reasons. [See here](https://online.stat.psu.edu/stat504/lesson/2/2.3/2.3.6) for an illustration of the relationship between multinomial and Poisson distributions; or [see this 2011 *Biometrika* paper](https://doi.org/10.1093/biomet/asr026) about using Poisson models to reduce bias in multinomial logit models. Richard McElreath has a subsection about this in *Statistical Rethinking* as well: "Multinomial in disguise as Poisson" (11.3.3). Or [as he said over on the currently-walled-garden Bluesky](https://bsky.app/profile/rmcelreath.bsky.social/post/3k4e5s6zbxc2j), "All count distributions are just one or more Poisson distributions in a trench coat." 

To account for the repeated subjects in the data, we'll use `svyglm()` from the {survey} package so that the standard errors are more accurate.

```{r model-chocolate-poisson, message=FALSE}
model_chocolate_poisson <- glm(
  choice ~ dark + soft + nuts, 
  data = chocolate, 
  family = poisson()
)

model_parameters(model_chocolate_poisson, digits = 4, p_digits = 4)
```

Lovely—the results are the same.

### `mlogit` model

Finally, we can use the {mlogit} package to fit the model. Before using `mlogit()`, we need to transform our data a bit to specify which column represents the choice (`choice)` and how the data is indexed: subjects (`subj`) with repeated alternatives (`alt`).

```{r make-dfidx-chocolate}
chocolate_idx <- dfidx(
  chocolate,
  idx = list("subj", "alt"),
  choice = "choice",
  shape = "long"
)
```

We can then use this indexed data frame with `mlogit()`, which uses the familiar R formula interface, but with some extra features separated by `|`s

```default
outcome ~ features | individual-level variables | alternative-level variables
```

If we had columns related to individual-level characteristics or alternative-level characteristics, we could include those in the model—and we'll do precisely that later in this post. (Incorporating individual-level covariates is the whole point of this post!)

Let's fit the model:

```{r model-chocolate-mlogit, message=FALSE}
model_chocolate_mlogit <- mlogit(
  choice ~ dark + soft + nuts | 0 | 0, 
  data = chocolate_idx
)

model_parameters(model_chocolate_mlogit, digits = 4, p_digits = 4)
```

Delightful. All the results are the same as the survival model and the Poisson model.

### Bayesian model

We can also fit this model in a Bayesian way using {brms}. Stan has a categorical distribution family for multinomial models, and we'll use it in the next example. For now, for the sake of simplicity, we'll use a Poisson family, since, as we saw above, that's a legal way of parameterizing multinomial distributions.

The data has a natural hierarchical structure to it, with 8 choices (for alternatives A through H) nested inside each of the 10 subjects.

```{tikz chocolate-multilevel-structure, engine.opts=font_opts}
#| echo: false
#| fig-cap: "Multilevel experimental structure, with candy choices $y_{\\text{A}\\dots\\text{H}}$ nested in subjects"
#| fig-align: center
#| fig-ext: svg
#| out-width: 100%
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\begin{tikzpicture}[{every node/.append style}=draw]
  \node [rectangle] (respondent1) at (0, 2.5) {Subject 1};
  \node [ellipse] (y11) at (-1.5, 1) {$y_{\text{A}_1}$};
  \node [draw=none] (y21) at (0, 1) {$\dots$};
  \node [ellipse] (y31) at (1.5, 1) {$y_{\text{H}_1}$};
  \draw [-latex] (respondent1) to (y11);
  \draw [-latex] (respondent1) to (y21);
  \draw [-latex] (respondent1) to (y31);

  \node [rectangle] (respondent2) at (4.5, 2.5) {Subject 2};
  \node [ellipse] (y12) at (3, 1) {$y_{\text{A}_2}$};
  \node [draw=none] (y22) at (4.5, 1) {$\dots$};
  \node [ellipse] (y32) at (6, 1) {$y_{\text{H}_2}$};
  \draw [-latex] (respondent2) to (y12);
  \draw [-latex] (respondent2) to (y22);
  \draw [-latex] (respondent2) to (y32);

  \node [ellipse, draw=white] (dots_top) at (7.25, 2.5) {$\dots$};
  \node [ellipse, draw=white] (dots_bottom) at (7.25, 1) {$\dots$};
	\draw [-latex] (dots_top) to (dots_bottom);

  \node [rectangle] (respondentn) at (10, 2.5) {Subject 10};
  \node [ellipse] (y1n) at (8.5, 1) {$y_{\text{A}_{10}}$};
  \node [draw=none] (y2n) at (10, 1) {$\dots$};
  \node [ellipse] (y3n) at (11.5, 1) {$y_{\text{H}_{10}}$};
  \draw [-latex] (respondentn) to (y1n);
  \draw [-latex] (respondentn) to (y2n);
  \draw [-latex] (respondentn) to (y3n);

  \node [rectangle] (population) at (5, 4) {Population of experiment subjects};
  \draw [-latex] (population) to (respondent1);
  \draw [-latex] (population) to (respondent2);
  \draw [-latex] (population) to (dots_top);
  \draw [-latex] (population) to (respondentn);
\end{tikzpicture}
```

We want to model candy choice (`choice`) based on candy characteristics (`dark`, `soft`, and `nuts`). We'll use the subscript $i$ to refer to individual candy choices and $j$ to refer to subjects. 

Since we can legally pretend that this multinomial selection process is actually Poisson, we'll model it as a Poisson process that has a rate of $\lambda_{i_j}$. We'll model that $\lambda_{i_j}$ with a log-linked regression model with covariates for each of the levels of each candy feature. To account for the multilevel structure, we'll include subject-specific offsets ($b_{0_j}$) from the global average, thus creating random intercepts. We'll specify fairly wide priors just because.

Here's the formal model for all this:

$$
\begin{aligned}
&\ \textbf{Probability of selection of alternative}_i \textbf{ in subject}_j \\
\text{Choice}_{i_j} \sim&\ \operatorname{Poisson}(\lambda_{i_j}) \\[10pt]
&\ \textbf{Model for probability of each option} \\
\log(\lambda_{i_j}) =&\ (\beta_0 + b_{0_j}) + \beta_1 \text{Dark}_{i_j} + \beta_2 \text{Soft}_{i_j} + \beta_3 \text{Nuts}_{i_j} \\[5pt]
b_{0_j} \sim&\ \mathcal{N}(0, \sigma_0) \qquad\qquad\quad \text{Subject-specific offsets from global choice probability} \\[10pt]
&\ \textbf{Priors} \\
\beta_0 \sim&\ \mathcal{N}(0, 3) \qquad\qquad\quad\ \ \text{Prior for global average choice probability} \\
\beta_1, \beta_2, \beta_3 \sim&\ \mathcal{N}(0, 3) \qquad\qquad\quad\ \ \text{Prior for candy feature levels} \\
\sigma_0 \sim&\ \operatorname{Exponential}(1) \qquad \text{Prior for between-subject variability}
\end{aligned}
$$

And here's the {brms} model:

```{r model-chocolate-brms}
model_chocolate_brms <- brm(
  bf(choice ~ dark + soft + nuts + (1 | subj)),
  data = chocolate,
  family = poisson(),
  prior = c(
    prior(normal(0, 3), class = Intercept),
    prior(normal(0, 3), class = b),
    prior(exponential(1), class = sd)
  ),
  chains = 4, cores = 4, iter = 2000, seed = 1234,
  backend = "cmdstanr", threads = threading(2), refresh = 0,
  file = "models/model_chocolate_brms"
)
```

The results are roughly the same as what we found with all the other models—they're slightly off because of random MCMC sampling.

```{r show-model-chocolate-brms, warning=FALSE, message=FALSE}
model_parameters(model_chocolate_brms)
```

## Predictions

In the SAS technical note example, they use the model to generated predicted probabilities of the choice of each of the options. In the world of marketing, this can also be seen as the predicted market share for each option. To do this, they plug each of the eight different different combinations of dark, soft, and nuts into the model and calculate the predicted output on the response (i.e. probability) scale. They get these results, where dark, chewy, and nuts is the most likely and popular option (commanding a 50% market share). 

```default
      Choice of Chocolate Candies

Obs    Dark    Soft     Nuts       p

  1    Dark    Chewy    Nuts       0.50400
  2    Dark    Chewy    No Nuts    0.21600
  3    Milk    Chewy    Nuts       0.12600
  4    Dark    Soft     Nuts       0.05600
  5    Milk    Chewy    No Nuts    0.05400
  6    Dark    Soft     No Nuts    0.02400
  7    Milk    Soft     Nuts       0.01400
  8    Milk    Soft     No Nuts    0.00600
```

We can do the same thing with R. 

### Frequentist predictions

{mlogit} model objects have predicted values stored in one of their slots (`model_chocolate_mlogit$probabilities`), but they're in a weird non-tidy matrix form and I like working with tidy data. I'm also a huge fan of [the {marginaleffects} package](https://vincentarelbundock.github.io/marginaleffects/), which provides a consistent way to calculate predictions, comparisons, and slopes/marginal effects (with `predictions()`, `comparisons()`, and `slopes()`) for dozens of kinds of models, including `mlogit()` models. 

So instead of wrangling the built-in `mlogit()` probabilities, we'll generate predictions by feeding the model the unique combinations of `dark`, `soft`, and `nuts` to `marginaleffects::predictions()`, which will provide us with probability- or proportion-scale predictions:

```{r pred-model-chocolate-mlogit}
preds_chocolate_mlogit <- predictions(
  model_chocolate_mlogit, 
  newdata = datagrid(dark = unique, soft = unique, nuts = unique)
)

preds_chocolate_mlogit %>% 
  # predictions() hides a bunch of columns; forcing it to be a tibble unhides them
  as_tibble() %>% 
  arrange(desc(estimate)) %>% 
  select(group, dark, soft, nuts, estimate, std.error, statistic, p.value)
```

Perfect! They're identical to the SAS output. 

We can play around with these predictions to describe the overall market for candy. Chewy candies dominate the market…

```{r preds-chocolate-dark-only}
preds_chocolate_mlogit %>% 
  group_by(dark) %>% 
  summarize(share = sum(estimate))
```

…and dark chewy candies are by far the most popular:

```{r preds-chocolate-dark-soft}
preds_chocolate_mlogit %>% 
  group_by(dark, soft) %>% 
  summarize(share = sum(estimate))
```


### Bayesian predictions

{marginaleffects} supports {brms} models too, so we can basically run the same `predictions()` function to generate predictions for our Bayesian model. Magical.

::: {.callout-important}
#### lol subject offsets

When plugging values into `predictions()` (or `avg_slopes()` or any function that calculates predictions from a model), we have to decide how to handle the random subject offsets ($b_{0_j}$). [I have a whole other blog post guide about this](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/#summary) and how absolutely maddening the nomenclature for all this is.

By default, `predictions()` and friends will calculate predictions for subjects on average by using the `re_formula = NULL` argument. This estimate includes details from the random offsets, either by integrating them out or by using the mean and standard deviation of the random offsets to generate a simulated average subject. When working with slopes, this is also called a *marginal effect*.

We could also use `re_formula = NA` to calculate predictions for a typical subject, or a subject where the random offset is set to 0. When working with slopes, this is also called a *conditional effect*.

- Conditional predictions/effect = average subject = `re_formula = NA`
- Marginal predictions/effect = subjects on average = `re_formula = NULL` (default), using existing subject levels or a new simulated subject

Again, [see this guide for way more about these distinctions](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects). In this example here, we'll just use the default marginal predictions/effects (`re_formula = NULL`), or the effect for subjects on average.
:::

The predicted proportions aren't identical to the SAS output, but they're close enough, given that it's a completely different modeling approach.

```{r pred-model-chocolate-brms}
preds_chocolate_brms <- predictions(
  model_chocolate_brms, 
  newdata = datagrid(dark = unique, soft = unique, nuts = unique)
) 

preds_chocolate_brms %>% 
  as_tibble() %>%
  arrange(desc(estimate)) %>% 
  select(dark, soft, nuts, estimate, conf.low, conf.high)
```

### Plots

Since `predictions()` returns a tidy data frame, we can plot these predicted probabilities (or market shares or however we want to think about them) with {ggplot2}:

```{r plot-preds-chocolate, fig.width=6, fig.height=5.5}
p1 <- preds_chocolate_mlogit %>% 
  arrange(estimate) %>% 
  mutate(label = str_to_sentence(glue::glue("{dark} chocolate, {soft} interior, {nuts}"))) %>% 
  mutate(label = fct_inorder(label)) %>% 
  ggplot(aes(x = estimate, y = label)) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high), color = clrs[7]) +
  scale_x_continuous(labels = label_percent()) +
  labs(
    x = "Predicted probability of selection", y = NULL,
    title = "Frequentist {mlogit} predictions") +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())

p2 <- preds_chocolate_brms %>% 
  posterior_draws() %>%  # Extract the posterior draws of the predictions
  arrange(estimate) %>% 
  mutate(label = str_to_sentence(glue::glue("{dark} chocolate, {soft} interior, {nuts}"))) %>% 
  mutate(label = fct_inorder(label)) %>% 
  ggplot(aes(x = draw, y = label)) +
  stat_halfeye(normalize = "xy", fill = clrs[7])  +
  scale_x_continuous(labels = label_percent()) +
  labs(
    x = "Predicted probability of selection", y = NULL,
    title = "Bayesian {brms} predictions") +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank()) +
  # Make the x-axis match the mlogit plot
  coord_cartesian(xlim = c(-0.05, 0.78))

p1 / p2
```


## AMCEs

The marketing world doesn't typically look at coefficients or marginal effects, but the political science world definitely does. In political science, the estimand we often care about the most is the average marginal component effect (AMCE), or the causal effect of moving one feature level to a different value, holding all other features constant. [I have a whole in-depth blog post about AMCEs and how to calculate them](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/)—go look at that for more details. Long story short—AMCEs are basically the coefficients in a regression model.

Interpreting the coefficients is difficult with models that aren't basic linear regression. Here, all these coefficients are on the log scale, so they're not directly interpretable. The original SAS technical note also doesn't really interpret any of these , they don't really interpret these things anyway, since they're more focused on predictions. All they say is this:

> The parameter estimate with the smallest *p*-value is for soft center. Since the parameter estimate is negative, chewy is the more preferred level. Dark is preferred over milk, and nuts over no nuts, however only the *p*-value for Soft is less than 0.05.

We could exponentiate the coefficients to make them multiplicative (akin to odds ratios in logistic regression). For center = soft, $e^{-2.19722}$ = `r exp(-2.19722)`, which means that candies with a soft center are `r label_percent()(1 - exp(-2.19722))` less likely to be chosen than candies with a chewy center, relative to the average candy. But that's weird to think about.

So instead we can turn to {marginaleffects} once again to calculate percentage-point scale estimands that we can interpret far more easily.

::: {.callout-important}
#### lol marginal effects

Nobody is ever consistent about the word "marginal effect." Some people use it to refer to averages; some people use it to refer to slopes. These are complete opposites. In calculus, averages = integrals and slopes = derivatives and they're the inverse of each other.

I like to think of marginal effects as what happens to the outcome when you move an explanatory variable a tiny bit. With continuous variables, that's a slope; with categorical variables, that's an offset in average outcomes. These correspond directly to how you normally interpret regression coefficients. Or returning to [my favorite analogy about regression](https://www.andrewheiss.com/blog/2022/05/20/marginalia/#regression-sliders-switches-and-mixing-boards), with numeric variables we care what happens to the outcome when we slide the value up a tiny bit; with categorical variables we care about what happens to the outcome when we switch on a category.

Additionally, there are like a billion different ways to calculate marginal effects: average marginal effects (AMEs), group-average marginal effects (G-AMEs), marginal effects at user-specified values, marginal effects at the mean (MEM), and counterfactual marginal effects. See [the documentation for {marginaleffects}](https://vincentarelbundock.github.io/marginaleffects/articles/slopes.html) + [this mega blog post](https://www.andrewheiss.com/blog/2022/05/20/marginalia/) for more about these subtle differences.
:::

### Bayesian comparisons/contrasts

We can use `avg_comparisons()` to calculate the difference (or average marginal effect) for each of the categorical coefficients on the percentage-point scale, showing the effect of moving from milk → dark, chewy → soft, and nuts → no nuts.

(Technically we can also use `avg_slopes()`, even though none of these coefficients are actually slopes. {marginaleffects} is smart enough to show contrasts for categorical variables and partial derivatives/slopes for continuous variables.)

```{r show-comparisons-chocolate-brms}
avg_comparisons(model_chocolate_brms)
```

When holding all other features constant, moving from chewy → soft is associated with a posterior median 18 percentage point decrease in the probability of selection (or drop in market share if you want to think of it that way), on average.

### Frequentist comparisons/contrasts

We went out of order in this section and showed how to use `avg_comparisons()` with the Bayesian model first instead of the frequentist model. That's because it was easy. `mlogit()` models [behave strangely with {marginaleffects}](https://vincentarelbundock.github.io/marginaleffects/articles/categorical.html#mlogit-package) because {mlogit} forces its predictions to use every possible value of the alternatives A–H. Accordingly, the estimate for any coefficients in the attributes section of the {mlogit} formula (`dark`, `soft`, and `nuts` here) will automatically be zero. Note how here there are 24 rows of comparisons instead of 3, since we get comparisons in each of the 8 groups, and note how the estimates are all zero:

```{r show-comparisons-chocolate-mlogit-wrong}
avg_comparisons(model_chocolate_mlogit)
```

If we had continuous variables, we could work around this by [specifying our own tiny amount of marginal change](https://vincentarelbundock.github.io/marginaleffects/articles/categorical.html#mlogit-package) to compare across, but we're working with categories and can't do that. Instead, with categorical variables, we can return to `predictions()` and [define custom aggregations of different features and levels](https://vincentarelbundock.github.io/marginaleffects/articles/predictions.html#multinomial-models). 

Before making custom aggregations, though, it'll be helpful to illustrate what exactly we're looking at when collapsing these results. Remember that earlier we calculated predictions for all the unique combinations of `dark`, `soft`, and `nuts`:

```{r preds-chocolate-mlogit-again}
preds_chocolate_mlogit <- predictions(
  model_chocolate_mlogit, 
  newdata = datagrid(dark = unique, soft = unique, nuts = unique)
) 

preds_chocolate_mlogit %>% 
  as_tibble() %>% 
  select(group, dark, soft, nuts, estimate)
```

Four of the groups have `dark` = Milk and four have `dark` = Dark, with other varying characteristics across those groups (chewy/soft, nuts/no nuts). If we want the average proportion of all milk and dark chocolate options, we can group and summarize:

```{r preds-chocolate-avg-dark-manual}
preds_chocolate_mlogit %>% 
  group_by(dark) %>% 
  summarize(avg_pred = mean(estimate))
```

The average market share for milk chocolate candies, holding all other features constant, is 5% ($\frac{0.0540 + 0.126 + 0.006 + 0.014}{2} = 0.05$); the average market share for dark chocolate candies is 20% ($\frac{0.216 + 0.504 + 0.024 + 0.056}{2} = 0.2$). These values are the averages of the predictions from the four groups where `dark` is either Milk or Dark.

Instead of calculating these averages manually (which would also force us to calculate standard errors and p-values manually, which, ugh), we can calculate these aggregate group means with `predictions()`. To do this, we can feed a little data frame to `predictions()` with the `by` argument. The data frame needs to contain columns for the features we want to collapse, and a `by` column with the labels we want to include in the output. For example, if we want to collapse the eight possible choices into those with milk chocolate and those with dark chocolate, we could create a `by` data frame like this:

```{r by-df-example}
by <- data.frame(dark = c("Milk", "Dark"), by = c("Milk", "Dark"))
by
```

If we use that `by` data frame in `predictions()`, we get the same 5% and 20% from before, but now with all of {marginaleffects}'s extra features like standard errors and confidence intervals:

```{r preds-chocolate-avg-by-dark}
predictions(
  model_chocolate_mlogit,
  by = by
)
```

Even better, we can use the `hypothesis` functionality of `predictions()` to conduct a hypothesis test and calculate the difference (or contrast) between these two averages, which is *exactly what we're looking for* with categorical AMCEs. This shows the average causal effect of moving from milk → dark—holding all other features constant, switching the chocolate type from milk to dark causes a 15 percentage point increase in the probability of selecting the candy, on average.

```{r preds-chocolate-avg-by-dark-hypothesis}
predictions(
  model_chocolate_mlogit,
  by = by,
  hypothesis = "revpairwise"
)
```

We can't simultaneously specify all the contrasts we're interested in single `by` argument, but we can do them separately and combine them into a single data frame:

```{r preds-chocolate-avg-all}
amces_chocolate_mlogit <- bind_rows(
  dark = predictions(
    model_chocolate_mlogit,
    by = data.frame(dark = c("Milk", "Dark"), by = c("Milk", "Dark")),
    hypothesis = "revpairwise"
  ),
  soft = predictions(
    model_chocolate_mlogit,
    by = data.frame(soft = c("Chewy", "Soft"), by = c("Chewy", "Soft")),
    hypothesis = "revpairwise"
  ),
  nuts = predictions(
    model_chocolate_mlogit,
    by = data.frame(nuts = c("No nuts", "Nuts"), by = c("No nuts", "Nuts")),
    hypothesis = "revpairwise"
  ),
  .id = "variable"
)
amces_chocolate_mlogit
```

### Plots

Plotting these AMCEs requires a bit of data wrangling, but we get really neat plots, so it's worth it. I've hidden all the code here for the sake of space.

```{r extract-chocolate-variable-levels}
#| code-fold: true
#| code-summary: "Extract variable labels"
chocolate_var_levels <- tibble(
  variable = c("dark", "soft", "nuts")
) %>% 
  mutate(levels = map(variable, ~{
    x <- chocolate[[.x]]
    if (is.numeric(x)) {
      ""
    } else if (is.factor(x)) {
      levels(x)
    } else {
      sort(unique(x))
    }
  })) %>% 
  unnest(levels) %>% 
  mutate(term = paste0(variable, levels))

# Make a little lookup table for nicer feature labels
chocolate_var_lookup <- tribble(
  ~variable, ~variable_nice,
  "dark",    "Type of chocolate",
  "soft",    "Type of center",
  "nuts",    "Nuts"
) %>% 
  mutate(variable_nice = fct_inorder(variable_nice))
```

```{r plot-amces-chocolate-mlogit}
#| code-fold: true
#| code-summary: "Combine full dataset of factor levels with model comparisons and make {mlogit} plot"
amces_chocolate_mlogit_split <- amces_chocolate_mlogit %>% 
  separate_wider_delim(
    term,
    delim = " - ", 
    names = c("variable_level", "reference_level")
  ) %>% 
  rename(term = variable)

plot_data <- chocolate_var_levels %>%
  left_join(
    amces_chocolate_mlogit_split,
    by = join_by(variable == term, levels == variable_level)
  ) %>% 
  # Make these zero
  mutate(
    across(
      c(estimate, conf.low, conf.high),
      ~ ifelse(is.na(.x), 0, .x)
    )
  ) %>% 
  left_join(chocolate_var_lookup, by = join_by(variable)) %>% 
  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))

p1 <- ggplot(
  plot_data,
  aes(x = estimate, y = levels, color = variable_nice)
) +
  geom_vline(xintercept = 0) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  scale_x_continuous(labels = label_pp) +
  scale_color_manual(values = clrs[c(1, 3, 8)]) +
  guides(color = "none") +
  labs(
    x = "Percentage point change in\nprobability of candy selection",
    y = NULL,
    title = "Frequentist AMCEs from {mlogit}"
  ) +
  facet_col(facets = "variable_nice", scales = "free_y", space = "free")
```

```{r plot-amces-chocolate-brms}
#| code-fold: true
#| code-summary: "Combine full dataset of factor levels with posterior draws and make {brms} plot"
# This is much easier than the mlogit mess because we can use avg_comparisons() directly
posterior_mfx <- model_chocolate_brms %>% 
  avg_comparisons() %>% 
  posteriordraws() 

posterior_mfx_nested <- posterior_mfx %>% 
  separate_wider_delim(
    contrast,
    delim = " - ", 
    names = c("variable_level", "reference_level")
  ) %>% 
  group_by(term, variable_level) %>% 
  nest()

# Combine full dataset of factor levels with model results
plot_data_bayes <- chocolate_var_levels %>%
  left_join(
    posterior_mfx_nested,
    by = join_by(variable == term, levels == variable_level)
  ) %>%
  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) %>% 
  unnest(data) %>% 
  left_join(chocolate_var_lookup, by = join_by(variable)) %>% 
  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))

p2 <- ggplot(plot_data_bayes, aes(x = draw, y = levels, fill = variable_nice)) +
  geom_vline(xintercept = 0) +
  stat_halfeye(normalize = "groups") +  # Make the heights of the distributions equal within each facet
  guides(fill = "none") +
  facet_col(facets = "variable_nice", scales = "free_y", space = "free") +
  scale_x_continuous(labels = label_pp) +
  scale_fill_manual(values = clrs[c(1, 3, 8)]) +
  labs(
    x = "Percentage point change in\nprobability of candy selection",
    y = NULL,
    title = "Posterior Bayesian AMCEs from {brms}"
  )
```

```{r plot-amces-chocolate-both, fig.height=4, fig.width=8, out.width="100%"}
#| column: body-outset
p1 | p2
```

\ 

# Part 2: Minivans; repeated questions; basic multinomial logit

## The setup

In this experiment, respondents are asked to choose which of these minivans they'd want to buy, based on four different features/attributes with different levels:

| Features/Attributes | Levels                       |
|:--------------------|:-----------------------------|
| Passengers          | 6, 7, 8                      |
| Cargo area          | 2 feet, 3 feet               |
| Engine              | Gas, electric, hybrid        |
| Price               | \$30,000; \$35,000; \$40,000 |

Respondents see this a question similar to this fifteen different times, with three options with randomly shuffled levels for each of the features.

:::: {.callout-tip}

### Example survey question

|            |            Option 1             |            Option 2             |            Option 3             |
|------------------|:----------------:|:----------------:|:----------------:|
| Passengers |                7                |                8                |                6                |
| Cargo area |             3 feet              |             3 feet              |             2 feet              |
| Engine     |            Electric             |               Gas               |             Hybrid              |
| Price      |            \$40,000             |            \$40,000             |            \$30,000             |
| Choice     | <input type="radio" name="ex2"> | <input type="radio" name="ex2"> | <input type="radio" name="ex2"> |

::::


## The data

The data for this kind of experiment has one row for each possible alternative (`alt`) within each set of 15 questions (`ques`), thus creating 3 × 15 = 45 rows per respondent (`resp.id`). There were 200 respondents, with 45 rows each, so there are 200 × 45 = 9,000 rows. Here, Respondent 1 chose a \$30,000 gas van with 6 seats and 3 feet of cargo space in the first set of three options, a \$35,000 gas van with 7 seats and 3 feet of cargo space in the second set of three options, and so on. 

There's also a column here for `carpool` indicating if the respondent carpools with others when commuting. It's an individual respondent-level characteristic and is constant throughout all the questions and alternatives, and we'll use it later.

```{r show-minivans}
minivans
```


## The model

Respondents were shown three different options and asked to select one. We thus have three possible outcomes: a respondent could have selected option 1, option 2, or option 3. Because everything was randomized, there shouldn't be any patterns in which options people choose—we don't want to see that the first column is more common, since that would indicate that respondents are just repeatedly selecting the first column to get through the survey. Since there are three possible outcomes (option 1, 2, and 3), we'll use multinomial logistic regression.

### Original model as a baseline

In the example in their textbook, @ChapmanFeit:2019 use {mlogit} to estimate this model and they find these results. This will be our baseline throughout this example.

![Original results from @ChapmanFeit:2019 p. 371](img/chapman-feit-mlogit.png)

### `mlogit` model

This data is a little more complex now, since there are alternatives nested inside questions inside respondents. To account for this panel structure when using {mlogit}, we need to define two index columns: one for the unique set of alternatives offered to the respondent and one for the respondent ID. We still do this with `dfidx()`, but need to create a new column with an ID number for each unique combination of respondent ID and question number:

```{r make-dfidx-minivans}
minivans_idx <- minivans %>% 
  # mlogit() needs a column with unique question id numbers
  group_by(resp.id, ques) %>% 
  mutate(choice.id = cur_group_id()) %>% 
  ungroup() %>% 
  # Make indexed data frame for mlogit
  dfidx(
    idx = list(c("choice.id", "resp.id"), "alt"),
    choice = "choice",
    shape = "long"
  )
```

Now we can fit the model. Note the `0 ~ seat` syntax here. That suppresses the intercept for the model, which behaves weirdly with multinomial models. Since there are three categories for the outcome (options 1, 2, and 3), there are two intercepts, representing cutpoints-from-ordered-logit-esque shifts in the probability of selecting option 1 vs. option 2 and option 2 vs. option 3. We don't want to deal with those, so we'll suppress them. 

```{r model-minivans-mlogit, message=FALSE}
model_minivans_mlogit <- mlogit(
  choice ~ 0 + seat + cargo + eng + price | 0 | 0, 
  data = minivans_idx
)
model_parameters(model_minivans_mlogit, digits = 4, p_digits = 4)
```

These are the same results from p. 371 in @ChapmanFeit:2019, so it worked. Again, the marketing world doesn't typically do much with these coefficients beyond looking at their direction and magnitude. For instance, in @ChapmanFeit:2019 they say that the estimate for `seat [7]` here is negative, which means that a 7-seat option is less preferred than 6-seat option, and that the estimate for `price [40]` is more negative than the already-negative estimate for `price [35]`, which means that (1) respondents don't like the \$35,000 option compared to the baseline \$30,000 and that (2) respondents *really* don't like the \$40,000 option. We could theoretically exponentiate these things—like, seeing 7 seats makes it $e^{-0.5353}$ = 0.5855 = 41% less likely to select the option compared to 6 seats—but again, that's weird.


### Bayesian model with {brms}

We can also fit this multinomial model in a Bayesian way using {brms}. Stan has a categorical family for dealing with mulitnomial/categorical outcomes. But first, we'll look at the nested structure of this data and incorporate that into the model, since we won't be using the weird {mlogit}-style indexed data frame. As with the chocolate experiment, the data has a natural hierarchy in it, with three questions nested inside 15 separate question sets, nested inside each of the 200 respondents.

```{tikz minivan-multilevel-structure, engine.opts=font_opts}
#| echo: false
#| fig-cap: "Multilevel experimental structure, with minivan choices $\\{y_1, y_2, y_3\\}$ nested in sets of questions in respondents"
#| fig-align: center
#| fig-ext: svg
#| column: page
#| out-width: 100%
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\begin{tikzpicture}[{every node/.append style}=draw]
  \node [rectangle] (respondent1) at (0, 2.5) {Respondent 1};

  \node [rectangle] (q11) at (-1.5, 1) {Set $1_1$};
  \node [draw=none] (q21) at (0, 1) {$\dots$};
  \node [rectangle] (q31) at (1.5, 1) {Set $15_1$};

  \node [circle] (y111) at (-3.75, -0.5) {$y_{1_{1_1}}$};
  \node [circle] (y211) at (-2.5, -0.5) {$y_{2_{1_1}}$};
  \node [circle] (y311) at (-1.25, -0.5) {$y_{3_{1_1}}$};

  \node [draw=none] (y121) at (0, -0.5) {$\dots$};

  \node [circle] (y131) at (1.25, -0.5) {$y_{1_{15_1}}$};
  \node [circle] (y231) at (2.5, -0.5) {$y_{2_{15_1}}$};
  \node [circle] (y331) at (3.75, -0.5) {$y_{3_{15_1}}$};

  \draw [-latex] (respondent1) to (q11);
  \draw [-latex] (respondent1) to (q21);
  \draw [-latex] (respondent1) to (q31);
  \draw [-latex] (q11) to (y111);
  \draw [-latex] (q11) to (y211);
  \draw [-latex] (q11) to (y311);
  \draw [-latex] (q21) to (y121);
  \draw [-latex] (q31) to (y131);
  \draw [-latex] (q31) to (y231);
  \draw [-latex] (q31) to (y331);

  \node [draw=none] (respondent2) at (5, 2.5) {$\dots$};
  \node [rectangle] (y12) at (4, 1) {Set 1};
  \node [draw=none] (y22) at (5, 1) {$\dots$};
  \node [rectangle] (y32) at (6, 1) {Set 15};
  \draw [-latex] (respondent2) to (y12);
  \draw [-latex] (respondent2) to (y22);
  \draw [-latex] (respondent2) to (y32);

  \node [rectangle] (respondentn) at (10, 2.5) {Respondent 200};
  \node [rectangle] (q1n) at (8.5, 1) {Set $1_{200}$};
  \node [draw=none] (q2n) at (10, 1) {$\dots$};
  \node [rectangle] (q3n) at (11.5, 1) {Set $15_{200}$};

  \node [circle] (y113) at (6.25, -0.5) {$y_{1_{1_{200}}}$};
  \node [circle] (y213) at (7.5, -0.5) {$y_{2_{1_{200}}}$};
  \node [circle] (y313) at (8.75, -0.5) {$y_{3_{1_{200}}}$};

  \node [draw=none] (y123) at (10, -0.5) {$\dots$};

  \node [circle] (y133) at (11.25, -0.5) {$y_{1_{15_{200}}}$};
  \node [circle] (y233) at (12.5, -0.5) {$y_{2_{15_{200}}}$};
  \node [circle] (y333) at (13.75, -0.5) {$y_{3_{15_{200}}}$};

  \draw [-latex] (respondentn) to (q1n);
  \draw [-latex] (respondentn) to (q2n);
  \draw [-latex] (respondentn) to (q3n);
  \draw [-latex] (q1n) to (y113);
  \draw [-latex] (q1n) to (y213);
  \draw [-latex] (q1n) to (y313);
  \draw [-latex] (q2n) to (y123);
  \draw [-latex] (q3n) to (y133);
  \draw [-latex] (q3n) to (y233);
  \draw [-latex] (q3n) to (y333);

  \node [rectangle] (population) at (5, 4) {Population of experiment respondents};
  \draw [-latex] (population) to (respondent1);
  \draw [-latex] (population) to (respondent2);
  \draw [-latex] (population) to (respondentn);
\end{tikzpicture}
```

Currently, our main outcome variable `choice` is binary. If we run the model with `choice` as the outcome with a categorical family, the model will fit, but it will go slow and {brms} will complain about it and recommend switching to regular logistic regression. The categorical family in Stan requires 2+ outcomes and a reference category. Here we have three possible options (1, 2, and 3), and we can imagine a reference category of 0 for rows that weren't selected. 

We can create a new outcome column (`choice_alt`) that indicates which option each respondent selected: 0 if they didn't choose the option and 1–3 if they chose the first, second, or third option. Because of how the data is recorded, this only requires multiplying `alt` and `choice`:

```{r create-choice-alt-column}
minivans_choice_alt <- minivans %>% 
  mutate(choice_alt = factor(alt * choice))

minivans_choice_alt %>% 
  select(resp.id, ques, alt, seat, cargo, eng, price, choice, choice_alt)
```

We can now use the new four-category `choice_alt` column as our outcome with the `categorical()` family. 

If we *realllly* wanted, we could add random effects for question sets nested inside respondents, like `(1 | resp.id / ques)`. We'd want to do that if there were set-specific things that could influences choices. Like maybe we want to account for the possibility that everyone's just choosing the first option, so it behaves differently? Or maybe the 5th set of questions is set to an extra difficult level on a quiz or something? Or maybe we have so many sets that we think the later ones will be less accurate because of respondent fatigue? idk. In this case, question set-specific effects don't matter at all. Each question set is equally randomized and no different from the others, so we won't bother modeling that layer of the hierarchy.

We want to model the choice of option 1, 2, or 3 (`choice_alt`) based on minivan characteristics (`seat`, `cargo`, `eng`, price). With the categorical model, we actually get a set of parameters to estimate the probability of selecting each of the options, which Stan calls $\mu$, so we have a set of three probabilities: $\{\mu_1, \mu_2, \mu_3\}$. We'll use the subscript $i$ to refer to individual minivan choices and $j$ to refer to respondents. Here's the fun formal model:

$$
\begin{aligned}
&\ \textbf{Multinomial probability of selection of choice}_i \textbf{ in respondent}_j \\
\text{Choice}_{i_j} \sim&\ \operatorname{Categorical}(\{\mu_{1,i_j}, \mu_{2,i_j}, \mu_{3,i_j}\}) \\[10pt]
&\ \textbf{Model for probability of each option} \\
\{\mu_{1,i_j}, \mu_{2,i_j}, \mu_{3,i_j}\} =&\ (\beta_0 + b_{0_j}) + \beta_1 \text{Seat[7]}_{i_j} + \beta_2 \text{Seat[8]}_{i_j} + \beta_3 \text{Cargo[3ft]}_{i_j} + \\
&\ \beta_4 \text{Engine[hyb]}_{i_j} + \beta_5 \text{Engine[elec]}_{i_j} + \beta_6 \text{Price[35k]}_{i_j} + \beta_7 \text{Price[40k]}_{i_j} \\[5pt]
b_{0_j} \sim&\ \mathcal{N}(0, \sigma_0) \qquad\quad\quad \text{Respondent-specific offsets from global probability} \\[10pt]
&\ \textbf{Priors} \\
\beta_{0 \dots 7} \sim&\ \mathcal{N} (0, 3) \qquad\qquad\ \ \text{Prior for choice-level coefficients} \\
\sigma_0 \sim&\ \operatorname{Exponential}(1) \quad \text{Prior for between-respondent variability}
\end{aligned}
$$

And here's the {brms} model. Notice the much-more-verbose prior section—because the categorical family in Stan estimates separate parameters for each of the categories ($\{\mu_1, \mu_2, \mu_3\}$), we have a mean and standard deviation for the probability of selecting each of those options. We need to specify each of these separately too instead of just doing something like `prior(normal(0, 3), class = b)`. Also notice the `refcat` argument in `categorical()`—this makes it so that all the estimates are relative to not choosing an option (or when `choice_alt` is 0). And *also* notice the slightly different syntax for the random respondent intercepts: `(1 | ID | resp.id)`. That new middle `ID` is special {brms} formula syntax that we can use when working with categorical or ordinal families, and it makes it so that the group-level effects for the different outcomes (here options 0, 1, 2, and 3) are correlated (see p. 4 of [this {brms} vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_multilevel.pdf) for more about this special syntax).

```{r model-minivans-categorical-brms}
model_minivans_categorical_brms <- brm(
  bf(choice_alt ~ 0 + seat + cargo + eng + price + (1 | ID | resp.id)),
  data = minivans_choice_alt,
  family = categorical(refcat = "0"),
  prior = c(
    prior(normal(0, 3), class = b, dpar = mu1),
    prior(normal(0, 3), class = b, dpar = mu2),
    prior(normal(0, 3), class = b, dpar = mu3),
    prior(exponential(1), class = sd, dpar = mu1),
    prior(exponential(1), class = sd, dpar = mu2),
    prior(exponential(1), class = sd, dpar = mu3)
  ),
  chains = 4, cores = 4, iter = 2000, seed = 1234,
  backend = "cmdstanr", threads = threading(2), refresh = 0,
  file = "models/model_minivans_categorical_brms"
)
```

```{r extract-cargo3ft-results, include=FALSE}
cargo3ft_results <- model_minivans_categorical_brms %>%
  # gather_draws(`b_.*`, regex = TRUE) %>%
  gather_draws(`b_mu\\d_cargo3ft`, regex = TRUE) %>%
  group_by(.variable) %>%
  median_qi() %>%
  mutate(across(
    c(.value, .lower, .upper),
    list(nice = ~ label_comma(accuracy = 0.01, style_negative = "minus")(.))
  )) %>% 
  mutate(ci = glue::glue("{.lower_nice}–{.upper_nice}")) %>% 
  split(~.variable)
```

This model gives us a ton of parameters! We get three estimates per feature level (i.e. `mu1_cargo3ft`, `mu2_cargo3ft`, and `mu3_cargo3ft` for the `cargo3ft` effect), since we're actually estimating the effect of each covariate on the probability of selecting each of the three options.

```{r show-model-minivans-brms, warning=FALSE, message=FALSE}
model_parameters(model_minivans_categorical_brms)
```

Importantly, the estimates here are all roughly equivalent to what we get from {mlogit}: the {mlogit} estimate for `cargo3ft` was 0.4775, while the three median posterior {brms} estimates are `r cargo3ft_results$b_mu1_cargo3ft$.value_nice` (95% credible interval: `r cargo3ft_results$b_mu1_cargo3ft$ci`), `r cargo3ft_results$b_mu2_cargo3ft$.value_nice` (`r cargo3ft_results$b_mu2_cargo3ft$ci`), and `r cargo3ft_results$b_mu3_cargo3ft$.value_nice` (`r cargo3ft_results$b_mu3_cargo3ft$ci`)

Since all the features are randomly shuffled between the three options each time, and each option is selected 1/3rd of the time, it's probably maybe legal to pool these posterior estimates together (maaaaybeee???) so that we don't have to work with three separate estimates for each parameter? To do this we'll take the average of each of the three $\mu$ estimates within each draw, which is also called "marginalizing" across the three options.

Here's how we'd do that with {tidybayes}. The medians are all roughly the same now!

```{r marginalize-cat-posterior-estimates}
minivans_cat_marginalized <- model_minivans_categorical_brms %>% 
  gather_draws(`^b_.*$`, regex = TRUE) %>% 
  # Each variable name has "mu1", "mu2", etc. built in, like "b_mu1_seat6". This
  # splits the .variable column into two parts based on a regular expression,
  # creating one column for the mu part ("b_mu1_") and one for the rest of the
  # variable name ("seat6")
  separate_wider_regex(
    .variable,
    patterns = c(mu = "b_mu\\d_", .variable = ".*")
  ) %>% 
  # Find the average of the three mu estimates for each variable within each
  # draw, or marginalize across the three options, since they're randomized
  group_by(.variable, .draw) %>% 
  summarize(.value = mean(.value)) 

minivans_cat_marginalized %>% 
  group_by(.variable) %>% 
  median_qi()
```

And for fun, here's what the posterior for new combined/collapsed/marginalized `cargo3ft` looks like. Great.

```{r plot-cargo3ft-combined-posterior, out.width="80%"}
minivans_cat_marginalized %>% 
  filter(.variable == "cargo3ft") %>% 
  ggplot(aes(x = .value, y = .variable)) +
  stat_halfeye(fill = clrs[4]) +
  labs(x = "Posterior distribution of β (logit-scale)", y = NULL)
```

## Predictions

[As we saw in the first example with chocolates](#predictions), the marketing world typically uses predictions from these kinds of models to estimate the predicted market share for products with different constellations of features. That was a pretty straightforward task with the chocolate model since respondents were shown all 8 options simultaneously. It's a lot trickier with the minivan example where respondents were shown 15 sets of 3 options. Dealing with multinomial predictions is a bear of a task because these models are a lot more complex. 

### Frequentist predictions

With the chocolate model, we could just use `avg_predictions(model_chocolate_mlogit)` and automatically get predictions for all 8 options. That's not the case here:

```{r show-mlogit-predictions-wrong}
avg_predictions(model_minivans_mlogit)
```

We get three predictions, and they're all 33ish%. That's because respondents were presented with three randomly shuffled options and chose one of them. All these predictions tell us is that across all 15 iterations of the questions, 1/3 of respondents selected the first option, 1/3 the second, and 1/3 the third. That's a good sign in this case—there's no evidence that people were just repeatedly choosing the first option. But in the end, these predictions aren't super useful.

We instead want to be able to get predicted market shares (or predicted probabilities) for any given mix of products. For instance, here are six hypothetical products with different combinations of seats, cargo space, engines, and prices:

```{r create-example-product-mix}
example_product_mix <- tribble(
  ~seat, ~cargo, ~eng, ~price,
  "7", "2ft", "hyb", "30",
  "6", "2ft", "gas", "30",
  "8", "2ft", "gas", "30",
  "7", "3ft", "gas", "40",
  "6", "2ft", "elec", "40",
  "7", "2ft", "hyb", "35"
) %>% 
  mutate(across(everything(), factor)) %>% 
  mutate(eng = factor(eng, levels = levels(minivans$eng)))
example_product_mix
```

If we were working with any other type of model, we could plug this data into the `newdata` argument of `predictions()` and get predicted values. That doesn't work here though. There were 200 respondents in the original data, and {mlogit}-predictions need to happen on a dataset with a multiple of that many rows. We can't just feed it 6 values.

```{r preds-minivan-mlogit-wrong, error=TRUE}
predictions(model_minivans_mlogit, newdata = example_product_mix)
```

Instead, following @ChapmanFeit:2019 (and [this Stan forum post](https://discourse.mc-stan.org/t/getting-predictions-for-multinomial-model-using-brms/22335)), we can manually multiply the covariates in `example_product_mix` with the model coefficients to calculate "utility" (or predicted vales on the logit scale), which we can then exponentiate and divide to calculate market shares.

::: {.callout-important}
#### Limits of {marginaleffects} and {mlogit}

From what I can tell, this is not possible with {marginaleffects} because that package can't work with the coefficients from {mlogit} models and can only really work with predictions. {mlogit} predictions are forced to be on the response/probability scale since they're, like, predictions, so there's no way to get them on the log/logit link scale to calculate utilities and shares.
:::

```{r generate-manual-predictions}
# Create a matrix of 0s and 1s for the values in `example_product_mix`, omitting
# the first column (seat6)
example_product_dummy_encoded <- model.matrix(
  update(model_minivans_mlogit$formula, 0 ~ .),
  data = example_product_mix
)[, -1]
example_product_dummy_encoded

# Matrix multiply the matrix of 0s and 1s with the model coefficients to get
# logit-scale predictions, or utility
utility <- example_product_dummy_encoded %*% coef(model_minivans_mlogit)

# Divide each exponentiated utility by the sum of the exponentiated utilities to
# get the market share
share <- exp(utility) / sum(exp(utility))

# Stick all of these in one final dataset
bind_cols(share = share, logits = utility, example_product_mix)
```

::: {.callout-tip collapse="true"}
### Function version of this kind of prediction

On p. 375 of @ChapmanFeit:2019 (and at [this Stan forum post](https://discourse.mc-stan.org/t/getting-predictions-for-multinomial-model-using-brms/22335/)), there's a function called `predict.mnl()` that does this utility and share calculation automatically. Because this post is more didactic and because I'm more interested in the Bayesian approach, I didn't use it earlier, but it works well.

```{r predict-mnl-example}
predict.mnl <- function(model, data) {
  # Function for predicting shares from a multinomial logit model 
  # model: mlogit object returned by mlogit()
  # data: a data frame containing the set of designs for which you want to 
  #       predict shares. Same format at the data used to estimate model. 
  data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[ , -1]
  utility <- data.model %*% model$coef
  share <- exp(utility) / sum(exp(utility))
  cbind(share, data)
}

predict.mnl(model_minivans_mlogit, example_product_mix)
```

:::

This new predicted `share` column sums to one, and it shows us the predicted market share assuming these are the only six products available. The \$30,000 six-seater 2ft gas van and the \$30,000 eight-seater 2ft gas van would comprise more than 75% (0.43337 + 0.31918) of a market consisting of these six products. Because we did this calculation by hand, we lose all of {marginaleffects}'s extra features like standard errors and hypothesis tests. Alas.

### Bayesian predictions

If we use the categorical multinomial {brms} model we run into the same issue of getting weird predictions. Here it shows that 2/3rds of predictions are 0, which makes sense—if a respondent is offered 10 iterations of 3 possible choices, that would be 30 total choices, but they can only choose one option per iteration, so 20 choices (or 20/30 or 2/3) wouldn't be selected. The other three groups are each 11%, since that's the remaining 33% divided evenly across three options. Neat, I guess, but still not super helpful.

```{r show-brms-categorical-predictions-wrong, cache=TRUE}
avg_predictions(model_minivans_categorical_brms)
```

Instead of going through the manual process of matrix-multiplying a dataset of some mix of products with a single set of coefficients, we can use `predictions(..., type = "link")` to get predicted values on the log-odds scale, or that utility value that we found before. 

::: {.callout-note}
### `marginaleffects::predictions()` vs. {tidybayes} functions

We can actually use either `marginaleffects::predictions()` or {tidybayes}'s `*_draw()` functions for these posterior predictions. They do the same thing, with slightly different syntax:

```{r mfx-preds-vs-linpred, eval=FALSE}
# Logit-scale predictions with marginaleffects::predictions()
model_minivans_categorical_brms %>% 
  predictions(newdata = example_product_mix, re_formula = NA, type = "link") %>% 
  posterior_draws()

# Logit-scale predictions with tidybayes::add_linpred_draws()
model_minivans_categorical_brms %>% 
  add_linpred_draws(newdata = example_product_mix, re_formula = NA)
```

[Earlier in the chocolate example](#bayesian-predictions), I used `marginaleffects::predictions()` with the Bayesian {brms} model. Here I'm going to switch to the {tidybayes} prediction functions instead, in part because these multinomial models with the `categorical()` family are a lot more complex (though {marginaleffects} can handle them nicely), but mostly because in the actual paper I'm working on with real conjoint data, our MCMC results were generated with raw Stan code through `rstan`, and {marginaleffects} doesn't support raw Stan models.

[Check out this guide](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/) for the differences between {tidybayes}'s three general prediction functions: `predicted_draws()`, `epred_draws()`, and `linpred_draws()`.
:::

Additionally, we now actually have 4,000 draws in 3 categories (option 1, option 2, and option 3), so we actually have 12,000 sets of coefficients (!). To take advantage of the full posterior distribution of these coefficients, we can calculate shares within each set of draws within each of the three categories, resulting in a distribution of shares rather than single values.

```{r generate-product-mix-shares-brms}
draws_df <- example_product_mix %>% 
  add_linpred_draws(model_minivans_categorical_brms, value = "utility", re_formula = NA)

shares_df <- draws_df %>% 
  # Look at each set of predicted utilities within each draw within each of the
  # three outcomes
  group_by(.draw, .category) %>% 
  mutate(share = exp(utility) / sum(exp(utility))) %>% 
  ungroup() %>% 
  mutate(
    mix_type = paste(seat, cargo, eng, price, sep = " "),
    mix_type = fct_reorder(mix_type, share)
  )
```

We can summarize this huge dataset of posterior shares to get medians and credible intervals, but we need to do one extra step first. Right now, we have three predictions for each mix type, one for each of the categories (i.e. option 1, option 2, and option 3.

```{r show-shares-brms-categorical-by-category}
shares_df %>% 
  group_by(mix_type, .category) %>% 
  median_qi(share)
```

Since those options were all randomized, we can lump them all together as a single choice. To do this we'll take the average share across the three categories (this is also called "marginalizing") within each posterior draw.

```{r show-shares-brms-categorical-marginalized}
shares_marginalized <- shares_df %>% 
  # Marginalize across categories within each draw
  group_by(mix_type, .draw) %>% 
  summarize(share = mean(share)) %>% 
  ungroup()

shares_marginalized %>% 
  group_by(mix_type) %>% 
  median_qi(share)
```

And we can plot them:

```{r plot-shares-brms-categorical}
shares_marginalized %>% 
  ggplot(aes(x = share, y = mix_type)) +
  stat_halfeye(fill = clrs[10], normalize = "xy") +
  scale_x_continuous(labels = label_percent()) +
  labs(x = "Predicted market share", y = "Hypothetical product mix")
```

This is great because (1) it includes the uncertainty in the estimated shares, and (2) it lets us do neat Bayesian inference and say things like "there's a 93% chance that in this market of 6 options, a \$30,000 6-passenger gas minivan with 2 feet of storage would reach at least 40% market share":

```{r plot-shares-pd-example, out.width="80%"}
shares_marginalized %>% 
  filter(mix_type == "6 2ft gas 30") %>% 
  summarize(prop_greater_40 = sum(share >= 0.4) / n())

shares_marginalized %>% 
  filter(mix_type == "6 2ft gas 30") %>% 
  ggplot(aes(x = share, y = mix_type)) +
  stat_halfeye(aes(fill_ramp = after_stat(x >= 0.4)), fill = clrs[10]) +
  geom_vline(xintercept = 0.4) +
  scale_x_continuous(labels = label_percent()) +
  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[10], 0.4), guide = "none") +
  labs(x = "Predicted market share", y = NULL)
```


## AMCEs

As explained in the [AMCEs section for the chocolate data](#amces), in the social sciences we're less concerned about predicted market shares and more concerned about causal effects. Holding all other features constant, what is the effect of a \$5,000 increase in price or moving from 2 feet → 3 feet of storage space on the probability (or favorability) of selecting a minivan?

In the chocolate example, we were able to use `marginaleffects::avg_comparisons()` with the Bayesian model and get categorical contrasts automatically. This was because we cheated and used a Poisson model, since those can secretly behave like multinomial models. For the frequentist {mlogit}-based model, we had to use `marginaleffects::predictions()` instead and specify a special `by` argument to collapse the predictions into the different contrasts we were interested in.

In this case, since both the frequentist and Bayesian models for minivans are true multinomial models, we have to return to [{marginaleffects}'s special syntax](https://vincentarelbundock.github.io/marginaleffects/articles/predictions.html#multinomial-models) that lets us pass a data frame as the `by` argument. 

### Frequentist comparisons/contrasts

To help with the intuition behind this, since it's more complex this time, we'll first create a data frame with all 54 combinations of all the feature levels (3 seats × 2 cargos × 3 engines × 3 prices) and create a `by` column label that concatenates all the labels together so there's a single unique label for each row:

```{r create-mlogit-minivans-by}
by_all_combos <- minivans %>% 
  tidyr::expand(seat, cargo, eng, price) %>% 
  mutate(by = paste(seat, cargo, eng, price, sep = "_"))
by_all_combos
```

We can then feed this `by_all_combos` data frame into `predictions()`, which will generate predictions for all these levels *collapsed by all three of the possible groups* (i.e. option 1, option 2, and option 3). We can then split the `by` column back into separate columns for each of the feature levels so that we have those original columns back.

```{r create-all-preds-multinomial}
all_preds_mlogit <- predictions(
  model_minivans_mlogit,
  # predictions.mlogit() weirdly gets mad when working with tibbles :shrug:
  by = as.data.frame(by_all_combos)
) %>%
  # Split the `by` column up into separate columns
  separate(by, into = c("seat", "cargo", "eng", "price"))
as_tibble(all_preds_mlogit)
```

There are a lot of predicted probabilities here, so we need to collapse and average these by groups to make any sense of them. For instance, suppose we're interested in the AMCE of cargo space. We can first find the average predicted probability of selection with some grouping and summarizing:

```{r manual-cargo-amce}
manual_cargo_example <- all_preds_mlogit %>% 
  group_by(cargo) %>% 
  summarize(avg_pred = mean(estimate))
manual_cargo_example

diff(manual_cargo_example$avg_pred)
```

Holding all other features constant, the average probability (or average favorability, or average market share, or whatever we want to call it) of selecting a minivan with 2 feet of storage space is 0.292 (this is the average of the 27 predictions from `all_preds_mlogit` where `cargo` = `2ft`); the average probability for a minivan with 3 feet of storage space is 0.375 (again, this is the average of the 27 predictions from `all_preds_mlogit` where `cargo` = `3ft`). There's an 8.3 percentage point difference between these groups. **This is the causal effect or AMCE**: switching from 2 feet to 3 feet increases minivan favorability by 8 percentage points on average.

This manual calculation works, but it's tedious and doesn't include anything about standard errors. So instead, we can do it automatically with `predictions(..., by = data.frame(...))`.

```{r preds-minivan-cargo-mlogit}
preds_minivan_cargo_mlogit <- predictions(
  model_minivans_mlogit, 
  by = data.frame(
    cargo = levels(minivans$cargo),
    by = levels(minivans$cargo)
  )
)
preds_minivan_cargo_mlogit
```

And if we use the `hypothesis` argument (or the standalone `hypotheses()` function), we can get the difference between these average predictions, or the AMCE we care about—moving from 2 feet → 3 feet causes an 8 percentage point increase in favorability.

```{r preds-minivan-cargo-hypothesis-mlogit}
hypotheses(preds_minivan_cargo_mlogit, hypothesis = "revpairwise")
```

The `hypothesis` functionality works with more than 2 levels too. If calculate average predictions for the 3 seat configurations and specify `pariwise` or `revpairwise`, {marginaleffects} will give three differences: row 2 − row 1; row 3 − row 1; and row 3 − row 2. If we specify `reference` or `revreference`, it'll only give two, all based on the first row.

```{r preds-minivan-seat-hypotheses-mlogit}
preds_minivan_seat_mlogit <- predictions(
  model_minivans_mlogit, 
  by = data.frame(
    seat = levels(minivans$seat),
    by = levels(minivans$seat)
  )
)

hypotheses(preds_minivan_seat_mlogit, hypothesis = "revpairwise")
hypotheses(preds_minivan_seat_mlogit, hypothesis = "reference")
```

We can specify any other comparisons with `bN`, where `N` stands for the row number from `predictions()`. For instance, if we just want the difference between 6 (row 1) and 8 (row 2), we can do this:

```{r preds-minivan-seat-hypotheses-rows-mlogit}
hypotheses(preds_minivan_seat_mlogit, hypothesis = "b2 = b1")
```

We can make a big data frame with all the AMCEs we're interested in. I've hidden the code here because it's really repetitive.

```{r generate-all-minivan-amces-mlogit}
#| code-fold: true
#| code-summary: "Make separate datasets of predictions and combine them in one data frame"
preds_minivan_seat_mlogit <- predictions(
  model_minivans_mlogit, 
  by = data.frame(
    seat = levels(minivans$seat),
    by = levels(minivans$seat)
  )
)

preds_minivan_cargo_mlogit <- predictions(
  model_minivans_mlogit, 
  by = data.frame(
    cargo = levels(minivans$cargo),
    by = levels(minivans$cargo)
  )
)

preds_minivan_eng_mlogit <- predictions(
  model_minivans_mlogit, 
  by = data.frame(
    eng = levels(minivans$eng),
    by = levels(minivans$eng)
  )
)

preds_minivan_price_mlogit <- predictions(
  model_minivans_mlogit, 
  by = data.frame(
    price = levels(minivans$price),
    by = levels(minivans$price)
  )
)

amces_minivan_mlogit <- bind_rows(
  seat = hypotheses(preds_minivan_seat_mlogit, hypothesis = "reference"),
  cargo = hypotheses(preds_minivan_cargo_mlogit, hypothesis = "reference"),
  eng = hypotheses(preds_minivan_eng_mlogit, hypothesis = "reference"),
  # Need to specify these two tests manually because `hypotheses()` reeeeallly
  # wants to use 35 as the reference level because it's the first value that
  # appears in the original data. See
  # https://github.com/vincentarelbundock/marginaleffects/issues/861
  price = bind_rows(
    hypotheses(preds_minivan_price_mlogit, hypothesis = "b1 = b2") %>% mutate(term = "35 - 30"),
    hypotheses(preds_minivan_price_mlogit, hypothesis = "b3 = b2") %>% mutate(term = "40 - 30")
  ),
  .id = "variable"
)
```

```{r show-all-minivan-amces-mlogit}
amces_minivan_mlogit
```


### Bayesian comparisons/contrasts

Unlike the chocolate example, where the outcome variable was binary, we have to do similar `by`-like shenanigans with the Bayesian minivan model here. We could theoretically work with things like `marginaleffects::comparisons()` or `marginaleffects::slopes()` to extract the AMCEs from the model, but as I'll show below, there are some weird mathy things we have to deal with because of the multinomial outcome, and I think it's beyond what {marginaleffects} is designed to easily do. 

So instead we can use `epred_draws()` from {tidybayes} and calculate posterior predictions ourselves ([see this guide](https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/) for an overview of all of {tidybayes}'s different prediction functions).

To illustrate why predicting things with this multinomial model is so weird, we'll first predict the probability that someone chooses a \$30,000 6-seater electric van with 2 feet of storage space. For this combination of minivan characteristics, there's a 66% chance that someone does not select it, shown as category 0. That means there's a 33% chance that someone *does* select it. Because options 1, 2 and 3 were randomized, that 33% is split evenly across categories 1, 2, and 3 in the predictions here.

```{r create-show-one-prediction}
one_prediction <- model_minivans_categorical_brms %>% 
  epred_draws(newdata = data.frame(
    seat = "6", cargo = "2ft", eng = "elec", price = "30", resp.id = 1)
  )

one_prediction %>% 
  group_by(.category) %>% 
  median_qi(.epred)
```

We could add the predictions for categories 1, 2, and 3 together, but that would take a bit of extra data manipulation work. Instead, we can rely on the the fact that the prediction for category 0 is actually the inverse of the sum of categories 1+2+3, so we can instead just use `1 - .epred` and only look at category 0. Even though the `category` column here says `0`, it's really the combined probability of choosing options 1, 2, or 3:

```{r show-one-prediction-reversed}
one_prediction %>% 
  mutate(.epred = 1 - .epred) %>%
  filter(.category == 0) %>% 
  median_qi(.epred)
```

With {mlogit}, we found AMCEs by essentially calculating marginal means for specific contrasts of predicted probabilities. We created a data frame of all 54 combinations of feature levels and then grouped and summarized that data frame as needed (e.g., the average of the 27 predictions for 2 feet of cargo space and the average of the 27 predictions for 3 feed of cargo space).

We can do the same thing with the {brms} model, but selecting only the 0 category and reversing the predicted value:

```{r create-all-combos-minivans}
newdata_all_combos <- minivans %>% 
  tidyr::expand(seat, cargo, eng, price) %>% 
  mutate(resp.id = 1)

all_preds_brms <- model_minivans_categorical_brms %>% 
  epred_draws(newdata = newdata_all_combos) %>% 
  filter(.category == 0) %>% 
  mutate(.epred = 1 - .epred)
```

To make sure it worked, here are the posterior medians for all the different levels. It's roughly the same as what we found with in `all_preds_mlogit`:

```{r preds-show-all-combos-minivans}
all_preds_brms %>% 
  group_by(seat, cargo, eng, price) %>% 
  median_qi(.epred)
```

To pull out specific group-level averages, we can group and summarize. For example, here are the posterior median predictions for the two levels of cargo space:

```{r preds-cargo-wrong}
all_preds_brms %>% 
  group_by(cargo) %>% 
  median_qi(.epred)
```

The medians here are correct and basically what we found with {mlogit}, but the credible intervals are *wildly* off (5% to 75% favorability?!). If we plot this we can see why:

```{r plot-preds-combo-wrong}
all_preds_brms %>% 
  ggplot(aes(x = .epred, y = cargo, fill = cargo)) +
  stat_halfeye() +
  scale_x_continuous(labels = label_percent()) +
  scale_fill_manual(values = clrs[c(11, 7)], guide = "none") +
  labs(x = "Marginal means", y = "Cargo space")
```

hahahaha check out those mountain ranges. All those peaks come from combining the 27 different 2ft- and 3ft- posterior distributions for all the different combinations of other feature levels. 

To get the actual marginal mean for cargo space, we need to marginalize out (or average out) all those other covariates. To do this, we need to group by the `cargo` column *and* the `.draw` column so that we find the average within each set of MCMC draws. To help with the intuition, look how many rows are in each of these groups of `cargo` and `.draw`—there are 27 different estimates for each of the 4,000 draws for 2 feet and 27 different estimates for each of the 4,000 draws for 3 feet. We want to collapse (or marginalize) those 27 rows into just one average. 

```{r show-draws-in-cargo}
all_preds_brms %>% 
  group_by(cargo, .draw) %>% 
  summarize(nrows = n())
```

To do that, we can find the average predicted value in those groups, then work with that as our main estimand. Check out these marginalized-out posteriors now—the medians are the same as before, but the credible intervals make a lot more sense:

```{r preds-cargo-marginalized-correct}
preds_cargo_marginalized <- all_preds_brms %>% 
  # Marginalize out the other covariates
  group_by(cargo, .draw) %>%
  summarize(avg = mean(.epred))

preds_cargo_marginalized %>% 
  group_by(cargo) %>% 
  median_qi()
```

We can confirm that marginalizing out the other covariates worked by plotting it:

```{r plot-preds-cargo-marginalized}
preds_cargo_marginalized %>% 
  ggplot(aes(x = avg, y = cargo, fill = cargo)) +
  stat_halfeye() +
  scale_x_continuous(labels = label_percent()) +
  scale_fill_manual(values = clrs[c(11, 7)], guide = "none") +
  labs(x = "Marginal means", y = "Cargo space")
```

heck. yes.

Finally, we're actually most interested in the AMCE, or the difference between these two cargo sizes. The `compare_levels()` function from {tidybayes} can calculate this automatically:

```{r show-preds-diffs-cargo}
preds_cargo_marginalized %>%
  compare_levels(variable = avg, by = cargo, comparison = "control") %>% 
  median_qi(avg)
```

That's it! The **causal effect** of moving from 2 feet → 3 feet of storage space, holding all other features constant, is 8 percentage points (with a 95% credible interval of 6.5 to 10 percentage points).

We can combine all these AMCEs into a huge data frame. The marginalization process + `compare_levels()` has to happen with one feature at a time, so we need to create several separate data frames:

```{r show-preds-minivan-all-marginalized}
# I could probably do this with purrr::map() to reduce all this repetition, but
# whatever, it works
amces_minivan_brms <- bind_rows(
  seat = all_preds_brms %>% 
    group_by(seat, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = seat, comparison = "control") %>% 
    rename(contrast = seat),
  cargo = all_preds_brms %>% 
    group_by(cargo, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = cargo, comparison = "control") %>% 
    rename(contrast = cargo),
  eng = all_preds_brms %>% 
    group_by(eng, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = eng, comparison = "control") %>% 
    rename(contrast = eng),
  price = all_preds_brms %>% 
    group_by(price, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = price, comparison = "control") %>% 
    rename(contrast = price),
  .id = "term"
)

amces_minivan_brms %>% 
  group_by(term, contrast) %>% 
  median_qi(avg)
```


### Plots

Again, plotting these AMCEs so that there's a reference category at 0 requires some extra data work, so I've hidden all that code for the sake of space.

```{r extract-minivan-variable-levels}
#| code-fold: true
#| code-summary: "Extract variable labels"
minivan_var_levels <- tibble(
  variable = c("seat", "cargo", "eng", "price")
) %>% 
  mutate(levels = map(variable, ~{
    x <- minivans[[.x]]
    if (is.numeric(x)) {
      ""
    } else if (is.factor(x)) {
      levels(x)
    } else {
      sort(unique(x))
    }
  })) %>% 
  unnest(levels) %>% 
  mutate(term = paste0(variable, levels))

# Make a little lookup table for nicer feature labels
minivan_var_lookup <- tribble(
  ~variable, ~variable_nice,
  "seat",    "Passengers",
  "cargo",   "Cargo space",
  "eng",     "Engine type",
  "price",   "Price (thousands of $)"
) %>% 
  mutate(variable_nice = fct_inorder(variable_nice))
```

```{r plot-amces-minivans-mlogit}
#| code-fold: true
#| code-summary: "Combine full dataset of factor levels with model comparisons and make {mlogit} plot"
amces_minivan_mlogit_split <- amces_minivan_mlogit %>% 
  separate_wider_delim(
    term,
    delim = " - ", 
    names = c("variable_level", "reference_level")
  ) %>% 
  rename(term = variable)

plot_data <- minivan_var_levels %>%
  left_join(
    amces_minivan_mlogit_split,
    by = join_by(variable == term, levels == variable_level)
  ) %>% 
  # Make these zero
  mutate(
    across(
      c(estimate, conf.low, conf.high),
      ~ ifelse(is.na(.x), 0, .x)
    )
  ) %>% 
  left_join(minivan_var_lookup, by = join_by(variable)) %>% 
  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))

p1 <- ggplot(
  plot_data,
  aes(x = estimate, y = levels, color = variable_nice)
) +
  geom_vline(xintercept = 0) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  scale_x_continuous(labels = label_pp) +
  scale_color_manual(values = clrs[c(3, 7, 8, 9)], guide = "none") +
  labs(
    x = "Percentage point change in\nprobability of minivan selection",
    y = NULL,
    title = "Frequentist AMCEs from {mlogit}"
  ) +
  facet_col(facets = "variable_nice", scales = "free_y", space = "free")
```

```{r plot-amces-minivans-brms}
#| code-fold: true
#| code-summary: "Combine full dataset of factor levels with marginalized posterior draws and make {brms} plot"
posterior_mfx_minivan_nested <- amces_minivan_brms %>% 
  separate_wider_delim(
    contrast,
    delim = " - ", 
    names = c("variable_level", "reference_level")
  ) %>% 
  group_by(term, variable_level) %>% 
  nest()

plot_data_minivan_bayes <- minivan_var_levels %>%
  left_join(
    posterior_mfx_minivan_nested,
    by = join_by(variable == term, levels == variable_level)
  ) %>%
  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %>% 
  unnest(data) %>% 
  left_join(minivan_var_lookup, by = join_by(variable)) %>% 
  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))

p2 <- ggplot(plot_data_minivan_bayes, aes(x = avg, y = levels, fill = variable_nice)) +
  geom_vline(xintercept = 0) +
  stat_halfeye(normalize = "groups") +  # Make the heights of the distributions equal within each facet
  facet_col(facets = "variable_nice", scales = "free_y", space = "free") +
  scale_x_continuous(labels = label_pp) +
  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = "none") +
  labs(
    x = "Percentage point change in\nprobability of minivan selection",
    y = NULL,
    title = "Posterior Bayesian AMCEs from {brms}"
  )
```


```{r plot-amces-minivans-both, fig.height=4.5, fig.width=8, out.width="100%"}
#| column: body-outset
p1 + p2
```

\ 

# Part 3: Minivans; repeated questions; full hierarchical multinomial logit

## The setup

We've cheated a little and have already used multilevel structures in the Bayesian models for the chocolate experiment and the minivan experiment. This was because these datasets had a natural panel grouping structure inside them. {mlogit} can work with panel-indexed data frames (that's the point of that strange `dfidx()` function). By creating respondent-specific intercepts like we did with the {brms} models, we helped account for some of the variation caused by respondent differences.

But we can do better than that and get far richer and more complex models and estimates and predictions. In addition to using respondent-specific intercepts, we can (1) include respondent-level characteristics as covariates, and (2) include respondent-specific slopes for the minivan characteristic.

In the minivan data, we have data on feature levels (`seat`, `cargo`, `eng`, `price`) *and* on individual characteristics (`carpool`). The `carpool` variable indicates if the respondent uses their vehicle for carpooling. This is measured at the respondent level and not the choice level (i.e. someone won't stop being a carpooler during one set of choices and then resume being a carpooler for another set). 
We can visualize where these different columns are measured by returning to the hierarchical model diagram:

```{tikz minivan-multilevel-structure-annotated, engine.opts=font_opts}
#| echo: false
#| fig-cap: "Multilevel experimental structure, with minivan choices $\\{y_1, y_2, y_3\\}$ nested in sets of questions in respondents, w ith variables measured at different levels"
#| fig-align: center
#| fig-ext: svg
#| column: page
#| out-width: 100%
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{backgrounds}
\definecolor{indiv}{HTML}{5F4690}
\definecolor{indiv1}{HTML}{CBBAF7}
\definecolor{choice}{HTML}{E17C05}
\definecolor{choice1}{HTML}{F7C694}
\begin{tikzpicture}[{every node/.append style}=draw]
  \node [rectangle, draw=none, fill=indiv1] (respondent1) at (0, 2.5) {Respondent 1};

  \node [rectangle] (q11) at (-1.5, 1) {Set $1_1$};
  \node [draw=none] (q21) at (0, 1) {$\dots$};
  \node [rectangle] (q31) at (1.5, 1) {Set $15_1$};

  \node [circle, fill=choice1, draw=none] (y111) at (-3.75, -0.5) {$y_{1_{1_1}}$};
  \node [circle, fill=choice1, draw=none] (y211) at (-2.5, -0.5) {$y_{2_{1_1}}$};
  \node [circle, fill=choice1, draw=none] (y311) at (-1.25, -0.5) {$y_{3_{1_1}}$};

  \node [draw=none] (y121) at (0, -0.5) {$\dots$};

  \node [circle, fill=choice1, draw=none] (y131) at (1.25, -0.5) {$y_{1_{15_1}}$};
  \node [circle, fill=choice1, draw=none] (y231) at (2.5, -0.5) {$y_{2_{15_1}}$};
  \node [circle, fill=choice1, draw=none] (y331) at (3.75, -0.5) {$y_{3_{15_1}}$};

  \draw [-latex] (respondent1) to (q11);
  \draw [-latex] (respondent1) to (q21);
  \draw [-latex] (respondent1) to (q31);
  \draw [-latex] (q11) to (y111);
  \draw [-latex] (q11) to (y211);
  \draw [-latex] (q11) to (y311);
  \draw [-latex] (q21) to (y121);
  \draw [-latex] (q31) to (y131);
  \draw [-latex] (q31) to (y231);
  \draw [-latex] (q31) to (y331);

  \node [draw=none] (respondent2) at (5, 2.5) {$\dots$};
  \node [rectangle] (y12) at (4, 1) {Set 1};
  \node [draw=none] (y22) at (5, 1) {$\dots$};
  \node [rectangle] (y32) at (6, 1) {Set 15};
  \draw [-latex] (respondent2) to (y12);
  \draw [-latex] (respondent2) to (y22);
  \draw [-latex] (respondent2) to (y32);

  \node [rectangle, draw=none, fill=indiv1] (respondentn) at (10, 2.5) {Respondent 200};
  \node [rectangle] (q1n) at (8.5, 1) {Set $1_{200}$};
  \node [draw=none] (q2n) at (10, 1) {$\dots$};
  \node [rectangle] (q3n) at (11.5, 1) {Set $15_{200}$};

  \node [circle, fill=choice1, draw=none] (y113) at (6.25, -0.5) {$y_{1_{1_{200}}}$};
  \node [circle, fill=choice1, draw=none] (y213) at (7.5, -0.5) {$y_{2_{1_{200}}}$};
  \node [circle, fill=choice1, draw=none] (y313) at (8.75, -0.5) {$y_{3_{1_{200}}}$};

  \node [draw=none] (y123) at (10, -0.5) {$\dots$};

  \node [circle, fill=choice1, draw=none] (y133) at (11.25, -0.5) {$y_{1_{15_{200}}}$};
  \node [circle, fill=choice1, draw=none] (y233) at (12.5, -0.5) {$y_{2_{15_{200}}}$};
  \node [circle, fill=choice1, draw=none] (y333) at (13.75, -0.5) {$y_{3_{15_{200}}}$};

  \draw [-latex] (respondentn) to (q1n);
  \draw [-latex] (respondentn) to (q2n);
  \draw [-latex] (respondentn) to (q3n);
  \draw [-latex] (q1n) to (y113);
  \draw [-latex] (q1n) to (y213);
  \draw [-latex] (q1n) to (y313);
  \draw [-latex] (q2n) to (y123);
  \draw [-latex] (q3n) to (y133);
  \draw [-latex] (q3n) to (y233);
  \draw [-latex] (q3n) to (y333);

  \node [rectangle] (population) at (5, 4) {Population of experiment respondents};
  \draw [-latex] (population) to (respondent1);
  \draw [-latex] (population) to (respondent2);
  \draw [-latex] (population) to (respondentn);

  \node [rounded corners, draw=none, align=center, fill=indiv, text=white] 
    (note_respondent) at (-0.5, 3.75) 
    {\textsf{Things measured here:}\\\texttt{carpool}};

  \node [rounded corners, draw=none, align=center, fill=choice, text=white] 
    (note_choice) at (0, -2.25) 
    {\textsf{Things measured here:}\\\texttt{seat}, \texttt{cargo}, \texttt{eng}, \texttt{price}};

  \begin{scope}[on background layer]
    \draw [-, color=indiv, ultra thick] (note_respondent.center) to (respondent1);
    \draw [-, color=indiv, thin] (note_respondent.center) to (respondentn);

    \draw [-, color=choice, thick] (note_choice.center) to (y111);
    \draw [-, color=choice, thick] (note_choice.center) to (y211);
    \draw [-, color=choice, thick] (note_choice.center) to (y311);
    \draw [-, color=choice, thick] (note_choice.center) to (y131);
    \draw [-, color=choice, thick] (note_choice.center) to (y231);
    \draw [-, color=choice, thick] (note_choice.center) to (y331);
  
    \draw [-, color=choice, ultra thin] (note_choice.center) to (y113.south);
    \draw [-, color=choice, ultra thin] (note_choice.center) to (y213.south);
    \draw [-, color=choice, ultra thin] (note_choice.center) to (y313.south);
    \draw [-, color=choice, ultra thin] (note_choice.center) to (y133.south);
    \draw [-, color=choice, ultra thin] (note_choice.center) to (y233.south);
    \draw [-, color=choice, ultra thin] (note_choice.center) to (y333.south);
  \end{scope}
\end{tikzpicture}
```

We can use hierarchical models (or multilevel models, or mixed effects models, or whatever you want to call them) to account for choice-level and respondent-level covariates *and* incorporate respondent-level heterogeneity and covariance into the model estimates.

![Image by [Chelsea Parlett-Pelleriti](https://twitter.com/chelseaparlett/status/1458461737431146500)](img/chelsea-meme.jpg){width=50%}

## Important sidenote on notation

But before looking at how to incorporate that `carpool` column into the model, we need to take a quick little detour into the world of notation. There's no consistent way of writing out multilevel models,^[These are not the only approaches—section 12.5 in @GelmanHill:2007 is called "Five ways to write the same model," and they don't include the offset notation as one of their five!] and accordingly, I thought it was impossible to run fully specified marketing-style hierarchical Bayesian models with {brms}—all because of notation!

There are a couple general ways I've seen group-level random effects written out in formal model notation: one with complete random β terms and one with random offsets from a global β term.

::: {.callout-tip}
### {brms} / {lme4} syntax

For the best overview of how to use {brms} and {lme4} with different random group-level intercept and slope specifications, [check out this summary table by Ben Bolker](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification).
:::

### Random intercepts

If you want group-specific intercept terms, you can use a formula like this:

``` r
bf(y ~ x + (1 | group))
```

In formal mathy terms, we can write this group-specific intercept as a complete coefficient: $\beta_{0_j}$. Each group $j$ gets its own intercept coefficient. Nice and straightforward.

$$
\begin{aligned}
Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) & \text{Outcome for individual}_i \text{ within group}_j \\
\mu_{i_j} &= \beta_{0_j} + \beta_1 X_{i_j} & \text{Linear model of within-group variation } \\
\beta_{0_j} &\sim \mathcal{N}(\beta_0, \sigma_0) & \text{Random group-specific intercepts}
\end{aligned}
$$

However, I actually like to think of these random effects in a slightly different way, where each group intercept is actually a combination of a global average ($\beta_0$) and a group-specific offset from that average ($b_{0_j}$), like this:

$$
\beta_{0_j} = \beta_0 + b_{0_j}
$$

That offset is assumed to be normally distributed with a mean of 0 ($\mathcal{N}(0, \sigma_0)$):

$$
\begin{aligned}
Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) & \text{Outcome for individual}_i \text{ within group}_j \\
\mu_{i_j} &= (b_{0_j} + \beta_0) + \beta_1 X_{i_j} & \text{Linear model of within-group variation } \\
b_{0_j} &\sim \mathcal{N}(0, \sigma_0) & \text{Random group-specific offsets from global intercept}
\end{aligned}
$$

I prefer this offset notation because it aligns with the output of {brms}, which reports population-level coefficients (i.e. the global average $\beta_0$) along with group-specific offsets from that average (i.e. $b_{0_j}$), which you can access with `ranef(model_name)`.


### Random slopes

If you want group-specific intercepts *and* slopes, you can use a formula like this:

``` r
bf(y ~ x + (1 + x | group))
```

The same dual syntax applies when using random slopes too. We can either use whole group-specific $\beta_{n_j}$ terms, or use offsets ($b_{n_j}$) from a global average slope ($\beta_n$). When working with random slopes, the math notation gets a little fancier because the random intercept and slope terms are actually correlated and move together across groups. The $\beta$ terms come from a multivariate (or joint) normal distribution with shared covariance. 

With the complete β approach, we're estimating the joint distribution of $\begin{pmatrix} \beta_{0_j} \\ \beta_{1_j} \end{pmatrix}$:

::: {.column-page-inset}

$$
\begin{aligned}
Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) & \text{Outcome for individual}_i \text{ within group}_j \\
\mu_{i_j} &= \beta_{0_j} + \beta_{1_j} X_{i_j} & \text{Linear model of within-group variation } \\
\left(
  \begin{array}{c}
  \beta_{0_j} \\
  \beta_{1_j}
  \end{array}
\right)
&\sim \operatorname{MV}\, \mathcal{N}
\left(
  \left(
    \begin{array}{c}
    \beta_0 \\
    \beta_1 \\
    \end{array}
  \right)
  , \,
\left(
  \begin{array}{cc}
     \sigma^2_{\beta_0} & \rho_{\beta_0, \beta_1}\, \sigma_{\beta_0} \sigma_{\beta_1} \\
     \dots & \sigma^2_{\beta_1}
  \end{array}
\right)
\right) & \text{Random group-specific slopes and intercepts}
\end{aligned}
$$

:::

With the offset approach, we're estimating the joint distribution of the offsets from the global intercept and slope, or $\begin{pmatrix} b_{0_j} \\ b_{1_j} \end{pmatrix}$:

::: {.column-page-inset}

$$
\begin{aligned}
Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) & \text{Outcome for individual}_i \text{ within group}_j \\
\mu_{i_j} &= (b_{0_j} + \beta_0) + (b_{1_j} + \beta_1) X_{i_j} & \text{Linear model of within-group variation } \\
\left(
  \begin{array}{c}
  b_{0_j} \\
  b_{1_j}
  \end{array}
\right)
&\sim \operatorname{MV}\, \mathcal{N}
\left(
  \left(
    \begin{array}{c}
    0 \\
    0 \\
    \end{array}
  \right)
  , \,
\left(
  \begin{array}{cc}
     \sigma^2_{\beta_0} & \rho_{\beta_0, \beta_1}\, \sigma_{\beta_0} \sigma_{\beta_1} \\
     \dots & \sigma^2_{\beta_1}
  \end{array}
\right)
\right) & \text{Random group-specific offsets from global intercept and slope}
\end{aligned}
$$

:::


### Summary table

And here's a helpful little table summarizing these two types of notation (mostly for future me).

::: {.column-page-inset}

|                          | Formula syntax            |                                                                                                                                                                                                                                                           Full $\beta$ notation                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                   Offset notation                                                                                                                                                                                                                                                                                                   |
|----------------|----------------|:------------------:|:------------------:|
| Random intercept         | `y ~ x + (1 | group)`     | $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                                                                  \begin{aligned}                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                     Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) \\                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                       \mu_{i_j} &= \beta_{0_j} + \beta_1 X_{i_j} \\                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                      \beta_{0_j} &\sim \mathcal{N}(\beta_0, \sigma_0)                                                                                                                                                                                                                                              
                                                                                                                                       \end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                                                        $$                                                                                                                                                                                                                                          |                                        $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \begin{aligned} 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) \\                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \mu_{i_j} &= (b_{0_j} + \beta_0) + \beta_1 X_{i_j} \\                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            b_{0_j} &\sim \mathcal{N}(0, \sigma_0)                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                $$        |
| Random intercept + slope | `y ~ x + (1 + x | group)` | $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                                                                  \begin{aligned}                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                     Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) \\                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                     \mu_{i_j} &= \beta_{0_j} + \beta_{1_j} X_{i_j} \\                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                           \left(                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                       \begin{array}{c}                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                        \beta_{0_j} \\                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                         \beta_{1_j}                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                         \end{array}                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                          \right)                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                           &\sim \operatorname{MV}\, \mathcal{N}                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                           \left(                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                            \left(                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                        \begin{array}{c}                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                           \beta_0 \\                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                           \beta_1 \\                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                          \end{array}                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                           \right)                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                         , \, \Sigma                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                         \right) \\                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                        \Sigma &\sim                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                           \left(                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                      \begin{array}{cc}                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                       \sigma^2_{\beta_0} & \rho_{\beta_0, \beta_1}\, \sigma_{\beta_0} \sigma_{\beta_1} \\                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                   \dots & \sigma^2_{\beta_1}                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                         \end{array}                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                          \right)                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                      \end{aligned}                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                                                        $$                                                                                                                                                                                                                                          |                                        $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \begin{aligned} 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\sim \mathcal{N}(\mu_{i_j}, \sigma_y) \\                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \mu_{i_j} &= (b_{0_j} + \beta_0) + (b_{1_j} + \beta_1) X_{i_j} \\                                                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \left(                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \begin{array}{c}                                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           b_{0_j} \\                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             b_{1_j}                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \end{array}                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \right)                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             &\sim \operatorname{MV}\, \mathcal{N}                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \left(                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \left(                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \begin{array}{c}                                                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \end{array}                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \right)                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           , \, \Sigma                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \right) \\                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \Sigma &\sim                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \left(                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \begin{array}{cc}                                                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \sigma^2_{\beta_0} & \rho_{\beta_0, \beta_1}\, \sigma_{\beta_0} \sigma_{\beta_1} \\                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \dots & \sigma^2_{\beta_1}                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \end{array}                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \right)                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \end{aligned}                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                $$        |

:::


## Translating from marketing-style Stan notation to {brms} syntax

In @ChapmanFeit:2019 and in all the marketing papers I've seen that use hierarchical Bayesian models—and even one I coauthored! [@ChaudhryDotsonHeiss:2021] ([see the appendix](https://stats.andrewheiss.com/who-cares-about-crackdowns/output/appendix.html#model-details))—they define their models using notation like this:

$$
\begin{aligned}
\text{Choice} &\sim \operatorname{Multinomial\ logit}(\beta X) & \text{where } X = \text{feature levels} \\
\beta &\sim \operatorname{Multivariate} \mathcal{N}(\gamma Z, \Sigma) & \text{where } Z = \text{individual characteristics}
\end{aligned}
$$

For the longest time this threw me off because it's slightly different from the two different notations we just reviewed (full βs vs. offsets from global β), and I figured that specifying a model like this with {brms} was impossible. The main reason for my confusion is that there are two different datasets involved in this model, and {brms} can only really work with one dataset. 

In raw Stan (like [in this tutorial on conjoint hierarchical Bayes models](https://github.com/ksvanhorn/ART-Forum-2017-Stan-Tutorial), or in [this example of a different conjoint hierarchical model](https://github.com/Bartosz-G/Conjoint-Analysis-in-R/)), you'd typically work with two different datasets or matrices: one $X$ with feature levels and one $Z$ with respondent-level characteristics. ([This is actually the recommended way to write hierarchical models in raw Stan!](https://mc-stan.org/docs/stan-users-guide/multivariate-hierarchical-priors.html)).

Here's what separate $X$ and $Z$ matrices would look like with the minivan data—`X` contains the full data without respondent-level covariates like `carpool` and it has 9,000 rows; `Z` contains only respondent-level characteristics like `carpool` and it only has 200 rows (one per respondent).

```{r show-x-z}
X <- minivans %>% select(-carpool)
X

Z <- minivans %>% 
  # Only keep the first row of each respondent
  group_by(resp.id) %>% slice(1) %>% ungroup() %>% 
  # Only keep the respondent-level columns
  select(resp.id, carpool)
Z
```

`X` and `Z` are then passed to Stan as separate matrices and used at different places in the model fitting process. Here's what that looks like in pseudo-Stan code. The matrix of individual characteristics `Z` is matrix-multiplied with a bunch of estimated $\gamma$ coefficients (`Gamma` here) to generate individual-specific $\beta$ coefficients (`Beta` here). The matrix of choices `X` is then matrix-multiplied with the individual-specific $\beta$ coefficients to generate predicted outcomes (`Y` here). 

``` stan
for (r in 1:n_respondents) {
  // All the individual-specific slopes and intercepts
  Beta[,r] ~ multi_normal(Gamma * Z[,r], ...);

  // All the question-level outcomes, using individual-specific slopes and intercepts
  for (s in 1:n_questions) {
     Y[r,s] ~ categorical_logit( X [r,s] * Beta[,r]);
  }
}
```

That's a neat way of working with multilevel models, but it's different from how I've always worked with them (and it requires working with raw Stan). As seen throughout this post, I'm a fan of {brms}'s formula-style syntax for specifying multilevel models, but {brms} can only work with one dataset at a time—you can't pass it both `X` and `Z` like you'd do with raw Stan. So I (naively) figured that this went beyond {brms}'s abilities and was only possible with raw Stan.

However, if we use {brms}'s [special formula syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_multilevel.pdf), we can actually specify an identical model with only one dataset (again, [see this for a fantastic overview of the syntax](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification)).

First, let's look at the marketing-style syntax again:

$$
\begin{aligned}
\text{Choice} &\sim \operatorname{Multinomial\ logit}(\beta X) & \text{where } X = \text{feature levels} \\
\beta &\sim \operatorname{Multivariate} \mathcal{N}(\gamma Z, \Sigma) & \text{where } Z = \text{individual characteristics}
\end{aligned}
$$

This is actually just a kind of *really* compact notation. That second line with the $\beta \sim \operatorname{Multivariate}\, \mathcal{N}(\cdot)$ distribution is a shorthand version of the full-β syntax from earlier. To illustrate this, let's expand this out to a more complete formal definition of the model. Instead of using $X$ to stand in for all the feature levels and $Z$ for all the individual characteristics, we'll expand those to include all the covariates we're using. And instead of calling the distribution "Multinomial logit" we'll call it "Categorical" so it aligns with Stan. It'll make for a *really massive formula*, but it shows what's really going on.

::: {.column-page-inset}

$$
\begin{aligned}
&\ \textbf{Multinomial probability of selection of choice}_i \textbf{ in respondent}_j \\
\text{Choice}_{i_j} \sim&\ \operatorname{Categorical}(\{\mu_{1,i_j}, \mu_{2,i_j}, \mu_{3,i_j}\}) \\[10pt]
&\ \textbf{Model for probability of each option} \\
\{\mu_{1,i_j}, \mu_{2,i_j}, \mu_{3,i_j}\} =&\ \beta_{0_j} + \beta_{1_j} \text{Seat[7]}_{i_j} + \beta_{2_j} \text{Seat[8]}_{i_j} + \beta_{3_j} \text{Cargo[3ft]}_{i_j} + \\
&\ \beta_{4_j} \text{Engine[hyb]}_{i_j} + \beta_{5_j} \text{Engine[elec]}_{i_j} + \\
&\ \beta_{6_j} \text{Price[35k]}_{i_j} + \beta_{7_j} \text{Price[40k]}_{i_j} \\[20pt]  
&\ \textbf{Respondent-specific slopes} \\
\left(
  \begin{array}{c} 
    \begin{aligned}
      &\beta_{0_j} \\
      &\beta_{1_j} \\
      &\beta_{2_j} \\
      &\beta_{3_j} \\
      &\beta_{4_j} \\
      &\beta_{5_j} \\
      &\beta_{6_j} \\
      &\beta_{7_j}
    \end{aligned}
  \end{array}
\right) \sim&\ \operatorname{Multivariate}\ \mathcal{N} \left[
\left(
  \begin{array}{c} 
    \begin{aligned}
      &\gamma^{\beta_{0}}_{0} + \gamma^{\beta_{0}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{1}}_{0} + \gamma^{\beta_{1}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{2}}_{0} + \gamma^{\beta_{2}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{3}}_{0} + \gamma^{\beta_{3}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{4}}_{0} + \gamma^{\beta_{4}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{5}}_{0} + \gamma^{\beta_{5}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{6}}_{0} + \gamma^{\beta_{6}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{7}}_{0} + \gamma^{\beta_{7}}_{1}\text{Carpool}
    \end{aligned}
  \end{array}
\right)
, 
\left(
  \begin{array}{cccccccc}
     \sigma^2_{\beta_{0j}} & \rho_{\beta_{0j}\beta_{1j}} & \rho_{\beta_{0j}\beta_{2j}} & \rho_{\beta_{0j}\beta_{3j}} & \rho_{\beta_{0j}\beta_{4j}} & \rho_{\beta_{0j}\beta_{5j}} & \rho_{\beta_{0j}\beta_{6j}} & \rho_{\beta_{0j}\beta_{7j}} \\ 
     \dots & \sigma^2_{\beta_{1j}} & \rho_{\beta_{1j}\beta_{2j}} & \rho_{\beta_{1j}\beta_{3j}} & \rho_{\beta_{1j}\beta_{4j}} & \rho_{\beta_{1j}\beta_{5j}} & \rho_{\beta_{1j}\beta_{6j}} & \rho_{\beta_{1j}\beta_{7j}} \\ 
     \dots & \dots & \sigma^2_{\beta_{2j}} & \rho_{\beta_{2j}\beta_{3j}} & \rho_{\beta_{2j}\beta_{4j}} & \rho_{\beta_{2j}\beta_{5j}} & \rho_{\beta_{2j}\beta_{6j}} & \rho_{\beta_{2j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \sigma^2_{\beta_{3j}} & \rho_{\beta_{3j}\beta_{4j}} & \rho_{\beta_{3j}\beta_{5j}} & \rho_{\beta_{3j}\beta_{6j}} & \rho_{\beta_{3j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \sigma^2_{\beta_{4j}} & \rho_{\beta_{4j}\beta_{5j}} & \rho_{\beta_{4j}\beta_{6j}} & \rho_{\beta_{4j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \dots & \sigma^2_{\beta_{5j}} & \rho_{\beta_{5j}\beta_{6j}} & \rho_{\beta_{5j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \dots & \dots & \sigma^2_{\beta_{6j}} & \rho_{\beta_{6j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \dots & \dots & \dots & \sigma^2_{\beta_{7j}}
  \end{array}
\right)
\right]
\end{aligned}
$$
:::

\ 

Importantly, pay attention to where the choice-level and respondent-level variables show up in this expanded version. All the choice-level variables have respondent-specific $\beta$ coefficients, while the respondent-level variable (`carpool`) is down in that massive multivariate normal matrix with its own $\gamma$ coefficients, helping determine the respondent-specific $\beta$ coefficients. That's great and exactly what we want, and we can do that with raw Stan, but raw Stan is no fun.

We can create this exact same model structure with {brms} like this:

``` r
bf(choice_alt ~
  # Choice-level predictors that are nested within respondents...
  (seat + cargo + eng + price) *
  # ...interacted with all respondent-level predictors...
  (carpool) +
  # ... with random respondent-specific slopes for the
  # nested choice-level predictors
  (1 + seat + cargo + eng + price | resp.id))
```

We can confirm that this worked by using [the miraculous {equatiomatic} package](https://datalorax.github.io/equatiomatic/), which automatically converts model objects into LaTeX code. {equatiomatic} doesn't work with {brms} models, but it does work with frequentist {lme4} models, so we can fit a throwaway frequentist model with this syntax (it won't actually converge and it'll give a warning, but that's fine—we don't actually care about this model) and then feed it to `equatiomatic::extract_eq()` to see what it looks like in formal notation. 

(This is actually how I figured out the correct combination of interactions and random slopes—I kept trying different combinations that I thought were right until the math matched the huge full model above, with the $\beta$ and $\gamma$ terms in the right places.)


```{r lme4-equatiomatic, results="asis", warning=FALSE, message=FALSE, cache=TRUE}
#| column: page
library(lme4)
library(equatiomatic)

model_throwaway <- lmer(
  choice ~ (seat + cargo + eng + price) * (carpool) +
    (1 + seat + cargo + eng + price | resp.id),
  data = minivans
)

print(extract_eq(model_throwaway))
```


### Different variations of group-level interactions

This syntax is a special R shortcut for interacting `carpool` with each of the feature variables:

``` r
# Short way
(seat + cargo + eng + price) * (carpool)

# This expands to this
seat*carpool + cargo*carpool + eng*carpool + price*carpool
```

If we had other respondent-level columns like age (`age`) and education (`ed`), the shortcut syntax is really helpful:

``` r
# Short way
(seat + cargo + eng + price) * (carpool + age + ed)

# This expands to this
seat*carpool + cargo*carpool + eng*carpool + price*carpool +
seat*age + cargo*age + eng*age + price*age +
seat*ed + cargo*ed + eng*ed + price*ed
```

We don't necessarily need to fully interact everything. For instance, if we have theoretical reasons to think that carpool status is associated with seat count preferences, but not other features, we can only interact `seat` and `carpool`:

``` r
bf(choice ~ 
    (seat * carpool) + cargo + eng + price + 
  (1 + seat + cargo + eng + price | resp.id))
```

::: {.callout-warning}
### Model running times

The more individual-level interactions you add, the longer it will take for the model to run. As we'll see below, interacting `carpool` with the four feature levels takes ≈30 minutes to fit. As you add more individual-level interactions, the running time blows up.

In the replication code for @JensenMarbleScheve:2021, where they model a ton of individual variables, they say it takes several days to run. Our models in @ChaudhryDotsonHeiss:2021 take hours and hours to run.
:::


## `mlogit` model

{mlogit} can estimate hierarchical models with something like this: 

```{r model-hierarchical-mlogit, eval=FALSE}
# Define the random parameters
model_mlogit_rpar <- rep("n", length = length(model_minivans_mlogit$coef))
names(model_mlogit_rpar) <- names(model_minivans_mlogit$coef)

# This means these random terms are all normally distributed
model_mlogit_rpar
#> seat7    seat8 cargo3ft   enghyb  engelec  price35  price40 
#>   "n"      "n"      "n"      "n"      "n"      "n"      "n" 

# Run the model with carpool as an individual-level covariate
model_mlogit_hierarchical <- mlogit(
  choice ~ 0 + seat + eng + cargo + price | carpool,
  data = minivans_idx,
  panel = TRUE, rpar = model_mlogit_rpar, correlation = TRUE
)

# Show the results
summary(model_mlogit_hierarchical)
```

However, I'm not a frequentist and I'm already not a huge fan of extracting the predictions and AMCEs from these {mlogit} models. Running and interpreting and working with the results of that object is left as an exercise for the reader :). (See p. 381 in @ChapmanFeit:2019 for a worked example of how to do it.)


## Finally, the full {brms} model

This is a really complex model with a ton of moving parts, but it's also incredibly powerful. It lets us account for individual-specific differences across each of the minivan features. For instance, whether an individual carpools probably influences their preferences for the number of seats, and maybe cargo space, but probably doesn't influence their preferences for engine type. If we had other individual-level characteristics, we could also let those influence feature preferences. Like, the number of kids an individual has probably influences seat count preferences; the individual's income probably influences their price preferences; and so on.

Let's define the model more formally again, this time with priors for the parameters we'll be estimating:

::: {.column-page-inset}

$$
\begin{aligned}
&\ \textbf{Multinomial probability of selection of choice}_i \textbf{ in respondent}_j \\
\text{Choice}_{i_j} \sim&\ \operatorname{Categorical}(\{\mu_{1,i_j}, \mu_{2,i_j}, \mu_{3,i_j}\}) \\[10pt]
&\ \textbf{Model for probability of each option} \\
\{\mu_{1,i_j}, \mu_{2,i_j}, \mu_{3,i_j}\} =&\ \beta_{0_j} + \beta_{1_j} \text{Seat[7]}_{i_j} + \beta_{2_j} \text{Seat[8]}_{i_j} + \beta_{3_j} \text{Cargo[3ft]}_{i_j} + \\
&\ \beta_{4_j} \text{Engine[hyb]}_{i_j} + \beta_{5_j} \text{Engine[elec]}_{i_j} + \\
&\ \beta_{6_j} \text{Price[35k]}_{i_j} + \beta_{7_j} \text{Price[40k]}_{i_j} \\[20pt]  
&\ \textbf{Respondent-specific slopes} \\
\left(
  \begin{array}{c} 
    \begin{aligned}
      &\beta_{0_j} \\
      &\beta_{1_j} \\
      &\beta_{2_j} \\
      &\beta_{3_j} \\
      &\beta_{4_j} \\
      &\beta_{5_j} \\
      &\beta_{6_j} \\
      &\beta_{7_j}
    \end{aligned}
  \end{array}
\right) \sim&\ \operatorname{Multivariate}\ \mathcal{N} \left[
\left(
  \begin{array}{c} 
    \begin{aligned}
      &\gamma^{\beta_{0}}_{0} + \gamma^{\beta_{0}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{1}}_{0} + \gamma^{\beta_{1}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{2}}_{0} + \gamma^{\beta_{2}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{3}}_{0} + \gamma^{\beta_{3}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{4}}_{0} + \gamma^{\beta_{4}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{5}}_{0} + \gamma^{\beta_{5}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{6}}_{0} + \gamma^{\beta_{6}}_{1}\text{Carpool} \\
      &\gamma^{\beta_{7}}_{0} + \gamma^{\beta_{7}}_{1}\text{Carpool}
    \end{aligned}
  \end{array}
\right)
, 
\left(
  \begin{array}{cccccccc}
     \sigma^2_{\beta_{0j}} & \rho_{\beta_{0j}\beta_{1j}} & \rho_{\beta_{0j}\beta_{2j}} & \rho_{\beta_{0j}\beta_{3j}} & \rho_{\beta_{0j}\beta_{4j}} & \rho_{\beta_{0j}\beta_{5j}} & \rho_{\beta_{0j}\beta_{6j}} & \rho_{\beta_{0j}\beta_{7j}} \\ 
     \dots & \sigma^2_{\beta_{1j}} & \rho_{\beta_{1j}\beta_{2j}} & \rho_{\beta_{1j}\beta_{3j}} & \rho_{\beta_{1j}\beta_{4j}} & \rho_{\beta_{1j}\beta_{5j}} & \rho_{\beta_{1j}\beta_{6j}} & \rho_{\beta_{1j}\beta_{7j}} \\ 
     \dots & \dots & \sigma^2_{\beta_{2j}} & \rho_{\beta_{2j}\beta_{3j}} & \rho_{\beta_{2j}\beta_{4j}} & \rho_{\beta_{2j}\beta_{5j}} & \rho_{\beta_{2j}\beta_{6j}} & \rho_{\beta_{2j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \sigma^2_{\beta_{3j}} & \rho_{\beta_{3j}\beta_{4j}} & \rho_{\beta_{3j}\beta_{5j}} & \rho_{\beta_{3j}\beta_{6j}} & \rho_{\beta_{3j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \sigma^2_{\beta_{4j}} & \rho_{\beta_{4j}\beta_{5j}} & \rho_{\beta_{4j}\beta_{6j}} & \rho_{\beta_{4j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \dots & \sigma^2_{\beta_{5j}} & \rho_{\beta_{5j}\beta_{6j}} & \rho_{\beta_{5j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \dots & \dots & \sigma^2_{\beta_{6j}} & \rho_{\beta_{6j}\beta_{7j}} \\ 
     \dots & \dots & \dots & \dots & \dots & \dots & \dots & \sigma^2_{\beta_{7j}}
  \end{array}
\right)
\right] \\[10pt]
&\ \textbf{Priors} \\
\beta_{0 \dots 7} \sim&\ \mathcal{N} (0, 3) \qquad\qquad\ [\text{Prior for choice-level coefficients}] \\
\gamma^{\beta_{0 \dots 7}}_0 \sim&\ \mathcal{N} (0, 3) \qquad\qquad\ [\text{Prior for individual-level coefficients}] \\
\sigma_{\beta_{0 \dots 7}} \sim&\ \operatorname{Exponential}(1) \qquad [\text{Prior for between-respondent intercept and slope variability}] \\
\rho \sim&\ \operatorname{LKJ}(1) \qquad\qquad [\text{Prior for correlation between random slopes and intercepts}]
\end{aligned}
$$
:::

\ 

Here it is in code form. There are a couple new things here in the Stan settings. First, we're going to create 4 MCMC chains with 4,000 draws rather than 2,000—there are so many parameters to be estimated that we need to let the simulation run longer. Second, we've modified the `adapt_delta` setting to 0.99. Conceptually, this adjusts the size of the steps the MCMC algorithm takes as it traverses the posterior space for each parameter—higher numbers make the steps smaller and more granular. This slows down the MCMC simulation, but it also helps avoid divergent transitions (or failed out-of-bounds draws).

On my 2021 M1 MacBook Pro, running through cmdstanr with 2 CPU cores per chain, it took about 30 minutes to fit. If you're following along with this post, start running this and go get some lunch or go for a walk or something.

::: {.callout-tip}
### Pre-run model

Alternatively, you can download [an .rds file of this completed](https://osf.io/zp6eh). This `brm()` code load the .rds file automatically instead of rerunning the model as long as you put it in a folder named "models" in your working directory. This code uses the [{osfr} package](https://docs.ropensci.org/osfr/) to download [the .rds file from OSF](https://osf.io/zp6eh) automatically and places it where it needs to go:

```{r get-rds-osf-again, eval=FALSE}
library(osfr)  # Interact with OSF via R

# Make a "models" folder if it doesn't exist already
if (!file.exists("models")) { dir.create("models") }

# Download model_minivans_mega_mlm_brms.rds from OSF
osf_retrieve_file("https://osf.io/zp6eh") |>
  osf_download(path = "models", conflicts = "overwrite", progress = TRUE)
```
:::

```{r model-minivans-mega-mlm}
model_minivans_mega_mlm_brms <- brm(
  bf(choice_alt ~
    # Choice-level predictors that are nested within respondents...
    (seat + cargo + eng + price) *
    # ...interacted with all respondent-level predictors...
    (carpool) +
    # ... with random respondent-specific slopes for the
    # nested choice-level predictors
    (1 + seat + cargo + eng + price | ID | resp.id)),
  data = minivans_choice_alt,
  family = categorical(refcat = "0"),
  prior = c(
    prior(normal(0, 3), class = b, dpar = mu1),
    prior(normal(0, 3), class = b, dpar = mu2),
    prior(normal(0, 3), class = b, dpar = mu3),
    prior(exponential(1), class = sd, dpar = mu1),
    prior(exponential(1), class = sd, dpar = mu2),
    prior(exponential(1), class = sd, dpar = mu3),
    prior(lkj(1), class = cor)
  ),
  chains = 4, cores = 4, warmup = 1000, iter = 5000, seed = 1234,
  backend = "cmdstanr", threads = threading(2), # refresh = 0,
  control = list(adapt_delta = 0.9),
  file = "models/model_minivans_mega_mlm_brms"
)
```

This model is incredibly rich. We just estimated more than 5,000 parameters (!!!)—we have three sets of coefficients for each of the three options, and those are all interacted with `carpool`, plus we have individual-specific offsets to each of those coefficients, plus all the $\rho$ terms in that massive correlation matrix.

```{r show-number-variables-mega-model}
length(get_variables(model_minivans_mega_mlm_brms))
```

Since we're dealing with interaction terms, these raw log odds coefficients are even *less* helpful on their own. It's nearly impossible to interpret these coefficients in any meaningful way—there's no point in even trying to combine each of the individual parts of each effect (random parts, interaction parts, etc.). The only way we'll be able to interpret these things is by making predictions.

```{r marginalize-mlm-posterior-estimates}
minivans_mega_marginalized <- model_minivans_mega_mlm_brms %>% 
  gather_draws(`^b_.*$`, regex = TRUE) %>% 
  # Each variable name has "mu1", "mu2", etc. built in, like "b_mu1_seat6". This
  # splits the .variable column into two parts based on a regular expression,
  # creating one column for the mu part ("b_mu1_") and one for the rest of the
  # variable name ("seat6")
  separate_wider_regex(
    .variable,
    patterns = c(mu = "b_mu\\d_", .variable = ".*")
  ) %>% 
  # Find the average of the three mu estimates for each variable within each
  # draw, or marginalize across the three options, since they're randomized
  group_by(.variable, .draw) %>% 
  summarize(.value = mean(.value)) 

minivans_mega_marginalized %>% 
  group_by(.variable) %>% 
  median_qi(.value)
```


## Predictions

In the non-hierarchical model earlier, [we made marketing-style predictions](#bayesian-predictions-1) by thinking of a product mix and figuring out the predicted utility and market share of each product in that mix. We can do the same thing here, but now we can incorporate individual-level characteristics too.

Here's our example product mix again, but this time we'll repeat it twice—once with `carpool` set to "yes" and once with it set to "no". This will let us see the predicted market share for each mix of products for a market of only carpoolers and only non-carpoolers.

```{r create-product-mix-carpool}
example_product_mix <- tribble(
  ~seat, ~cargo, ~eng, ~price,
  "7", "2ft", "hyb", "30",
  "6", "2ft", "gas", "30",
  "8", "2ft", "gas", "30",
  "7", "3ft", "gas", "40",
  "6", "2ft", "elec", "40",
  "7", "2ft", "hyb", "35"
)

product_mix_carpool <- bind_rows(
  mutate(example_product_mix, carpool = "yes"),
  mutate(example_product_mix, carpool = "no")
) %>% 
  mutate(across(everything(), factor)) %>% 
  mutate(eng = factor(eng, levels = levels(minivans$eng)))
```

We can now go through [the same process from earlier](#bayesian-predictions-1) where we get logit-scale predictions for this smaller dataset and find the shares inside each draw + category (options 1, 2, and 3) + carpool status group. 

```{r create-shares-mega-model-carpool}
draws_df <- product_mix_carpool %>% 
  add_linpred_draws(model_minivans_mega_mlm_brms, value = "utility", re_formula = NA)

shares_df <- draws_df %>% 
  # Look at each set of predicted utilities within each draw within each of the
  # three outcomes across both levels of carpooling
  group_by(.draw, .category, carpool) %>% 
  mutate(share = exp(utility) / sum(exp(utility))) %>% 
  ungroup() %>% 
  mutate(
    mix_type = paste(seat, cargo, eng, price, sep = " "),
    mix_type = fct_reorder(mix_type, share)
  )
```

This new dataset contains 576,000 (!!) rows: 6 products × 2 carpool types × 3 $\mu$-specific sets of coefficients × 16,000 MCMC draws. We can summarize this to get posterior medians and credible intervals, making sure to find the average share across the three outcomes (choice 1, 2, and 3), or marginalizing across the outcome.

```{r show-shares-carpool}
shares_df %>% 
  # Marginalize across the three outcomes within each draw
  group_by(mix_type, carpool, .draw) %>% 
  summarize(share = mean(share)) %>% 
  median_qi(share)
```

And we can plot them:

```{r plot-shares-carpool, fig.width=7}
shares_df %>% 
  mutate(carpool = case_match(carpool, "no" ~ "Non-carpoolers", "yes" ~ "Carpoolers")) %>% 
  # Marginalize across the three outcomes within each draw
  group_by(mix_type, carpool, .draw) %>% 
  summarize(share = mean(share)) %>% 
  ggplot(aes(x = share, y = mix_type, slab_alpha = carpool)) +
  stat_halfeye(normalize = "groups", fill = clrs[10]) +
  scale_x_continuous(labels = label_percent()) +
  scale_slab_alpha_discrete(
    range = c(1, 0.4),
    guide = "none"
  ) +
  facet_wrap(vars(carpool)) +
  labs(x = "Predicted market share", y = "Hypothetical product mix")
```

This is so cool! In general, the market share for these six hypothetical products is roughly the same across carpoolers and non-carpoolers, with one obvious exception—among non-carpoolers, the \$30,000 8-passenger gas minivan with 2 feet of space has 26% of the market, while among carpoolers it has 42%. Individuals who carpool apparently *really* care about the number of passengers their vehicle can carry.


## AMCEs

To find the average marginal component effects (AMCEs), or the causal effect of moving one of these features to another value, holding all other variables constant, we can go through [the same process as before](#bayesian-comparisonscontrasts-1). We'll calculate the predicted probabilities of choosing option 0, 1, 2, and 3 across a full grid of all the combinations of feature levels *and* carpool status. We'll then filter those predictions to only look at option 0 and reverse the predicted probabilities. Again, that feels weird, but it's a neat little trick—if there's a 33% chance that someone will select a specific combination of features, that would imply a 66% chance of not selecting it and an 11% chance of selecting it when it appears in option 1, option 2, and option 3. Rather than adding the probabilities within those three options together, we can do 100% − 66% to get the same 33% value, only it's automatically combined.

Earlier we had 54 combinations—now we have 108 (54 × 2). We'll set `resp.id` to one that's not in the dataset (201) so that these effects all deal with a generic hypothetical respondent (we could also do some fancy "integrating out" work and find population-level averages; [see here for more about that](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/)).

```{r create-all-combos-carpool}
newdata_all_combos_carpool <- minivans %>% 
  tidyr::expand(seat, cargo, eng, price, carpool) %>% 
  mutate(resp.id = 201)
newdata_all_combos_carpool
```

Next we can plug this grid into the model, filter to only keep option 0, and reverse the predictions:

```{r generate-preds-all-combos-carpool}
all_preds_brms_carpool <- model_minivans_mega_mlm_brms %>% 
  epred_draws(
    newdata = newdata_all_combos_carpool,
    re_formula = NULL, allow_new_levels = TRUE
  ) %>% 
  filter(.category == 0) %>% 
  mutate(.epred = 1 - .epred)
```

This thing has 1.7 million rows in it, so we need to group and summarize to do anything useful with it. We also need to marginalize across all the other covariates when grouping (i.e. if we want the estimates for passenger seat count across carpool status, we need to average out all the other covariates).

To test that this worked, here are the posterior marginal means for seat count:

```{r show-marginal-means-seat-carpool}
preds_seat_carpool_marginalized <- all_preds_brms_carpool %>% 
  # Marginalize out the other covariates in each draw
  group_by(seat, carpool, .draw) %>% 
  summarize(avg = mean(.epred))

preds_seat_carpool_marginalized %>% 
  group_by(seat, carpool) %>% 
  median_qi(avg)
```

Those credible intervals all look reasonable (i.e. not ranging from 5% to 80% or whatever), but it's hard to see any trends from just this table. Let's plot it:

```{r plot-marginal-means-seat-carpool}
preds_seat_carpool_marginalized %>% 
  ggplot(aes(x = avg, y = seat)) +
  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) + 
  scale_x_continuous(labels = label_percent()) +
  scale_slab_alpha_discrete(
    range = c(0.4, 1),
    labels = c("Non-carpoolers", "Carpoolers"),
    guide = guide_legend(
      reverse = TRUE, override.aes = list(fill = "grey10"), 
      keywidth = 0.8, keyheight = 0.8
    )
  ) +
  labs(
    x = "Marginal means",
    y = NULL,
    slab_alpha = NULL
  ) +
  theme(
    legend.position = "top",
    legend.justification = "left",
    legend.margin = margin(l = -7, t = 0)
  )
```

Neat! The average posterior predicted probability of choosing six seats is substantially higher for carpoolers than for non-carpoolers, while the probability for seven and eight seats is bigger for carpoolers.

We're most interested in the AMCE though, and not the marginal means, so we'll use `compare_levels()` to find the carpool-specific differences between the effect of moving from 6 → 7 and 6 → seats:

```{r show-amces-seat-carpool}
amces_seat_carpool <- preds_seat_carpool_marginalized %>% 
  group_by(carpool) %>% 
  compare_levels(variable = avg, by = seat, comparison = "control") 

amces_seat_carpool %>% 
  median_qi(avg)
```

Among carpoolers, the causal effect of moving from 6 → 7 passengers, holding all other features constant, is a 5ish percentage point increase in the probability of selecting the vehicle. The effect is bigger (9ish percentage points) when moving from 6 → 8.

Among non-carpoolers, the causal effect is reversed. Moving from 6 → 7 passengers causes a 16 percentage point decrease in the probability of selection, while moving from 6 → 8 causes a 12 percentage point decrease, holding all other features constant.

These effects are "significant" and have a 90–97% probability of being greater than zero for carpoolers and 98–99% probability of being less than zero for the non-carpoolers.

```{r show-pd-amces-seat-carpool}
# Calculate probability of direction
amces_seat_carpool %>% 
  group_by(seat, carpool) %>% 
  summarize(p_gt_0 = sum(avg > 0) / n()) %>% 
  mutate(p_lt_0 = 1 - p_gt_0)
```

```{r plot-amces-seat-carpool}
amces_seat_carpool %>% 
  ggplot(aes(x = avg, y = seat)) +
  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) +
  geom_vline(xintercept = 0) +
  scale_x_continuous(labels = label_pp) +
  scale_slab_alpha_discrete(
    range = c(0.4, 1),
    labels = c("Non-carpoolers", "Carpoolers"),
    guide = guide_legend(
      reverse = TRUE, override.aes = list(fill = "grey10"), 
      keywidth = 0.8, keyheight = 0.8
    )
  ) +
  labs(
    x = "Percentage point change in probability of minivan selection",
    y = NULL,
    slab_alpha = NULL
  ) +
  theme(
    legend.position = "top",
    legend.justification = "left",
    legend.margin = margin(l = -7, t = 0)
  )
```

Here are all the AMCEs across carpool status:

```{r calculate-all-amces-carpool}
amces_minivan_carpool <- bind_rows(
  seat = all_preds_brms_carpool %>% 
    group_by(seat, carpool, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = seat, comparison = "control") %>% 
    rename(contrast = seat),
  cargo = all_preds_brms_carpool %>% 
    group_by(cargo, carpool, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = cargo, comparison = "control") %>% 
    rename(contrast = cargo),
  eng = all_preds_brms_carpool %>% 
    group_by(eng, carpool, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = eng, comparison = "control") %>% 
    rename(contrast = eng),
  price = all_preds_brms_carpool %>% 
    group_by(price, carpool, .draw) %>%
    summarize(avg = mean(.epred)) %>% 
    compare_levels(variable = avg, by = price, comparison = "control") %>% 
    rename(contrast = price),
  .id = "term"
)

amces_minivan_carpool %>% 
  group_by(term, carpool, contrast) %>% 
  median_qi(avg)
```

And finally, here's a polisci-style plot of all these AMCEs, which is so so neat. An individual's carpooling behavior interacts with seat count (increasing the seat count causes carpoolers to select the minivan more often), and it also interacts a bit with cargo space (increasing the cargo space makes both types of individuals more likely to select the minivan, but moreso for carpoolers) and also with price (increasing the price makes both types of individuals less likely to select the minivan, but moreso for carpoolers). Switching from gas → hybrid and gas → electric has a negative effect on both types of consumers, and there's no carpooling-based difference.

```{r extract-minivan-variable-levels-again}
#| code-fold: true
#| code-summary: "Extract variable labels"
minivan_var_levels <- tibble(
  variable = c("seat", "cargo", "eng", "price")
) %>% 
  mutate(levels = map(variable, ~{
    x <- minivans[[.x]]
    if (is.numeric(x)) {
      ""
    } else if (is.factor(x)) {
      levels(x)
    } else {
      sort(unique(x))
    }
  })) %>% 
  unnest(levels) %>% 
  mutate(term = paste0(variable, levels))

# Make a little lookup table for nicer feature labels
minivan_var_lookup <- tribble(
  ~variable, ~variable_nice,
  "seat",    "Passengers",
  "cargo",   "Cargo space",
  "eng",     "Engine type",
  "price",   "Price (thousands of $)"
) %>% 
  mutate(variable_nice = fct_inorder(variable_nice))
```

```{r plot-amces-minivans-carpool, fig.height=5}
#| code-fold: true
#| code-summary: "Combine full dataset of factor levels with marginalized posterior draws and make plot"
posterior_amces_minivan_carpool_nested <- amces_minivan_carpool %>% 
  separate_wider_delim(
    contrast,
    delim = " - ", 
    names = c("variable_level", "reference_level")
  ) %>% 
  group_by(term, variable_level) %>% 
  nest()

plot_data_minivan_carpool <- minivan_var_levels %>%
  left_join(
    posterior_amces_minivan_carpool_nested,
    by = join_by(variable == term, levels == variable_level)
  ) %>%
  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %>% 
  unnest(data) %>% 
  left_join(minivan_var_lookup, by = join_by(variable)) %>% 
  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %>% 
  # Make the missing carpool values be "yes" for the reference category
  mutate(carpool = replace_na(carpool, "yes"))

plot_data_minivan_carpool %>% 
  ggplot(aes(x = avg, y = levels, fill = variable_nice)) +
  geom_vline(xintercept = 0) +
  stat_halfeye(aes(slab_alpha = carpool), normalize = "groups") + 
  facet_col(facets = "variable_nice", scales = "free_y", space = "free") +
  scale_x_continuous(labels = label_pp) +
  scale_slab_alpha_discrete(
    range = c(0.4, 1),
    labels = c("Non-carpoolers", "Carpoolers"),
    guide = guide_legend(
      reverse = TRUE, override.aes = list(fill = "grey10"), 
      keywidth = 0.8, keyheight = 0.8
    )
  ) +
  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = "none") +
  labs(
    x = "Percentage point change in probability of minivan selection",
    y = NULL,
    title = "AMCEs across respondent carpool status",
    slab_alpha = NULL
  ) +
  theme(
    legend.position = "top",
    legend.justification = "left",
    legend.margin = margin(l = -7, t = 0)
  )
```

---

# tl;dr: Moral of the story

holy crap this might be the longest guide I've ever posted here. This is hard.

::: {.callout-tip}
### Main points

- OLS is nice and easy, but it fails to capture all the dynamics between sets of features and individual characteristics. Use multilevel multinomial models to account for all the heterogeneity in choices and individuals.

- {brms} provides a powerful, easy-to-use frontend for running complex multilevel models in Stan. There's no need to write raw Stan code if you don't want to.

- This is hard stuff. Multinomial logit models are complex and weird to work with. But the power and flexibility and richness is worth it.
:::

Here's a general summary of the main code for all this, based on a hypothetical conjoint survey with three features, like this:

| Features/Attributes | Levels            |
|:--------------------|:------------------|
| Brand (`brand`)     | A, B, C           |
| Color (`color`)     | Blue, red, yellow |
| Size (`size`)       | Small, large      |

And imagine that we have data on respondent age (`resp_age`) and education (`resp_ed`).

- Specify a full luxury multilevel model with individual-level characteristics informing choice-level coefficients like this ([see here](#translating-from-marketing-style-stan-notation-to-brms-syntax)):

```{r example-mlm-formula, eval=FALSE, indent="  "}
model <- brm(
  bf(choice ~
      # Columns for feature interacted with respondent-level covariates
      (brand + color + size) * (resp_age + resp_ed) +
      # Respondent-specific intercepts and slopes for all features
      (1 + brand + color + sice | ID | resp_id)),
  family = categorical(),
  data = data
)
```

- Find marketing-style predicted shares for hypothetical mixes of products by feeding a smaller dataset of hypothetical mixes to `brms::posterior_linpred()` (or `tidybayes::linpred_draws()`) to find utility (or logit-scale predictions), then calculate the share for each draw, and then marginalize (or average) the shares across the multiple choices within each draw. ([See here](#predictions-2).)

```{r example-shares, eval=FALSE, indent="  "}
product_mix <- tribble(
  ~brand, ~color,   ~size,   ~resp_age,
  "A",    "Blue",   "Small", 25,
  "B",    "Red",    "Small", 25,
  "C",    "Yellow", "Large", 25,
  "A",    "Blue",   "Small", 45,
  "B",    "Red",    "Small", 45,
  "C",    "Yellow", "Large", 45
)

model %>% 
  linpred_draws(newdata = product_mix, value = "utility") %>% 
  group_by(.draw, .category, resp_age) %>% 
  mutate(share = exp(utility) / sum(exp(utility))) %>% 
  # Marginalize across the outcomes within each draw
  group_by(brand, color, size, resp_age, .draw) %>% 
  summarize(share = mean(share))
```

- Find political science-style marginal means and AMCEs by feeding a *complete grid of all possible feature levels*, along with any respondent-level characteristics of interest to `brms::posterior_epred()` (or `tidybayes::epred_draws()`), then filter those probabilities to look only at option 0 (i.e. not choosing an option), and calculate `1 - prediction` to reverse it. Then group by the feature level and respondent-level characteristics you're interested in, find the average of predictions, and marginalize (or average) out the other covariates in each draw. ([See here](#amces-2).)

```{r example-mms-and-amces, eval=FALSE, indent="  "}
all_feature_combos_with_age <- data %>% 
  tidyr::expand(brand, color, size, resp_age)

all_predictions <- model %>% 
  epred_draws(newdata = all_feature_combos_with_age) %>% 
  # Only look at category 0 and reverse predictions
  filter(.category == 0) %>% 
  mutate(.epred = 1 - .epred)

# Marginal means for brand across respondent age
all_predictions %>% 
  # Marginalize out the other covariates in each draw
  group_by(brand, resp_age, .draw) %>% 
  summarize(avg = mean(.epred))

# AMCEs for brand across respondent age
all_predictions %>% 
  # Marginalize out the other covariates in each draw
  group_by(brand, resp_age, .draw) %>% 
  summarize(avg = mean(.epred)) %>% 
  group_by(resp_age) %>% 
  compare_levels(variable = avg, by = brand, comparison = "control")
```

---

Fin.
