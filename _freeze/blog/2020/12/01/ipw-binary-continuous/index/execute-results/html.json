{
  "hash": "f7568c9531098531e5369cd9b8f01289",
  "result": {
    "markdown": "---\ntitle: \"Generating inverse probability weights for both binary and continuous treatments\"\ndate: 2020-12-01\ndescription: \"Use R to close backdoor confounding by generating and using inverse probability weights for both binary and continuous treatments\"\ncategories: \n  - r\n  - tidyverse\n  - causal inference\n  - DAGs\n  - do calculus\n  - inverse probability weighting\nimage: index_files/figure-html/dag-binary-1.png\ndoi: 10.59350/1svkc-rkv91\ncitation: true\n---\n\n\n\n\nMy [program evaluation class](https://evalf20.classes.andrewheiss.com/) is basically a fun wrapper around topics in causal inference and econometrics. I'm a big fan of Judea Pearl-style [\"causal revolution\"](https://bigthink.com/errors-we-live-by/judea-pearls-the-book-of-why-brings-news-of-a-new-science-of-causes) causal graphs (or [DAGs](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html)), and they've made it easier for both me and my students to understand econometric approaches like diff-in-diff, regression discontinuity, and instrumental variables. \n\nDAGs are also incredibly helpful for doing causal inference with observational data *without* needing a specific quasi-experimental situation. As I show [in this blog post](/blog/2020/02/25/closing-backdoors-dags/) (and in [this new textbook chapter!](/research/chapters/heiss-causal-inference-2021/)), you can use DAGs to identify confounders that distort the relationship (i.e. open up backdoors) between treatment and outcome. You can then use statistical methods to close those backdoors and adjust for the confounding. In both that blog post and the chapter, I show how to do this with matching and with inverse probability weighting (IPW).\n\nHowever, those examples assume that the treatment is binary. This is fine—lots of social programs *are* binary (used program/didn't use program), and the math for creating inverse probability weights with binary treatment variables is fairly straightforward. However, treatment variables are also often *not* binary, especially outside of program evaluation. \n\nIn my own research, I'm working on a couple projects right now where the \"treatment\" is a count of anti-NGO legal restrictions in a country. I want to be able to use DAGs and inverse probability weighting to adjust for confounders, but I can't use the IPW stuff I've been teaching because that variable isn't binary! This research project gets even more complicated because it involves time-series cross-sectional (TSCS) data with both time-varying and time-invarying confounders, which opens up a whole other can of worms that I'll figure out soon following @BlackwellGlynn:2018.\n\nSo I had to teach myself how to do IPW with continuous variables. This post shows how to calculate IPWs for both binary and continuous treatments, both manually and with a couple different R packages ([**ipw**](https://cran.r-project.org/package=ipw) and [**WeightIt**](https://github.com/ngreifer/WeightIt)). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(truncnorm)\nlibrary(ipw)\nlibrary(WeightIt)\n```\n:::\n\n\n## Binary treatments\n\n### Example data\n\nFor this example, we'll generate a DAG for a hypothetical program where bed net use causes a reduction in malaria risk. That relationship is confounded by both income and health, and income influences health. Income and health both increase the probability of net usage.\n\nThe treatment here is binary: either people use nets or they don't.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmosquito_dag <- dagify(mal ~ net + inc + hlth,\n                       net ~ inc + hlth,\n                       hlth ~ inc,\n                       coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3),\n                                     y = c(mal = 1, net = 1, inc = 2, hlth = 2)),\n                       exposure = \"net\",\n                       outcome = \"mal\")\n\nggdag_status(mosquito_dag) +\n  guides(color = \"none\") +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/dag-binary-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe'll measure these nodes like so:\n\n- **Malaria risk**: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n- **Net use**: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution. However, since we want to use other variables that increase the likelihood of using a net, we'll generate a latent continuous variable, rescale it to 0–1, and then use it as probabilities in `rbinom()` and assign people to treatment based on those probabilities.\n- **Income**: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n- **Health**: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1138 people (just for fun)\nn_people <- 1138\n\nnet_data <- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %>% \n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %>% \n  # Generate net variable based on income, health, and random noise\n  mutate(net_score = (0.5 * income) + (1.5 * health) + rnorm(n_people, mean = 0, sd = 15),\n         # Scale net score down to 0.05 to 0.95 to create a probability of using a net\n         net_probability = rescale(net_score, to = c(0.05, 0.95)),\n         # Randomly generate a 0/1 variable using that probability\n         net = rbinom(n_people, 1, net_probability)) %>% \n  # Finally generate a malaria risk variable based on income, health, net use,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down by 10 when using a net. Because we rescale things,\n         # though, we have to make the effect a lot bigger here so it scales\n         # down to -10. Risk also decreases as health and income go up. I played\n         # with these numbers until they created reasonable coefficients.\n         malaria_effect = (-30 * net) + (-1.9 * health) + (-0.1 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70))) %>% \n  select(-c(health_base, health_income_effect, net_score, net_probability, \n            malaria_risk_base, malaria_effect))\n\nhead(net_data)\n## # A tibble: 6 × 5\n##      id income health   net malaria_risk\n##   <int>  <dbl>  <dbl> <int>        <dbl>\n## 1     1   409.   63.1     0         45.1\n## 2     2   521.   83.5     1         23.4\n## 3     3   581.   73.0     0         36.5\n## 4     4   324.   60.6     0         58.7\n## 5     5   532.   73.4     1         32.7\n## 6     6   538.   42.6     0         52.5\n```\n:::\n\n\n\n### IPW manually, binary treatment\n\nIf we just look at the effect of nets on malaria risk without any statistical adjustment, we see that nets cause a decrease of 13 points in malaria risk. This is wrong though because there's confounding.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Wrong correlation-is-not-causation effect\nmodel_net_naive <- lm(malaria_risk ~ net, data = net_data)\ntidy(model_net_naive)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)     41.9     0.413     102.  0        \n## 2 net            -13.6     0.572     -23.7 2.90e-101\n```\n:::\n\n\nAccording to *do*-calculus logic, we need to adjust for both income and health:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadjustmentSets(mosquito_dag)\n## { hlth, inc }\n```\n:::\n\n\nWe'll do that with inverse probability weighting. First we'll use the health and income confounders to predict the treatment, or net use, and then we'll generate propensity scores. We'll then use those propensity scores to generate inverse probability weights following this formula:\n\n$$\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n$$\n\nThis formula will calculate weights for the average treatment effect (ATE). [Lucy D'Agostino McGowan has formulas for a bunch of different IPWs](https://livefreeordichotomize.com/2019/01/17/understanding-propensity-score-weighting/#how-do-we-incorporate-a-propensity-score-in-a-weight), including the average treatment on the treated (ATT), average treatment among the controls (ATC), and other effects.\n\nHere's how we do that with R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Logit model to predict net use\nmodel_predict_net <- glm(net ~ income + health,\n                         family = binomial(link = \"logit\"),\n                         data = net_data)\n\n# Generate propensity scores and IPWs\nnet_data_ipw <- augment_columns(model_predict_net, net_data,\n                                type.predict = \"response\") %>% \n  rename(propensity = .fitted) %>% \n  mutate(ipw = (net / propensity) + ((1 - net) / (1 - propensity)))\n\nnet_data_ipw %>% \n  select(id, income, health, net, malaria_risk, propensity, ipw) %>% \n  head()\n## # A tibble: 6 × 7\n##      id income health   net malaria_risk propensity   ipw\n##   <int>  <dbl>  <dbl> <int>        <dbl>      <dbl> <dbl>\n## 1     1   409.   63.1     0         45.1      0.380  1.61\n## 2     2   521.   83.5     1         23.4      0.628  1.59\n## 3     3   581.   73.0     0         36.5      0.659  2.93\n## 4     4   324.   60.6     0         58.7      0.266  1.36\n## 5     5   532.   73.4     1         32.7      0.597  1.68\n## 6     6   538.   42.6     0         52.5      0.459  1.85\n```\n:::\n\n\nFinally we'll use those weights in a regression model to find the ATE. After adjusting for confounding and closing the backdoor paths opened by income and health, **the effect of nets is -10.5**, which is more accurate than the naive estimate we found before. Yay!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_net_ipw <- lm(malaria_risk ~ net, data = net_data_ipw, weights = ipw)\ntidy(model_net_ipw)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     40.4     0.409      98.8 0       \n## 2 net            -10.5     0.578     -18.3 1.83e-65\n```\n:::\n\n\n\n### IPW with the **ipw** package, binary treatment\n\nInstead of running a logistic regression model and generating propensity scores by hand, we can use the **ipw** package to generate that `ipw` column automatically. Specify the confounders in the `denominator` argument. There's a `numerator` argument too that we can use for generating stabilized weights, but we'll skip that for now.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ipwpoint() can't handle tibbles! Force net_data to be a data.frame\nweights_ipwpoint <- ipwpoint(\n  exposure = net,\n  family = \"binomial\",  # The treatment is binary\n  link = \"logit\",\n  denominator = ~ income + health,\n  data = as.data.frame(net_data)\n)\n\n# They're the same!\nhead(weights_ipwpoint$ipw.weights)\n## [1] 1.61 1.59 2.93 1.36 1.68 1.85\nhead(net_data_ipw$ipw)\n## [1] 1.61 1.59 2.93 1.36 1.68 1.85\n```\n:::\n\n\nThe resulting `weights` object here is a standalone object, and you can do other things with it like `summary()`. We can add the weights back into the main data and then fit the final model (*technically* we don't need to—we could just say `weights = weights_ipwpoint$ipw.weights` and it would work just fine, but I don't like working with standalone vectors and prefer to have them be columns, just so everything is all together in one place). \n\nWe get the same ATE of -10.5.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet_data_ipwpoint <- net_data %>% \n  mutate(ipw = weights_ipwpoint$ipw.weights)\n\nmodel_net_ipwpoint <- lm(malaria_risk ~ net, \n                         data = net_data_ipwpoint, weights = ipw)\ntidy(model_net_ipwpoint)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     40.4     0.409      98.8 0       \n## 2 net            -10.5     0.578     -18.3 1.83e-65\n```\n:::\n\n\n### IPW with the **WeightIt** package, binary treatment\n\nWe can also use [the **WeightIt** package](https://ngreifer.github.io/WeightIt/articles/WeightIt.html) to generate weights. It has slightly different syntax and can find all sorts of different estimands beyond the ATE (like [most of the ones Lucy has listed](https://livefreeordichotomize.com/2019/01/17/understanding-propensity-score-weighting/#how-do-we-incorporate-a-propensity-score-in-a-weight)). It can also handle a bunch of different methods beyond propensity scores. **WeightIt** can also handle tibbles, which is nice. It *also* provides a bunch of other summary information (if you use `summary()`), like effective sample sizes (ESS) in the treated/untreated groups and covariate balance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nweights_weightit <- weightit(net ~ income + health,  # Model net use with confounders\n                             data = net_data, \n                             estimand = \"ATE\",  # Find the ATE\n                             method = \"ps\")  # Build weights with propensity scores\nweights_weightit\n## A weightit object\n##  - method: \"glm\" (propensity score weighting with GLM)\n##  - number of obs.: 1138\n##  - sampling weights: none\n##  - treatment: 2-category\n##  - estimand: ATE\n##  - covariates: income, health\n\n# See even more details here\n# summary(weights_weightit)\n\n# Same as the other methods!\nhead(weights_weightit$weights)\n## [1] 1.61 1.59 2.93 1.36 1.68 1.85\n```\n:::\n\n\nAs with **ipw**, we can add the weights to the dataset and run the model to find the same -10.5 ATE:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet_data_weightit <- net_data %>% \n  mutate(ipw = weights_weightit$weights)\n\nmodel_net_weightit <- lm(malaria_risk ~ net, \n                         data = net_data_weightit, weights = ipw)\ntidy(model_net_weightit)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     40.4     0.409      98.8 0       \n## 2 net            -10.5     0.578     -18.3 1.83e-65\n```\n:::\n\n\n\n## Continuous treatments\n\n### Example data\n\nInverse probability weights work with continuous treatment variables too, but the math is a ~~little~~ lot trickier. For this example, we'll generate a DAG for a hypothetical program where poorer families are given cash grants that they can spend on malaria prevention supplies, like mosquito nets, chemical treatments, and medication. It's a voluntary program—people self select into it, and we'll assume that people with lower health scores and lower income will sign up. The amount of the grant depends on income.\n\nThe treatment here is continuous: people get different amounts of anti-malaria grant money. For the sake of simplicity here, everyone gets some grant money. I'm not even going to try multilevel zero-inflated models or anything ([though those are cool!](https://vuorre.netlify.app/post/2019/02/18/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/)).\n\nThe DAG looks the same as before (since we're trying to keep things super simple here):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrant_dag <- dagify(mal ~ grant + inc + hlth,\n                    grant ~ inc + hlth,\n                    hlth ~ inc,\n                    coords = list(x = c(mal = 4, grant = 1, inc = 2, hlth = 3),\n                                  y = c(mal = 1, grant = 1, inc = 2, hlth = 2)),\n                    exposure = \"grant\",\n                    outcome = \"mal\")\n\nggdag_status(grant_dag) +\n  guides(color = \"none\") +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/dag-continuous-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe'll measure these nodes like so:\n\n- **Malaria risk**: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n- **Grant**: amount between 5 and 40, centered around 20ish.\n- **Income**: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n- **Health**: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1504 people\nn_people <- 1504\n\ngrant_data <- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %>%\n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %>% \n  # Generate grant variable\n  mutate(grant_base = rtruncnorm(n_people, mean = 18, sd = 10, a = 5, b = 40),\n         # Grants are higher for people with lower incomes; higher for people with lower health\n         grant_effect = (income * -0.25) + (health * -0.5),\n         # Make the final grant amount + noise + rescale it back down\n         grant = grant_base + grant_effect + rnorm(n_people, mean = 0, sd = 8),\n         grant = round(rescale(grant, to = c(5, 40)), 0)) %>% \n  # Finally generate a malaria risk variable based on income, health, grant amount,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down as grant money goes up. I played with these numbers\n         # until they created reasonable coefficients.\n         malaria_effect = (-40 * grant) + (-25 * health) + (-0.05 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70))) %>% \n  select(-c(health_base, health_income_effect, grant_base, grant_effect, \n            malaria_risk_base, malaria_effect))\n\nhead(grant_data)\n## # A tibble: 6 × 5\n##      id income health grant malaria_risk\n##   <int>  <dbl>  <dbl> <dbl>        <dbl>\n## 1     1   409.   70.2    27         26.3\n## 2     2   521.   75.3    18         33.3\n## 3     3   581.   62.6    20         42.3\n## 4     4   324.   83.9    28         11.8\n## 5     5   532.   74.3    20         31.8\n## 6     6   538.   75.9    15         36.2\n```\n:::\n\n\n### IPW manually, continuous treatment\n\nIf we just look at the effect of grants on malaria risk without any adjustment, every extra grant dollar causes a drop of 0.4 malaria risk points. Once again, though, this is wrong because of confounding.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Wrong correlation-is-not-causation effect\nmodel_grant_naive <- lm(malaria_risk ~ grant, data = grant_data)\ntidy(model_grant_naive)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   44.2      1.35       32.8  2.56e-178\n## 2 grant         -0.417    0.0615     -6.78 1.69e- 11\n```\n:::\n\n\nAccording to *do*-calculus logic, we again need to adjust for both income and health:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadjustmentSets(grant_dag)\n## { hlth, inc }\n```\n:::\n\n\nHere's where the math gets tricky. When we worked with a binary treatment, we calculated the propensity score for each observation and then used this formula to generate inverse probability weights:\n\n$$\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n$$\n\nWe can't do that with continuous treatment variables, though, since we don't really have propensity scores. Instead, we use this hairy-but-not-too-scary formula (from @NaimiMoodieAuger:2014; [ungated version here](http://www.jayskaufman.com/uploads/3/0/8/9/30891283/naimi_constructing_ipw_for_continuous_exposures_epidemiology_2014.pdf); also [see this for another R example](https://meghapsimatrix.com/post/continuous-r-rmarkdown/)):\n\n$$\n\\text{IPW} = \\frac{f_X (X; \\mu_1, \\sigma^2_1)}{f_{X | C} (X | C = c; \\mu_2, \\sigma^2_2)}\n$$\n\nPhew. That's a lot of math, but it's not too bad if we take it apart:\n\n- $X$ stands for the continuous exposure or treatment variable\n- $C$ stands for the confounders\n- The $f_\\cdot (\\cdot)$ function in both the numerator and denominator stands for a probability density function with a mean of $\\mu$ and a variance of $\\sigma^2$\n- The numerator $f_X (X; \\mu_1, \\sigma^2_1)$ refers to the probability distribution of just the treatment variable (technically you could just use 1 as the numerator, but that can lead to unstable weights—using the probability distribution of the treatment helps stabilize the weights)\n- The denominator $f_{X | C} (X | C = c; \\mu_2, \\sigma^2_2)$ refers to the probability distribution of the treatment variable explained by the confounders\n\n(Fun fact: I'm like 85% sure that the $\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}$ formula is just an algebraically rearranged and simplified version of this fancier equation)\n\nWe can calculate each element of this fraction and then generate the inverse probability weights. Here's how to do that with R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# The numerator is the probability distribution of just the treatment variable.\n# We'll use a normal distribution for it (hence dnorm()). We need to feed\n# dnorm() the grant amount for each person, the predicted value from a simple\n# grant ~ 1 model, and the sd of the residuals from that model\nmodel_num <- lm(grant ~ 1, data = grant_data)\nnum <- dnorm(grant_data$grant,\n             predict(model_num),\n             sd(model_num$residuals))\n\n# The denominator is the probability distribution of the treatment variable\n# explained by the confounders. We'll again use a normal distribution for it.\n# We'll feed dnorm() the grant amount, the predicted value from a model that\n# includes the confounders, and the sd of the residuals from that model\nmodel_den <- lm(grant ~ health + income, data = grant_data)\nden <- dnorm(grant_data$grant,\n             predict(model_den),\n             sd(model_den$residuals))\n\n# Finally, we make actual IPW weights by building the fraction\ngrant_data_ipw <- grant_data %>% \n  mutate(ipw = num / den)\n\nhead(grant_data_ipw)\n## # A tibble: 6 × 6\n##      id income health grant malaria_risk   ipw\n##   <int>  <dbl>  <dbl> <dbl>        <dbl> <dbl>\n## 1     1   409.   70.2    27         26.3 0.288\n## 2     2   521.   75.3    18         33.3 0.492\n## 3     3   581.   62.6    20         42.3 0.687\n## 4     4   324.   83.9    28         11.8 0.177\n## 5     5   532.   74.3    20         31.8 0.499\n## 6     6   538.   75.9    15         36.2 0.748\n```\n:::\n\n\nNow we can use the weights to find the ATE just like we did with the binary treatment:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_grant_ipw <- lm(malaria_risk ~ grant, data = grant_data_ipw, weights = ipw)\ntidy(model_grant_ipw)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)    58.4     1.79        32.6 1.95e-176\n## 2 grant          -1.11    0.0806     -13.7 1.45e- 40\n```\n:::\n\n\nEach dollar of grant money thus **causes a drop of -1.1 malaria risk points**. Neato!\n\n\n### IPW with the **ipw** package, continuous treatment\n\nManually creating the numerator and denominator can get tedious though. We can use the `ipwpoint()` function from **ipw** to generate continuous weights in one step. Instead of specifying a binomial treatment like we did before, we'll use a Gaussian (normal) family. We also specify both the numerator and denominator. It will generate identical weights.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nweights_continuous_ipwpoint <- ipwpoint(\n  exposure = grant,\n  family = \"gaussian\",\n  numerator = ~ 1,\n  denominator = ~ health + income,\n  data = as.data.frame(grant_data)\n)\n\n# Same values!\nhead(grant_data_ipw$ipw)\n## [1] 0.288 0.492 0.687 0.177 0.499 0.748\nhead(weights_continuous_ipwpoint$ipw.weights)\n## [1] 0.288 0.492 0.687 0.177 0.499 0.748\n```\n:::\n\n\nWe can then put those weights into the dataset and run a model with them. We get the same ATE of -1.1:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrant_data_ipwpoint <- grant_data %>% \n  mutate(ipw = weights_continuous_ipwpoint$ipw.weights)\n\nmodel_grant_ipwpoint <- lm(malaria_risk ~ grant, \n                           data = grant_data_ipwpoint, weights = ipw)\ntidy(model_grant_ipwpoint)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)    58.4     1.79        32.6 1.95e-176\n## 2 grant          -1.11    0.0806     -13.7 1.45e- 40\n```\n:::\n\n\n\n### IPW with the **WeightIt** package, continuous treatment\n\nThe **WeightIt** package also handles continuous weights. The syntax is a lot simpler—there's no need to worry about numerators and denominators.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nweights_weightit <- weightit(grant ~ income + health,  # Model grant amount with confounders\n                             data = grant_data, \n                             stabilize = TRUE)\nweights_weightit\n## A weightit object\n##  - method: \"glm\" (propensity score weighting with GLM)\n##  - number of obs.: 1504\n##  - sampling weights: none\n##  - treatment: continuous\n##  - covariates: income, health\n\n# See even more details here\n# summary(weights_weightit)\n\n# Not the same as the other methods :(\nhead(weights_weightit$weights)\n## [1] 0.580 0.990 1.384 0.356 1.005 1.506\n```\n:::\n\n\nHowever(!), for mathy reasons I don't understand, the weights it generates are not the same as what we get when doing it by hand or with `ipwpoint()`. In fact, they're almost exactly twice as large as the manual and `ipwpoint()` weights:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Manual weights\nhead(grant_data_ipw$ipw)\n## [1] 0.288 0.492 0.687 0.177 0.499 0.748\n\n# weightit() weights / 2\nhead(weights_weightit$weights) / 2\n## [1] 0.290 0.495 0.692 0.178 0.502 0.753\n```\n:::\n\n\nSurely there's an argument to `weightit()` that I'm missing somewhere.\n\nRegardless, for more mathy reasons I don't understand, the ATE is identical even though the weights are roughly doubled:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrant_data_weightit <- grant_data %>% \n  mutate(ipw = weights_weightit$weights)\n\nmodel_grant_weightit <- lm(malaria_risk ~ grant, \n                           data = grant_data_weightit, weights = ipw)\ntidy(model_grant_weightit)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)    58.4     1.79        32.6 1.95e-176\n## 2 grant          -1.11    0.0806     -13.7 1.45e- 40\n```\n:::\n\n\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}