{
  "hash": "23e6f98bdd3ed7de8abfd1fe5efe6abc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are\"\ndate: 2022-05-20\ndescription: \"Define what marginal effects even are, and then explore the subtle differences between average marginal effects, marginal effects at the mean, and marginal effects at representative values with the marginaleffects and emmeans R packages\"\nimage: images/twitter-cover@3x.png\ncategories:\n  - r\n  - tidyverse\n  - regression\n  - statistics\n  - data visualization\n  - marginal effects\nformat: \n  html: \n    toc-depth: 4\nresources:\n  - video/*\ndoi: 10.59350/40xaj-4e562\ncitation: true\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n<style type=\"text/css\">\n.smaller {\n  font-size: 85%;\n}\n</style>\n:::\n\n\n::: {.callout-tip}\n### Diagrams!\n\nYou can download PDF, SVG, and PNG versions of the marginal effects diagrams in this guide, as well as the original Adobe Illustrator file, here:\n\n- [PDFs, SVGs, and PNGs](http://files.andrewheiss.com/marginal-effects-diagrams/marginal-effects-output.zip)\n- [Illustrator .ai file](http://files.andrewheiss.com/marginal-effects-diagrams/marginal-effects.ai)\n\nDo whatever you want with them! They're licensed under [Creative Commons Attribution-ShareAlike (BY-SA 4.0)](http://creativecommons.org/licenses/by-sa/4.0/).\n:::\n\n\nI'm a huge fan of doing research and analysis in public. I try to make [my research public and freely accessible](https://www.andrewheiss.com/research/), but ever since watching [David Robinson's \"The unreasonable effectiveness of public work\" keynote from rstudio::conf 2019](https://www.rstudio.com/resources/rstudioconf-2019/the-unreasonable-effectiveness-of-public-work/), I've tried to make my research *process* open and accessible too.\n\nAccording to David, researchers typically view their work like this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/normal-work.png){fig-align='center' width=90%}\n:::\n:::\n\n\nPeople work towards a final published product, which is the most valuable output of the whole process. The intermediate steps like the code, data, preliminary results, and so on, are less valuable and often hidden from the public. People only see the final published thing.\n\nDavid argues that we should instead see our work like this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/public-work.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIn this paradigm, anything on your computer and only accessible by you isn't that valuable. Anything you make accessible to the public online—including all the intermediate stuff like code, data, and preliminary results, in addition to the final product—is incredibly valuable. The world can benefit from neat code tricks you stumble on while making graphs; the world can benefit from new data sources you find or your way of processing data; the world can benefit from a toy example of a new method you read about in some paper, even if the actual code you write to play around with the method never makes it into any published paper. It's all useful to the broader community of researchers. \n\nPublic work also builds community norms—if more people share their behind-the-scenes work, it encourages others to do the same and engage with it and improve it (see [this super detailed and helpful comment](https://github.com/andrewheiss/ath-hugo/commit/70197c7389c87b46db6cd61fc1ef8ce7b5c94616#commitcomment-73816081) with corrections to my previous post, for example!).\n\nPublic work is also valuable for another more selfish reason. Building an online presence with a wide readership is hard, and my little blog post contributions aren't famous or anything—they're just sitting out here in a tiny corner of the internet. But these guides have been indispensable for me. They've allowed me to work through and understand tricky statistical and programming concepts, *and then* have allowed me to come back to them months later and remember how they work. This whole blog is primarily a resource for future me.\n\nSo here's yet another blog post that is hopefully potentially useful for the general public, but that is definitely useful for future me.\n\nIn a few of my ongoing research projects, I'm working with non-linear regression models, and I've been struggling to interpret their results. In my past few posts (like [this one on hurdle models](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/), or [this one on multilevel panel data](https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/), or [this one on beta and zero-inflated models](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/)), I've explored a bunch of different ways to work with and interpret these more complex models and calculate their marginal effects. I even wrote [a guide to calculating average marginal effects for multilevel models](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/). TURNS OUT™, though, that I've actually been a bit wrong about my terminology for all the marginal effects I've talked about in those posts.\n\nPart of the reason for this wrongness is because there are so many quasi-synonyms for the idea of \"marginal effects\" and people seem to be pretty [loosey goosey](https://www.youtube.com/watch?v=iNukPHje4Fs) about what exactly they're referring to. There are statistical effects, marginal effects, marginal means, marginal slopes, conditional effects, conditional marginal effects, marginal effects at the mean, and many other similarly-named ideas. There are also regression coefficients and estimates, which have marginal effects vibes, but may or may not actually be marginal effects depending on the complexity of the model.\n\nThe question of what the heck \"marginal effects\" are has plagued me for a while. In October 2021 [I publicly announced](https://twitter.com/andrewheiss/status/1448719033306648586) that I would finally buckle down and figure out their definitions and nuances:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/original-tweet.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAnd then I didn't.\n\nSo here I am, 7 months later, publicly figuring out the differences between regression coefficients, regression predictions, [**marginaleffects**](https://marginaleffects.com/), [**emmeans**](https://cran.r-project.org/web/packages/emmeans/index.html), marginal slopes, average marginal effects, marginal effects at the mean, and all these other \"marginal\" things that researchers and data scientists use.\n\nThis guide is highly didactic and slowly builds up the concept of marginal effects as slopes and partial derivatives. [The tl;dr section at the end](#tldr-overall-summary-of-all-these-marginal-effects-approaches) has a useful summary of everything here, with a table showing all the different approaches to marginal effects with corresponding **marginaleffects** and **emmeans** code, as well as some diagrams outlining the two packages' different approaches to averaging. Hopefully it's useful—it is for me!\n\nLet's get started by looking at some lines and slopes (after loading a bunch of packages and creating some useful little functions).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load packages\n# ---------------\nlibrary(tidyverse)        # dplyr, ggplot2, and friends\nlibrary(broom)            # Convert models to data frames\nlibrary(marginaleffects)  # Marginal effects stuff\nlibrary(emmeans)          # Marginal effects stuff\n\n# Visualization-related packages\nlibrary(ggtext)           # Add markdown/HTML support to text in plots\nlibrary(glue)             # Python-esque string interpolation\nlibrary(scales)           # Functions to format numbers nicely\nlibrary(gganimate)        # Make animated plots\nlibrary(patchwork)        # Combine ggplots\nlibrary(ggrepel)          # Make labels that don't overlap\nlibrary(MetBrewer)        # Artsy color palettes\n\n# Data-related packages\nlibrary(palmerpenguins)   # Penguin data\nlibrary(WDI)              # Get data from the World Bank's API\nlibrary(countrycode)      # Map country codes to different systems\nlibrary(vdemdata)         # Use data from the Varieties of Democracy (V-Dem) project\n# Install vdemdata from GitHub, not CRAN\n# devtools::install_github(\"vdeminstitute/vdemdata\")\n\n\n# Helpful functions\n# -------------------\n# Format numbers in pretty ways\nnice_number <- label_number(style_negative = \"minus\", accuracy = 0.01)\nnice_p <- label_pvalue(prefix = c(\"p < \", \"p = \", \"p > \"))\n\n# Point-slope formula: (y - y1) = m(x - x1)\nfind_intercept <- function(x1, y1, slope) {\n  intercept <- slope * (-x1) + y1\n  return(intercept)\n}\n\n# Visualization settings\n# ------------------------\n\n# Custom ggplot theme to make pretty plots\n# Get IBM Plex Sans Condensed at https://fonts.google.com/specimen/IBM+Plex+Sans+Condensed\ntheme_mfx <- function() {\n  theme_minimal(base_family = \"IBM Plex Sans Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# Make labels use IBM Plex Sans by default\nupdate_geom_defaults(\"label\", \n                     list(family = \"IBM Plex Sans Condensed\"))\nupdate_geom_defaults(ggtext::GeomRichText, \n                     list(family = \"IBM Plex Sans Condensed\"))\nupdate_geom_defaults(\"label_repel\", \n                     list(family = \"IBM Plex Sans Condensed\"))\n\n# Use the Johnson color palette\nclrs <- met.brewer(\"Johnson\")\n```\n:::\n\n\n\n## What does \"marginal\" even mean in the first place?\n\nPut as simply as possible, in the world of statistics, \"marginal\" means \"additional,\" or what happens to outcome variable $y$ when explanatory variable $x$ changes a little.\n\nTo find out precisely how much things change, we need to use calculus. \n\nOh no.\n\n### Super quick crash course in differential calculus (it's not scary, I promise!)\n\nI haven't taken a formal calculus class since my senior year of high school in 2002. I enjoyed it a ton and got the highest score on the [AP Calculus BC test](https://apstudents.collegeboard.org/courses/ap-calculus-bc), which gave me enough college credits to not need it as an undergraduate, given that I majored in Middle East Studies, Arabic, and Italian. I figured I'd never need to think about calculus every again. lol. \n\nIn my first PhD-level stats class in 2012, the professor cancelled class for the first month and assigned us all to go relearn calculus with Khan Academy, since I wasn't alone in my unlearning of calculus. Even after that crash course refresher, I don't really ever use it in my own research. When I do, I only use it to think about derivatives and slopes, since those are central to statistics.\n\nCalculus can be boiled down to two forms: (1) **differential calculus** is all about finding rates of changes by calculating derivatives, or slopes, while (2) **integral calculus** is all about finding total amounts, or areas, by adding infinitesimally small things together. According to [the fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus), these two types are actually the inverse of each other—you can find the total area under a curve based on its slope, for instance. Super neat stuff. If you want a cool accessible refresher / history of all this, check out Steven Strogatz's [*Infinite Powers: How Calculus Reveals the Secrets of the Universe*](https://www.amazon.com/Infinite-Powers-Calculus-Reveals-Universe/dp/1328879984)—it's great.\n\nIn the world of statistics and marginal effects all we care about are slopes, which are solely a differential calculus idea.\n\nLet's pretend we have a line that shows the relationship between $x$ and $y$ that's defined with an equation using the form $y = mx + b$, where $m$ is the slope and $b$ is the y-intercept. We can plot it with ggplot using the helpful `geom_function()` function:\n\n$$\ny = 2x - 1\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# y = 2x - 1\na_line <- function(x) (2 * x) - 1\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_line, linewidth = 1, color = clrs[2]) +\n  scale_x_continuous(breaks = -2:5, limits = c(-1, 3)) +\n  scale_y_continuous(breaks = -3:9) +\n  annotate(geom = \"segment\", x = 1, y = 1.3, xend = 1, yend = 3, color = clrs[4], linewidth = 0.5) +\n  annotate(geom = \"segment\", x = 1, y = 3, xend = 1.8, yend = 3, color = clrs[4], linewidth = 0.5) +\n  annotate(geom = \"richtext\", x = 1.4, y = 3.1, label = \"Slope: **2**\", vjust = 0) +\n  labs(x = \"x\", y = \"y\") +\n  coord_equal() +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-line-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe line crosses the y-axis at -1, and its slope, or its $\\frac{\\text{rise}}{\\text{run}}$ is 2, or $\\frac{2}{1}$, meaning that we go up two units and to the right one unit.\n\nImportantly, the slope shows the relationship between $x$ and $y$. If $x$ increases by 1 unit, $y$ increases by 2: when $x$ is 1, $y$ is 1; when $x$ is 2, $y$ is 3, and so on. We can call this the *marginal effect*, or the change in $y$ that results from one additional $x$.\n\nWe can think about this slope using calculus language too. In differential calculus, slopes are called *derivatives* and they represent the change in $y$ that results from changes in $x$, or $\\frac{dy}{dx}$. The $d$ here refers to an infinitesimal change in the values of $x$ and $y$, rather than a one-unit change like we think of when looking at the slope as $\\frac{\\text{rise}}{\\text{run}}$. Even more technically, the $d$ indicates that we're working with the total derivative, since there's only one variable ($x$) to consider. If we had more variables (like $y = 2x + 3z -1$), we would need to find the partial derivative for $x$, holding $z$ constant, and we'd write the derivative with a $\\partial$ symbol instead: $\\frac{\\partial y}{\\partial x}$. More on that in a bit.\n\nBy plotting this line, we can figure out $\\frac{dy}{dx}$ visually—the slope is 2. But we can figure it out mathematically too. Differential calculus is full of [fancy tricks and rules of thumb](https://en.wikipedia.org/wiki/Differentiation_rules) for figuring out derivatives, like the [power rule](https://en.wikipedia.org/wiki/Power_rule), the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), and so on. The easiest one for me to remember is the power rule, which says you can find the slope of a variable like $x$ by decreasing its exponent by 1 and multiplying that exponent by the variable's coefficient. All constants (terms without $x$) disappear.\n\n$$\n\\begin{aligned}\ny &= 2x - 1 \\\\\n&= 2x^1 - 1 \\\\\n\\frac{dy}{dx}&= (1 \\times 2) x^0 \\\\\n&=  2\n\\end{aligned}\n$$\n\n(My secret is that I only know the power rule and so I avoid calculus at all costs and either [use R](https://www.andrewheiss.com/blog/2018/02/15/derivatives-r-fun/) or use Wolfram Alpha—go to [Wolfram Alpha](https://www.wolframalpha.com), type in `derivative y = 2x - 1` and you'll [see some magic](https://www.wolframalpha.com/input?i=derivative+y+%3D+2x+-+1).)\n\nWe thus know that the derivative of $y = 2x - 1$ is $\\frac{dy}{dx} = 2$. At every point on this line, the slope is 2—it never changes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nslope_annotations <- tibble(x = c(-0.25, 1.2, 2.4)) |> \n  mutate(y = a_line(x)) |> \n  mutate(nice_y = y + 1) |> \n  mutate(nice_label = glue(\"x: {x}; y: {y}<br>\",\n                           \"Slope (dy/dx): **{2}**\"))\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_line, linewidth = 1, color = clrs[2]) +\n  geom_point(data = slope_annotations, aes(x = x, y = y)) +\n  geom_richtext(data = slope_annotations, \n                aes(x = x, y = y, label = nice_label),\n                nudge_y = 0.5) +\n  scale_x_continuous(breaks = -2:5, limits = c(-1, 3)) +\n  scale_y_continuous(breaks = -3:9) +\n  labs(x = \"x\", y = \"y\") +\n  coord_equal() +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-line-slope-labels-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe power rule seems super basic for equations with non-exponentiated $x$s, but it's really helpful with more complex equations, like this parabola $y = -0.5x^2 + 5x + 5$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# y = -0.5x^2 + 5x + 5\na_parabola <- function(x) (-0.5 * x^2) + (5 * x) + 5\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_parabola, linewidth = 1, color = clrs[2]) +\n  xlim(-5, 15) +\n  labs(x = \"x\", y = \"y\") +\n  coord_cartesian(ylim = c(-5, 20)) +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-parabola-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWhat's interesting here is that there's no longer a single slope for the whole function. The steepness of the slope across a range of $x$s depends on whatever $x$ currently is. The curve is steeper at really low and really high values of $x$ and it is shallower around 5 (and it is completely flat when $x$ is 5).\n\nIf we apply the power rule to the parabola formula we can find the exact slope:\n\n$$\n\\begin{aligned}\ny &= -0.5x^2 + 5x^1 + 5 \\\\\n\\frac{dy}{dx} &= (2 \\times -0.5) x + (1 \\times 5) x^0 \\\\\n&= -x + 5\n\\end{aligned}\n$$\n\nWhen $x$ is 0, the slope is 5 ($-0 + 5$); when $x$ is 8, the slope is −3 ($-8 + 5$), and so on. We can visualize this if we draw some lines tangent to some different points on the equation. The slope of each of these tangent lines represents the instantaneous slope of the parabola at each $x$ value.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# dy/dx = -x + 5\nparabola_slope <- function(x) (-x) + 5\n\nslope_annotations <- tibble(\n  x = c(0, 3, 8)\n) |> \n  mutate(y = a_parabola(x),\n         slope = parabola_slope(x),\n         intercept = find_intercept(x, y, slope),\n         nice_slope = glue(\"Slope (dy/dx)<br><span style='font-size:12pt;color:{clrs[4]}'>**{slope}**</span>\"))\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_parabola, linewidth = 1, color = clrs[2]) +\n  geom_abline(data = slope_annotations,\n              aes(slope = slope, intercept = intercept),\n              linewidth = 0.5, color = clrs[4], linetype = \"21\") +\n  geom_point(data = slope_annotations, aes(x = x, y = y),\n             size = 3, color = clrs[4]) +\n  geom_richtext(data = slope_annotations, aes(x = x, y = y, label = nice_slope),\n                nudge_y = 2) +\n  xlim(-5, 15) +\n  labs(x = \"x\", y = \"y\") +\n  coord_cartesian(ylim = c(-5, 20)) +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-parabola-labels-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAnd here's an animation of what the slope looks like across a whole range of $x$s. Neat!\n\n\n\n\n\n<video controls width=\"90%\" style=\"display: block; margin: auto;\">\n  <source src=\"video/parabola_slope.mp4\" type=\"video/mp4\">\n</video>\n\n::: {.callout-note}\nFor the sake of space, I didn't include the code for this here, but you can see how I made this animation with [gganimate](https://gganimate.com/) at [the R Markdown file for this post at GitHub](https://github.com/andrewheiss/ath-hugo/blob/main/content/blog/2022-05-20_marginalia/index.Rmarkdown#L317).\n:::\n\n\n### Marginal things in economics\n\nIn the calculus world, the term \"marginal\" isn't used all that often. Instead they talk about derivatives. But in the end, all these marginal/derivative things are just slopes.\n\nBefore looking at how this applies to the world of statistics, let's look at a quick example from economics, since economists also use the word \"marginal\" to refer to slopes. My first exposure to the word \"marginal\" meaning \"changes in things\" wasn't actually in the world of statistics, but in economics. I took my first microeconomics class as a first-year MPA student in 2010 (and hated it; ironically [I teach it now](https://econs21.classes.andrewheiss.com/) 🤷).\n\nOne common question in microeconomics relates to how people maximize their happiness, or utility, under budget constraints ([see here for an R-based example](https://www.andrewheiss.com/blog/2019/02/16/algebra-calculus-r-yacas/)). Economists imagine that people have utility functions in their heads that take inputs and convert them to utility (or happiness points). For instance, let's pretend that the happiness/utility ($u$) you get from the number of cookies you eat ($x$) is defined like this:\n\n$$\nu = -0.5x^2 + 5x\n$$\n\nHere's what that looks like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# u = -0.5x^2 + 5x\nu_cookies <- function(x) (-0.5 * x^2) + (5 * x)\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = u_cookies, linewidth = 1, color = clrs[2]) +\n  scale_x_continuous(breaks = seq(0, 12, 2), limits = c(0, 12)) +\n  labs(x = \"Cookies\", y = \"Utility (happiness points)\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-cookies-utility-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThis parabola represents your *total* utility from cookies. Eat 1 cookie, get 4.5 happiness points; eat 3 cookies, get 10.5 points; eat 6, get 12 points; and so on.\n\nThe *marginal* utility, on the other hand, tells how how much more happiness you'd get from eating one more cookie. If you're currently eating 1, how many more happiness points would you get by moving to 2? If if you're eating 7, what would happen to your happiness if you moved to 8? We can figure this out by looking at the slope of the parabola, which will show us the instantaneous rate of change, or marginal utility, for any number of cookies.\n\nPower rule time! <small>(or type [`derivative -0.5x^2 + 5x`](https://www.wolframalpha.com/input?i=derivative+-0.5x%5E2+%2B+5x) at Wolfram Alpha)</small>\n\n$$\n\\begin{aligned}\nu &= -0.5x^2 + 5x \\\\\n\\frac{du}{dx} &= (2 \\times -0.5) x^1 + 5x^0 \\\\\n&= -x + 5 \n\\end{aligned}\n$$\n\nLet's plot this really quick too:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# du/dx = -x + 5\nmu_cookies <- function(x) -x + 5\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_vline(xintercept = 5, linewidth = 0.5, \n             linetype = \"21\", color = clrs[3]) +\n  geom_function(fun = mu_cookies, linewidth = 1, color = clrs[5]) +\n  scale_x_continuous(breaks = seq(0, 12, 2), limits = c(0, 12)) +\n  labs(x = \"Cookies\", y = \"Marginal utility (additional happiness points)\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-cookies-marginal-utility-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIf you're currently eating 1 cookie and you grab another one, you'll gain 4 *extra* or *marginal* happiness points. If you're eating 6 and you grab another one, you'll actually *lose* some happiness—the marginal utility at 6 is -1. If you're an economist who wants to maximize your happiness, you should eat the number of cookies where the extra happiness you'd get is 0, or where marginal utility is 0:\n\n$$\n\\begin{aligned}\n\\frac{du}{dx} &= -x + 5 \\\\\n0 &= -x + 5 \\\\\nx &= 5\n\\end{aligned}\n$$\n\nEat 5 cookies, maximize your happiness. Eat any more and you'll start getting disutility (like a stomachache). This is apparent in the marginal utility plot too. All the values of marginal utility to the left of 5 are positive; all the values to the right of 5 are negative. Economists call this *decreasing marginal utility*.\n\nThis relationship between total utility and marginal utility is even more apparent if we look at both simultaneously (for fun I included the second derivative ($\\frac{d^2u}{dx^2}$), or the slope of the first derivative, in the marginal utility panel):\n\n\n\n\n\n<video controls width=\"90%\" style=\"display: block; margin: auto;\">\n  <source src=\"video/cookie_mfx.mp4\" type=\"video/mp4\">\n</video>\n\n::: {.callout-note}\nAgain, I omitted the code for this here, but [you can see it at GitHub](https://github.com/andrewheiss/ath-hugo/blob/main/content/blog/2022-05-20_marginalia/index.Rmarkdown#L397).\n:::\n\n\n## What about marginal things in statistics?\n\nMarginal utility, marginal revenue, marginal costs, and all those other marginal things are great for economists, but how does this \"marginal\" concept relate to statistics? Is it the same?\n\nYep! Basically!\n\nAt its core, regression modeling in statistics is all about fancy ways of finding averages and fancy ways of drawing lines. Even if you're doing non-regression things like t-tests, [those are technically still just regression behind the scenes](https://lindeloev.github.io/tests-as-linear/). \n\nStatistics is all about lines, and lines have slopes, or derivatives. These slopes represent the marginal changes in an outcome. As you move an independent/explanatory variable $x$, what happens to the dependent/outcome variable $y$?\n\n### Regression, sliders, switches, and mixing boards\n\nBefore getting into the mechanics of statistical marginal effects, it's helpful to review what exactly regression coefficients are doing in statistical models, especially when dealing with both continuous and categorical explanatory variables. \n\nWhen I teach statistics to my students, my favorite analogy for regression is to think of sliders and switches. Sliders represent continuous variables: as you move them up and down, something gradual happens to the resulting light. Switches represent categorical variables: as you turn them on and off, there are larger overall changes to the resulting light.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/slider-switch-annotated-80.jpg){fig-align='center' width=100%}\n:::\n:::\n\n\nLet's look at some super tiny quick models to illustrate this, using data from [**palmerpenguins**](https://allisonhorst.github.io/palmerpenguins/):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins <- penguins |> drop_na()\n\nmodel_slider <- lm(body_mass_g ~ flipper_length_mm, data = penguins)\ntidy(model_slider)\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic   p.value\n##   <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)        -5872.     310.       -18.9 1.18e- 54\n## 2 flipper_length_mm     50.2      1.54      32.6 3.13e-105\n\nmodel_switch <- lm(body_mass_g ~ species, data = penguins)\ntidy(model_switch)\n## # A tibble: 3 × 5\n##   term             estimate std.error statistic   p.value\n##   <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)        3706.       38.1    97.2   6.88e-245\n## 2 speciesChinstrap     26.9      67.7     0.398 6.91e-  1\n## 3 speciesGentoo      1386.       56.9    24.4   1.01e- 75\n```\n:::\n\n\nDisregard the intercept for now and just look at the coefficients for `flipper_length_mm` and `species*`. Flipper length is a continuous variable, so it's a slider—as flipper length increases by 1 mm, penguin body mass increases by 50 grams. Slide it up more and you'll see a bigger increase: if flipper length increases by 10 mm, body mass should increase by 500 grams. Slide it down for fun too! If flipper length decreases by 1 mm, body mass decreases by 50 grams. Imagine it like a sliding light switch.\n\nSpecies, on the other hand, is a switch. There are three possible values here: Adelie, Chinstrap, and Gentoo. The base case in the results here is Adelie since it comes fist alphabetically. The coefficients for `speciesChinstrap` and `speciesGentoo` aren't sliders—you can't talk about one-unit increases in Gentoo-ness or Chinstrap-ness. Instead, the values show what happens in relation to the average weight of Adelie penguins if you flip the Chinstrap or Gentoo switch. Chinstrap penguins are 29 grams heavier than Adelie penguins on average, while the chonky Gentoo penguins are 1.4 kg heavier than Adellie penguins. With these categorical coefficients, we're flipping a switch on and off: Adelie vs. Chinstrap and Adelie vs. Gentoo.\n\nThis slider and switch analogy holds when thinking about multiple regression too, though we need to think of lots of sliders and switches, like in an audio mixer board:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/mixer-board-annotated-80.jpg){fig-align='center' width=100%}\n:::\n:::\n\n\nWith a mixer board, we can move many different sliders up and down and use different combinations of switches, all of which ultimately influence the audio output. \n\nLet's make a more complex mixer-board-esque regression model with multiple continuous (slider) and categorical (switch) explanatory variables:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_mixer <- lm(body_mass_g ~ flipper_length_mm + bill_depth_mm + species + sex,\n                  data = penguins)\ntidy(model_mixer)\n## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -1212.     568.       -2.13 3.36e- 2\n## 2 flipper_length_mm     17.5      2.87      6.12 2.66e- 9\n## 3 bill_depth_mm         74.4     19.7       3.77 1.91e- 4\n## 4 speciesChinstrap     -78.9     45.5      -1.73 8.38e- 2\n## 5 speciesGentoo       1154.     119.        9.73 8.02e-20\n## 6 sexmale              435.      44.8       9.72 8.79e-20\n```\n:::\n\n\nInterpreting these coefficients is a little different now, since we're working with multiple moving parts. In regular stats class, you've probably learned to say something like \"Holding all other variables constant, a 1 mm increase in flipper length is associated with a 17.5 gram increase in body mass, on average\" (slider) or \"Holding all other variables constant, Chinstrap penguins are 79 grams lighter than Adelie penguins, on average\" (switch). \n\nThis idea of \"holding everything constant\" though can be tricky to wrap your head around. Imagining this model like a mixer board can help, though. Pretend that you set the bill depth slider to some value (0, the average, whatever), you flip the Chinstrap and Gentoo switches off, you flip the male switch off, and then you slide only the flipper length switch up and down. You'd be looking at the marginal effect of flipper length for female Adelie penguins with an average (or 0 or whatever) length of bill depth. Stop moving the flipper length slider and start moving the bill depth slider and you'll see the marginal effect of bill depth for female Adelie penguins. Flip on the male switch and you'll see the marginal effect of bill depth for male Adelie penguins. Flip on the Gentoo switch and you'll see the marginal effect of bill depth for male Gentoo penguins. And so on.\n\nIn calculus, if you have a model like `model_slider` with just one continuous variable, the slope or derivative of that variable is the *total* derivative, or $\\frac{dy}{dx}$. If you have a model like `model_mixer` with lots of other variables, the slope or derivative of any of the individual explanatory variables is the *partial* derivative, or $\\frac{\\partial y}{\\partial x}$, where all other variables are held constant.\n\n\n## What are marginal effects?\n\nOops. When talking about these penguin regression results up there ↑ I used the term \"marginal effect,\" but we haven't officially defined it in the statistics world yet. It's tricky to do that, though, because there are so many synonyms and near synonyms for the idea of a statistical effect, like marginal effect, marginal mean, marginal slope, conditional effect, conditional marginal effect, and so on.\n\nFormally defined, a marginal effect is a partial derivative from a regression equation. It's the instantaneous slope of one of the explanatory variables in a model, with all the other variables held constant. If we continue with the mixing board analogy, it represents what would happen to the resulting audio levels if we set all sliders and switches to some stationary level and we moved just one slider up a tiny amount.\n\nHowever, in practice, people use the term \"marginal effect\" to mean a lot more than just a partial derivative. For instance, in a randomized controlled trial, the difference in group means between the treatment and control groups is often called a marginal effect (and sometimes called a conditional effect, or even a conditional marginal effect). The term is also often used to talk about other group differences, like differences in penguin weights across species.\n\nIn my mind, all these quasi-synonymous terms represent the same idea of a *statistical effect*, or what would happen to an outcome $y$ if one of the explanatory variables $x$ (be it continuous, categorical, or whatever) were different. The more precise terms like marginal effect, conditional effect, marginal mean, and so on, are variations on this theme. This is similar to how a square is a rectangle, but a rectangle is not a square—they're all super similar, but with minor subtle differences depending on the type of $x$ we're working with:\n\n- **Marginal effect**: the statistical effect for continuous explanatory variables; the partial derivative of a variable in a regression model; the effect of a single slider\n- **Conditional effect** or **group contrast**: the statistical effect for categorical explanatory variables; the difference in means when a condition is on vs. when it is off; the effect of a single switch\n\n\n## Slopes and marginal effects\n\nLet's look at true marginal effects, or the partial derivatives of continuous variables in a model (or sliders, in our slider/switch analogy). For the rest of this post, we'll move away from penguins and instead look at some cross-national data about the relationship between public sector corruption, the legal requirement to disclose donations to political campaigns, and respect for human rights, since that's all more related to what I do in my own research (I know nothing about penguins). We'll explore two different political science/policy questions:\n\n1. What is the relationship between a country's respect for civil liberties and its level of public sector corruption? Do countries that respect individual human rights tend to have less corruption too?\n2. Does a country's level of public sector corruption influence whether it has laws that require campaign finance disclosure? How does corruption influence a country's choice to be electorally transparent?\n\nWe'll use data from the [World Bank](https://data.worldbank.org/) and from the [Varieties of Democracy project](https://www.v-dem.net/) and just look at one year of data (2020) so we don't have to worry about panel data. There's [a great R package for accessing V-Dem data](https://github.com/vdeminstitute/vdemdata) without needing to download it manually from their website, but it's not on CRAN—it has to be installed from GitHub.\n\nV-Dem and the World Bank have hundreds of different variables, but we only need a few, and we'll make a few adjustments to the ones we do need. Here's what we'll do:\n\n- **Main continuous outcome** and **continuous explanatory variable**: Public sector corruption index (`v2x_pubcorr` in V-Dem). This is a 0–1 scale that measures…\n\n  > To what extent do public sector employees grant favors in exchange for bribes, kickbacks, or other material inducements, and how often do they steal, embezzle, or misappropriate public funds or other state resources for personal or family use?\n  \n  Higher values represent *worse* corruption.\n\n- **Main binary outcome**: Disclosure of campaign donations (`v2eldonate_ord` in V-Dem). This is an ordinal variable with these possible values:\n\n  > - 0: No. There are no disclosure requirements.\n  > - 1: Not really. There are some, possibly partial, disclosure requirements in place but they are not observed or enforced most of the time.\n  > - 2: Ambiguous. There are disclosure requirements in place, but it is unclear to what extent they are observed or enforced.\n  > - 3: Mostly. The disclosure requirements may not be fully comprehensive (some donations not covered), but most existing arrangements are observed and enforced.\n  > - 4: Yes. There are comprehensive requirements and they are observed and enforced almost all the time.\n  \n  For the sake of simplicity, we'll collapse this into a binary variable. Countries have disclosure laws if they score a 3 or a 4; they don't if they score a 0, 1, or 2.\n\n- **Other continuous explanatory variables**: \n\n  - Electoral democracy index, or polyarchy (`v2x_polyarchy` in V-Dem): a continuous variable measured from 0–1 with higher values representing greater achievement of democratic ideals\n  - Civil liberties index (`v2x_civlib` in V-Dem): a continuous variable measured from 0–1 with higher values representing better respect for human rights and civil liberties\n  - Log GDP per capita ([`NY.GDP.PCAP.KD`](https://data.worldbank.org/indicator/NY.GDP.PCAP.KD) at the World Bank): GDP per capita in constant 2015 USD\n\n- **Region**: V-Dem provides multiple regional variables with varying specificity (19 different regions, 10 different regions, and 6 different regions). We'll use the 6-region version (`e_regionpol_6C`) for simplicity here:\n\n  > - 1: Eastern Europe and Central Asia (including Mongolia)\n  > - 2: Latin America and the Caribbean\n  > - 3: The Middle East and North Africa (including Israel and Turkey, excluding Cyprus)\n  > - 4: Sub-Saharan Africa\n  > - 5: Western Europe and North America (including Cyprus, Australia and New Zealand)\n  > - 6: Asia and Pacific (excluding Australia and New Zealand)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get data from the World Bank's API\nwdi_raw <- WDI(country = \"all\", \n               indicator = c(population = \"SP.POP.TOTL\",\n                             gdp_percapita = \"NY.GDP.PCAP.KD\"), \n               start = 2000, end = 2020, extra = TRUE)\n\n# Clean up the World Bank data\nwdi_2020 <- wdi_raw |> \n  filter(region != \"Aggregates\") |> \n  filter(year == 2020) |> \n  mutate(log_gdp_percapita = log(gdp_percapita)) |> \n  select(-region, -status, -year, -country, -lastupdated, -lending)\n\n# Get data from V-Dem and clean it up\nvdem_2020 <- vdem %>% \n  select(country_name, country_text_id, year, region = e_regionpol_6C,\n         disclose_donations_ord = v2eldonate_ord, \n         public_sector_corruption = v2x_pubcorr,\n         polyarchy = v2x_polyarchy, civil_liberties = v2x_civlib) %>% \n  filter(year == 2020) %>% \n  mutate(disclose_donations = disclose_donations_ord >= 3,\n         disclose_donations = ifelse(is.na(disclose_donations), FALSE, disclose_donations)) %>% \n  # Scale these up so it's easier to talk about 1-unit changes\n  mutate(across(c(public_sector_corruption, polyarchy, civil_liberties), ~ . * 100)) |> \n  mutate(region = factor(region, \n                         labels = c(\"Eastern Europe and Central Asia\",\n                                    \"Latin America and the Caribbean\",\n                                    \"Middle East and North Africa\",\n                                    \"Sub-Saharan Africa\",\n                                    \"Western Europe and North America\",\n                                    \"Asia and Pacific\")))\n\n# Combine World Bank and V-Dem data into a single dataset\ncorruption <- vdem_2020 |> \n  left_join(wdi_2020, by = c(\"country_text_id\" = \"iso3c\")) |> \n  drop_na(gdp_percapita)\n\nglimpse(corruption)\n```\n:::\n\n\n::: {.callout-warning}\n### Static data!\n\nWhen I wrote this in May 2022, I based it on the data that both the World Bank and V-Dem had available at that time. In the months since then, they've revised their data. If you run all this code now (I'm writing this in February 2024), you'll get slightly different results because of these revisions. For instance, when this ran in 2022, the World Bank reported that Mexico's log GDP per capita in 2020 was 9.10; when running this in 2024, it reports that it was 9.13. This is a minor difference, but all these minor differences change the results slightly. The code in this chunk above still runs just fine, you'll just get slightly different data, and thus slightly different results.\n\nSo, in the interest of reproducibility, you should download and load this `.RData` file that contains `wdi_raw`, `wdi_2020`, `vdem_2020`, and `corruption` from when I wrote this in 2022.\n\n- [`data_2022.RData`](data_2022.Rdata)\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"data_2022.RData\")\nglimpse(corruption)\n## Rows: 168\n## Columns: 17\n## $ country_name             <chr> \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\",…\n## $ country_text_id          <chr> \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MMR\",…\n## $ year                     <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2…\n## $ region                   <fct> Latin America and the Caribbean, Latin America and the …\n## $ disclose_donations_ord   <dbl> 3, 1, 2, 0, 2, 1, 3, 2, 3, 2, 2, 0, 3, 3, 4, 3, 4, 2, 1…\n## $ public_sector_corruption <dbl> 48.8, 24.8, 1.3, 1.4, 65.2, 57.1, 3.7, 36.8, 70.6, 71.2…\n## $ polyarchy                <dbl> 64.7, 76.1, 90.8, 89.4, 72.0, 70.3, 83.2, 43.6, 26.2, 4…\n## $ civil_liberties          <dbl> 71.2, 87.7, 96.9, 94.8, 90.4, 82.2, 92.8, 56.9, 43.0, 8…\n## $ disclose_donations       <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, T…\n## $ iso2c                    <chr> \"MX\", \"SR\", \"SE\", \"CH\", \"GH\", \"ZA\", \"JP\", \"MM\", \"RU\", \"…\n## $ population               <dbl> 1.29e+08, 5.87e+05, 1.04e+07, 8.64e+06, 3.11e+07, 5.93e…\n## $ gdp_percapita            <dbl> 8923, 7530, 51542, 85685, 2021, 5659, 34556, 1587, 9711…\n## $ capital                  <chr> \"Mexico City\", \"Paramaribo\", \"Stockholm\", \"Bern\", \"Accr…\n## $ longitude                <chr> \"-99.1276\", \"-55.1679\", \"18.0645\", \"7.44821\", \"-0.20795…\n## $ latitude                 <chr> \"19.427\", \"5.8232\", \"59.3327\", \"46.948\", \"5.57045\", \"-2…\n## $ income                   <chr> \"Upper middle income\", \"Upper middle income\", \"High inc…\n## $ log_gdp_percapita        <dbl> 9.10, 8.93, 10.85, 11.36, 7.61, 8.64, 10.45, 7.37, 9.18…\n```\n:::\n\n\nLet's start off by looking at the effect of civil liberties on public sector corruption by using a really simple model with one explanatory variable:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_corruption <- corruption |> \n  mutate(highlight = civil_liberties == min(civil_liberties) | \n           civil_liberties == max(civil_liberties))\n\nggplot(plot_corruption, aes(x = civil_liberties, y = public_sector_corruption)) +\n  geom_point(aes(color = highlight)) +\n  stat_smooth(method = \"lm\", formula = y ~ x, linewidth = 1, color = clrs[1]) +\n  geom_label_repel(data = filter(plot_corruption, highlight == TRUE), \n                   aes(label = country_name), seed = 1234) +\n  scale_color_manual(values = c(\"grey30\", clrs[3]), guide = \"none\") +\n  labs(x = \"Civil liberties index\", y = \"Public sector corruption index\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-civlib-corruption-1.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_simple <- lm(public_sector_corruption ~ civil_liberties,\n                   data = corruption)\ntidy(model_simple)\n## # A tibble: 2 × 5\n##   term            estimate std.error statistic  p.value\n##   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)      102.       5.72        17.8 2.21e-40\n## 2 civil_liberties   -0.805    0.0779     -10.3 1.27e-19\n```\n:::\n\n\n\n\nWe have a nice fitted OLS line here with uncertainty around it. What's the marginal effect of civil liberties on public sector corruption? What kind of calculus and math do we need to do to find it? Not much, happily!\n\nIn general, we have a regression formula here that looks a lot like the $y = mx + b$ stuff we were using before, only now the intercept $b$ is $\\beta_0$ and the slope $m$ is $\\beta_1$. If we use the power rule to find the first derivative of this equation, we'll see that the slope of the entire line is $\\beta_1$:\n\n$$\n\\begin{aligned}\n\\operatorname{E}[y \\mid x] &= \\beta_0 + \\beta_1 x \\\\[4pt]\n\\frac{\\partial \\operatorname{E}[y \\mid x]}{\\partial x} &= \\beta_1\n\\end{aligned}\n$$\n\nIf we add actual coefficients from the model into the formula we can see that the $\\beta_1$ coefficient for `civil_liberties` (−0.80) is indeed the marginal effect:\n\n$$\n\\begin{aligned}\n\\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}] &= 101.89 + (−0.80 \\times \\text{Civil liberties}) \\\\[6pt]\n\\frac{\\partial \\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}]}{\\partial\\ \\text{Civil liberties}} &= −0.80\n\\end{aligned}\n$$\n\nThe $\\beta_1$ coefficient by itself is thus enough to tell us what the effect of moving civil liberties around is—it is *the* marginal effect of civil liberties on public sector corruption. Slide the civil liberties index up by 1 point and public sector corruption will be −0.80 points lower, on average.\n\nImportantly, this is only the case because we're using simple linear regression without any curvy parts. If your model is completely linear without any polynomials or logs or interaction terms or doesn't use curvy regression families like logistic or beta regression, you can use individual coefficients as marginal effects.\n\nLet's see what happens when we add curves. We'll add a polynomial term, including both `civil_liberties` and `civil_liberties^2` so that we can capture the parabolic shape of the relationship between civil liberties and corruption:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(plot_corruption, aes(x = civil_liberties, y = public_sector_corruption)) +\n  geom_point(aes(color = highlight)) +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), linewidth = 1, color = clrs[2]) +\n  geom_label_repel(data = filter(plot_corruption, highlight == TRUE), \n                   aes(label = country_name), seed = 1234) +\n  scale_color_manual(values = c(\"grey30\", clrs[3]), guide = \"none\") +\n  labs(x = \"Civil liberties index\", y = \"Public sector corruption index\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-civlib-corruption-sq-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThis is most likely not a great model fit in real life, but using the quadratic term here makes a neat curved line, so we'll go with it for the sake of the example. But don't, like, make any policy decisions based on this line.\n\nWhen working with polynomials in regression, the coefficients appear and work a little differently:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_sq <- lm(public_sector_corruption ~ civil_liberties + I(civil_liberties^2),\n               data = corruption)\ntidy(model_sq)\n## # A tibble: 3 × 5\n##   term                 estimate std.error statistic      p.value\n##   <chr>                   <dbl>     <dbl>     <dbl>        <dbl>\n## 1 (Intercept)           41.9     11.6          3.60 0.000427    \n## 2 civil_liberties        1.58     0.419        3.77 0.000230    \n## 3 I(civil_liberties^2)  -0.0197   0.00341     -5.77 0.0000000382\n```\n:::\n\n\n\n\nWe now have two coefficients for civil liberties: $\\beta_1$ and $\\beta_2$. Importantly, we cannot use just one of these to talk about the marginal effect of changing civil liberties. A one-point increase in the civil liberties index is *not* associated with a 1.58 increase or a −0.02 decrease in corruption. The slope of the fitted line now comprises multiple moving parts: (1) the coefficient for the non-squared term, (2) the coefficient for the squared term, and (3) some value of civil liberties, since the slope isn't the same across the whole line. The math shows us why and how. \n\nWe have terms for both $x$ and $x^2$ in our model. To find the derivative, we can use the power rule to get rid of the $x$ term ($\\beta x^1 \\rightarrow (1 \\times \\beta x^0) \\rightarrow \\beta$), but the $x$ in the $x^2$ term doesn't disappear ($\\beta x^2 \\rightarrow (2 \\times \\beta \\times x^1) \\rightarrow 2 \\beta x$). The slope of the line thus depends on both the βs and the $x$:\n\n$$\n\\begin{aligned}\n\\operatorname{E}[y \\mid x] &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 \\\\[4pt]\n\\frac{\\partial \\operatorname{E}[y \\mid x]}{\\partial x} &= \\beta_1 + 2 \\beta_2 x\n\\end{aligned}\n$$\n\nHere's what that looks like with the results of our civil liberties and corruption model:\n\n$$\n\\begin{aligned}\n\\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}] &= 41.86 + (1.58 \\times \\text{Civil liberties}) + (−0.02 \\times \\text{Civil liberties}^2) \\\\[6pt]\n\\frac{\\partial \\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}]}{\\partial\\ \\text{Civil liberties}} &= 1.58 + (2\\times −0.02 \\times \\text{Civil liberties})\n\\end{aligned}\n$$\n\nBecause the actual slope depends on the value of civil liberties, we need to plug in different values to get the instantaneous slopes at each value. Let's plug in 25, 55, and 80, for fun:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract the two civil_liberties coefficients\nciv_lib1 <- tidy(model_sq) |> filter(term == \"civil_liberties\") |> pull(estimate)\nciv_lib2 <- tidy(model_sq) |> filter(term == \"I(civil_liberties^2)\") |> pull(estimate)\n\n# Make a little function to do the math\nciv_lib_slope <- function(x) civ_lib1 + (2 * civ_lib2 * x)\n\nciv_lib_slope(c(25, 55, 80))\n## [1]  0.594 -0.587 -1.572\n```\n:::\n\n\nWe have three different slopes now: 0.59, −0.59, and −1.57 for civil liberties of 25, 55, and 80, respectively. We can plot these as tangent lines:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntangents <- model_sq |> \n  augment(newdata = tibble(civil_liberties = c(25, 55, 80))) |> \n  mutate(slope = civ_lib_slope(civil_liberties),\n         intercept = find_intercept(civil_liberties, .fitted, slope)) |> \n  mutate(nice_label = glue(\"Civil liberties: {civil_liberties}<br>\",\n                           \"Fitted corruption: {nice_number(.fitted)}<br>\",\n                           \"Slope: **{nice_number(slope)}**\"))\n\nggplot(corruption, aes(x = civil_liberties, y = public_sector_corruption)) +\n  geom_point(color = \"grey30\") +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), linewidth = 1, se = FALSE, color = clrs[4]) +\n  geom_abline(data = tangents, aes(slope = slope, intercept = intercept), \n              linewidth = 0.5, color = clrs[2], linetype = \"21\") +\n  geom_point(data = tangents, aes(x = civil_liberties, y = .fitted), size = 4, shape = 18, color = clrs[2]) +\n  geom_richtext(data = tangents, aes(x = civil_liberties, y = .fitted, label = nice_label), nudge_y = -7) +\n  labs(x = \"Civil liberties index\", y = \"Public sector corruption index\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-civlib-slopes-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nDoing the calculus by hand here is tedious though, especially once we start working with even more covariates in a model. Plus we don't have any information about uncertainty, like standard errors and confidence intervals. There are official mathy ways to figure those out by hand, but who even wants to do that. Fortunately there are two different packages that let us find marginal slopes automatically, with important differences in their procedures, which we'll explore in detail below. But before looking at their differences, let's first see how they work.\n\nFirst, we can use the `slopes()` function from **marginaleffects** to see the slope (the `estimate` column here) at various levels of civil liberties. We'll look at the mechanics of this function in more detail in the next section—for now we'll just plug in our three values of civil liberties and see what happens. We'll also set the `eps` argument: behind the scenes, `slopes()` doesn't actually do the by-hand calculus of piecing together first derivatives—instead, it calculates the fitted value of corruption when civil liberties is a value, calculates the fitted value of corruption when civil liberties is that same value plus a tiny bit more, and then subtracts them. The `eps` value controls that tiny amount. In this case, it'll calculate the predictions for `civil_liberties = 25` and `civil_liberties = 25.001` and then find the slope of the tiny tangent line between those two points. It's a neat little mathy trick to avoid calculus.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_sq |> \n  slopes(newdata = datagrid(civil_liberties = c(25, 55, 80)),\n         eps = 0.001)\n## \n##             Term Estimate Std. Error      z Pr(>|z|)   2.5 % 97.5 % civil_liberties\n##  civil_liberties    0.594     0.2528   2.35   0.0187  0.0989  1.090              25\n##  civil_liberties   -0.587     0.0806  -7.28   <0.001 -0.7452 -0.429              55\n##  civil_liberties   -1.572     0.1509 -10.42   <0.001 -1.8676 -1.276              80\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, public_sector_corruption, civil_liberties\n```\n:::\n\n\nSecond, we can use the `emtrends()` function from **emmeans** to also see the slope (the `civil_liberties.trend` column here) at various levels of civil liberties. The syntax is different (note the `delta.var` argument instead of `eps`), but the results are essentially the same:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_sq |> \n  emtrends(~ civil_liberties, var = \"civil_liberties\",\n           at = list(civil_liberties = c(25, 55, 80)),\n           delta.var = 0.001)\n##  civil_liberties civil_liberties.trend     SE  df lower.CL upper.CL\n##               25                 0.594 0.2527 165    0.095    1.093\n##               55                -0.587 0.0806 165   -0.746   -0.428\n##               80                -1.572 0.1509 165   -1.870   -1.274\n## \n## Confidence level used: 0.95\n```\n:::\n\n\nBoth `slopes()` and `emtrends()` also helpfully provide uncertainty, with standard errors and confidence intervals, with a lot of super fancy math behind the scenes to make it all work. `slopes()` provides p-values automatically; if you want p-values from `emtrends()` you need to wrap it in `test()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_sq |> \n  emtrends(~ civil_liberties, var = \"civil_liberties\",\n           at = list(civil_liberties = c(25, 55, 80)),\n           delta.var = 0.001) |> \n  test()\n##  civil_liberties civil_liberties.trend     SE  df t.ratio p.value\n##               25                 0.594 0.2527 165   2.350  0.0198\n##               55                -0.587 0.0806 165  -7.280  <.0001\n##               80                -1.572 0.1509 165 -10.420  <.0001\n```\n:::\n\n\nAnother neat thing about these more automatic functions is that we can use them to create a marginal effects plot, placing the value of the slope on the y-axis rather than the fitted value of public corruption. **marginaleffects** helpfully has `plot_slopes()` that will plot the values of `estimate` across the whole range of civil liberties automatically. Alternatively, if we want full control over the plot, we can use either `slopes()` or `emtrends()` to create a data frame that we can plot ourselves with ggplot:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\n# Automatic plot from marginaleffects::plot_slopes()\nmfx_marginaleffects_auto <- plot_slopes(model_sq, \n                                        variables = \"civil_liberties\", \n                                        condition = \"civil_liberties\") +\n  labs(x = \"Civil liberties\", y = \"Marginal effect of civil liberties on public sector corruption\",\n       subtitle = \"Created automatically with marginaleffects::plot_slopes()\") +\n  theme_mfx()\n\n# Piece all the geoms together manually with results from marginaleffects::slopes()\nmfx_marginaleffects <- model_sq |> \n  slopes(newdata = datagrid(civil_liberties = \n                              seq(min(corruption$civil_liberties), \n                                 max(corruption$civil_liberties), 0.1)),\n                  eps = 0.001) |> \n  ggplot(aes(x = civil_liberties, y = estimate)) +\n  geom_vline(xintercept = 42, color = clrs[3], linewidth = 0.5, linetype = \"24\") +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.1, fill = clrs[1]) +\n  geom_line(linewidth = 1, color = clrs[1]) +\n  labs(x = \"Civil liberties\", y = \"Marginal effect of civil liberties on public sector corruption\",\n       subtitle = \"Calculated with slopes()\") +\n  theme_mfx()\n\n# Piece all the geoms together manually with results from emmeans::emtrends()\nmfx_emtrends <- model_sq |> \n  emtrends(~ civil_liberties, var = \"civil_liberties\",\n           at = list(civil_liberties = \n                       seq(min(corruption$civil_liberties), \n                           max(corruption$civil_liberties), 0.1)),\n           delta.var = 0.001) |> \n  as_tibble() |> \n  ggplot(aes(x = civil_liberties, y = civil_liberties.trend)) +\n  geom_vline(xintercept = 42, color = clrs[3], linewidth = 0.5, linetype = \"24\") +\n  geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL), alpha = 0.1, fill = clrs[1]) +\n  geom_line(linewidth = 1, color = clrs[1]) +\n  labs(x = \"Civil liberties\", y = \"Marginal effect of civil liberties on public sector corruption\",\n       subtitle = \"Calculated with emtrends()\") +\n  theme_mfx()\n\nmfx_marginaleffects_auto | mfx_marginaleffects | mfx_emtrends\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-cmes-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThis kind of plot is useful since it shows precisely how the effect changes across civil liberties. The slope is 0 at around 42, positive before that, and negative after that, which—assuming this is a good model and who even knows if that's true—implies that countries with low levels of respect for civil liberties will see an increase in corruption as civil liberties increases, while countries with high respect for civil liberties will see a decrease in corruption as they improve their respect for human rights.\n\n\n## **marginaleffects**'s and **emmeans**'s philosophies of averaging\n\nFinding marginal effects for lines like $y = 2x - 1$ and $y = -0.5x^2 + 5x + 5$ with calculus is fairly easy since there's no uncertainty involved. Finding marginal effects for fitted lines from a regression model, on the other hand, is more complicated because uncertainty abounds. The estimated partial slopes all have standard errors and measures of statistical significance attached to them. The slope of civil liberties at 55 is −0.59, but it could be higher and it could be lower. Could it even possibly be zero? Maybe! (But most likely not; the p-value that we saw above is less than 0.001, so there's only a sliver of a chance of seeing a slope like −0.59 in a world where it is actually 0ish).\n\nWe deal with the uncertainty of these marginal effects by taking averages, which is why we talk about \"average marginal effects\" when interpreting these effects. So far, `marginaleffects::slopes()` and `emmeans::emtrends()` have given identical results. But behind the scenes, these packages take two different approaches to calculating these marginal averages. The difference is very subtle, but incredibly important.\n\nLet's look at how these two packages calculate their marginal effects by default.\n\n### Average marginal effects (the default in **marginaleffects**)\n\nBy default, **marginaleffects** calculates the *average marginal effect* (AME) for its partial slopes/coefficients. To do this, it follows a specific process of averaging:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/flow-ame@3x.png){fig-align='center' width=100%}\n:::\n:::\n\n\nIt first plugs each row of the original dataset into the model and generates predictions for each row. It then uses fancy math (i.e. adding 0.001) to calculate the instantaneous slope for each row and stores each individual slope in the `estimate` column here:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmfx_sq <- slopes(model_sq)\nhead(mfx_sq)\n## \n##             Term Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %\n##  civil_liberties    -1.23      0.102 -12.02   <0.001 -1.43  -1.03\n##  civil_liberties    -1.88      0.199  -9.43   <0.001 -2.26  -1.49\n##  civil_liberties    -2.24      0.258  -8.66   <0.001 -2.74  -1.73\n##  civil_liberties    -2.16      0.245  -8.81   <0.001 -2.63  -1.68\n##  civil_liberties    -1.98      0.216  -9.17   <0.001 -2.41  -1.56\n##  civil_liberties    -1.66      0.164 -10.10   <0.001 -1.98  -1.34\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, public_sector_corruption, civil_liberties\n```\n:::\n\n\nIt finally calculates the average of the `estimate` column. We can do that ourselves:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmfx_sq |> \n  group_by(term) |> \n  summarize(avg_slope = mean(estimate))\n## # A tibble: 1 × 2\n##   term            avg_slope\n##   <chr>               <dbl>\n## 1 civil_liberties     -1.17\n```\n:::\n\n\nOr we can feed a `marginaleffects` object to `summary()`, which will calculate the correct uncertainty statistics, like the standard errors:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(mfx_sq)\n## \n##             Term    Contrast Estimate Std. Error     z Pr(>|z|) 2.5 % 97.5 %\n##  civil_liberties mean(dY/dX)    -1.17     0.0948 -12.3   <0.001 -1.35  -0.98\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n```\n:::\n\n\nAlternatively, we can use `avg_slopes()` instead of `slopes(...) |> summary()` to do the averaging automatically:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\navg_slopes(model_sq)\n## \n##             Term Estimate Std. Error     z Pr(>|z|) 2.5 % 97.5 %\n##  civil_liberties    -1.17     0.0948 -12.3   <0.001 -1.35  -0.98\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n```\n:::\n\n\nNote that the average marginal effect here isn't the same as what we saw before when we set civil liberties to different values. In this case, the effect is averaged across the whole range of civil liberties—one single grand average mean. It shows that in general, the overall average slope of the fitted line is −1.17. \n\nDon't worry about the number too much here—we're just exploring the underlying process of calculating this average marginal effect. In general, as the image shows above, for average marginal effects, we take the full original data, feed it to the model, generate fitted values for each original row, and then collapse the results into a single value.\n\nThe main advantage of doing this is that each `estimate` prediction uses values that exist in the actual data. The first `estimate` slope estimate is for Mexico in 2020 and is based on Mexico's actual value of `civil_liberties` (and any other covariates if we had included any others in the model). It's thus more reflective of reality.\n\n### Marginal effects at the mean (the default in **emmeans**)\n\nA different approach for this averaging is to calculate the *marginal effect at the mean*, or MEM. This is what the **emmeans** package does by default. (The **emmeans** package actually calculates two average things: \"marginal effects at the means\" (MEM), or average *slopes* using `emtrends()`, and \"estimated marginal means\" (EMM), or average *predictions* using `emmeans()`. It's named after the second of these, hence the name ***emm*eans**). \n\nTo do this, we follow a slightly different process of averaging:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/flow-mem@3x.png){fig-align='center' width=100%}\n:::\n:::\n\n\nFirst, we calculate the average value of each of the covariates in the model (in this case, just `civil_liberties`):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\navg_civ_lib <- mean(corruption$civil_liberties)\navg_civ_lib\n## [1] 69.7\n```\n:::\n\n\nWe then plug that average (and that average plus 0.001) into the model and generate fitted values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nciv_lib_fitted <- model_sq |> \n  augment(newdata = tibble(civil_liberties = c(avg_civ_lib, avg_civ_lib + 0.001)))\nciv_lib_fitted\n## # A tibble: 2 × 2\n##   civil_liberties .fitted\n##             <dbl>   <dbl>\n## 1            69.7    56.3\n## 2            69.7    56.3\n```\n:::\n\n\nBecause of rounding (and because the values are so tiny), this looks like the two rows are identical, but they're not—the second one really is 0.001 more than 69.682.\n\nWe then subtract the two and divide by 0.001 to get the final marginal effect at the mean:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(civ_lib_fitted[2,2] - civ_lib_fitted[1,2]) / 0.001\n##   .fitted\n## 1   -1.17\n```\n:::\n\n\nThat doesn't give us any standard errors or uncertainty or anything, so it's better to use `emtrends()` or `slopes()`. `emtrends()` calculates this MEM automatically:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_sq |> \n  emtrends(~ civil_liberties, var = \"civil_liberties\", delta.var = 0.001)\n##  civil_liberties civil_liberties.trend     SE  df lower.CL upper.CL\n##             69.7                 -1.17 0.0948 165    -1.35   -0.978\n## \n## Confidence level used: 0.95\n```\n:::\n\n\nWe can also calculate the MEM with `avg_slopes()` if we include the `newdata = \"mean\"` argument, which will automatically shrink the original data down into average or typical values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_sq |> \n  avg_slopes(newdata = \"mean\")\n## \n##             Term Estimate Std. Error     z Pr(>|z|) 2.5 % 97.5 %\n##  civil_liberties    -1.17     0.0948 -12.3   <0.001 -1.35  -0.98\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n```\n:::\n\n\nThe disadvantage of this approach is that no actual country has a `civil_liberties` score of exactly 69.682. If we had other covariates in the model, no country would have exactly the average of every variable. The marginal effect is thus calculated based on a hypothetical country that might not possibly exist in real life.\n\n\n## Where this subtle difference really matters\n\nSo far, comparing average marginal effects (AME) with marginal effects at the mean (MEM) hasn't been that useful, since both `slopes()` and `emtrends()` provided nearly identical results with our simple model with civil liberties squared. That's because nothing that strange is going on in the model—there are no additional explanatory variables, no interactions or logs, and we're using OLS and not anything fancy like logistic regression or beta regression.\n\nThings change once we leave the land of OLS.\n\nLet's make a new model that predicts if a country has campaign finance disclosure laws based on public sector corruption. Disclosure laws is a binary outcome, so we'll use logistic regression to constrain the fitted values and predictions to between 0 and 1. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_corruption_logit <- corruption |> \n  mutate(highlight = public_sector_corruption == min(public_sector_corruption) | \n           public_sector_corruption == max(public_sector_corruption))\n  \nggplot(plot_corruption_logit, \n       aes(x = public_sector_corruption, y = as.numeric(disclose_donations))) +\n  geom_point(aes(color = highlight)) +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link = \"logit\")),\n              color = clrs[2]) +\n  geom_label(data = slice(filter(plot_corruption_logit, highlight == TRUE), 1), \n             aes(label = country_name), nudge_y = 0.06, hjust = 1) +\n  geom_label(data = slice(filter(plot_corruption_logit, highlight == TRUE), 2), \n             aes(label = country_name), nudge_y = -0.06, hjust = 0) +\n  scale_color_manual(values = c(\"grey30\", clrs[3]), guide = \"none\") +\n  labs(x = \"Public sector corruption\", \n       y = \"Presence or absence of\\ncampaign finance disclosure laws\\n(Line shows predicted probability)\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-logit-corruption-donations-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nEven without any squared terms, we're already in non-linear land. We can build a model and explore this relationship:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit <- glm(\n  disclose_donations ~ public_sector_corruption,\n  family = binomial(link = \"logit\"),\n  data = corruption\n)\n\ntidy(model_logit)\n## # A tibble: 2 × 5\n##   term                     estimate std.error statistic  p.value\n##   <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)                1.98     0.388        5.09 3.51e- 7\n## 2 public_sector_corruption  -0.0678   0.00991     -6.84 7.85e-12\n```\n:::\n\n\n\n\nThe coefficients here are on a different scale and are measured in log odds units (or logits), not probabilities or percentage points. That means we can't use those coefficients directly. We can't say things like \"a one-unit increase in public sector corruption is associated with a −0.068 percentage point decrease in the probability of having a disclosure law.\" That's wrong! We have to convert those logit scale coefficients to a probability scale instead. We can do this mathematically by combining both the intercept and the coefficient using `plogis(intercept + coefficient) - plogis(intercept)`, but that's generally not recommended, especially when there are other coefficients (see [this section on logistic regression for more details](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#2-fractional-logistic-regression)). Additionally, manually combining intercepts and coefficients won't give us standard errors or any other kind of uncertainty. \n\nInstead, we can calculate the average slope of the logistic regression fit using either `slopes()` or `emtrends()`.\n\nFirst we'll use `avg_slopes()`. Remember that it calculates the *average marginal effect* (AME) by plugging each row of the original data into the model, generating predictions and instantaneous slopes for each row, and then averaging the `estimate` column. Each row contains actual observed data, so the predictions arguably reflect variation in reality. `avg_slopes()` helpfully converts the AME into percentage points, so we can interpret the value directly.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit |> \n  avg_slopes()\n## \n##                      Term Estimate Std. Error     z Pr(>|z|)    2.5 %   97.5 %\n##  public_sector_corruption -0.00846   0.000261 -32.4   <0.001 -0.00897 -0.00795\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n```\n:::\n\n\n\n\nThe average marginal effect for public sector corruption is −0.0085, which means that on average, a one-point increase in the public sector corruption index (i.e. as corruption gets worse) is associated with a −0.85 percentage point decrease in the probability of a country having a disclosure law.\n\nNext we'll use `emtrends()`, which calculates the *marginal effect at the mean* (MEM) by averaging all the model covariates first, plugging those averages into the model, and generating a single instantaneous slope. The values that get plugged into the model won't necessarily reflect reality—especially once more covariates are involved, which we'll see later. By default `emtrends()` returns the results on the logit scale, but we can convert them to the response/percentage point scale by adding the `regrid = \"response\"` argument:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit |> \n  emtrends(~ public_sector_corruption, \n           var = \"public_sector_corruption\", \n           regrid = \"response\")\n##  public_sector_corruption public_sector_corruption.trend     SE  df asymp.LCL asymp.UCL\n##                      45.8                        -0.0125 0.0017 Inf   -0.0158  -0.00916\n## \n## Confidence level used: 0.95\n\n# avg_slopes() will show the same MEM result with `newdata = \"mean\"`\n# avg_slopes(model_logit, newdata = \"mean\")\n```\n:::\n\n\n\n\nWhen we plug the average public sector corruption (45.82) into the model, we get an MEM of −0.0125, which means that on average, a one-point increase in the public sector corruption index is associated with a −1.25 percentage point decrease in the probability of a country having a disclosure law. That's different (and bigger!) than the AME we found with `slopes()`!\n\nLet's plot these marginal effects and their uncertainty to see how much they differ:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get tidied results from slopes()\nplot_ame <- model_logit |> \n  avg_slopes()\n\n# Get tidied results from emtrends()\nplot_mem <- model_logit |> \n  emtrends(~ public_sector_corruption, \n           var = \"public_sector_corruption\", \n           regrid = \"response\") |> \n  tidy(conf.int = TRUE) |> \n  rename(estimate = public_sector_corruption.trend)\n\n# Combine the two tidy data frames for plotting\nplot_effects <- bind_rows(\"AME\" = plot_ame, \"MEM\" = plot_mem, .id = \"type\") |> \n  mutate(nice_slope = nice_number(estimate * 100))\n\nggplot(plot_effects, aes(x = estimate * 100, y = fct_rev(type), color = type)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[1]) +\n  geom_pointrange(aes(xmin = conf.low * 100, xmax = conf.high * 100)) +\n  geom_label(aes(label = nice_slope), nudge_y = 0.3) +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL) +\n  scale_color_manual(values = c(clrs[2], clrs[5]), guide = \"none\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-mfx-ame-mem-basic-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThat's fascinating! The confidence interval around the AME is really small compared to the MEM, likely because the AME estimate comes from the average of 168 values, while the MEM is the prediction of a single value. Additionally, while both estimates hover around a 1 percentage point decrease, the AME is larger than −1 while the MEM is smaller.\n\nFor fun, let's make a super fancy logistic regression model with a quadratic term and an interaction. We'll compare the AME and MEM for public sector corruption again. This is where either `slopes()` or `emtrends()` is incredibly helpful—correctly combining all the necessary coefficients, given that corruption is both squared and interacted, and given that there are other variables to worry about, would be really hard.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy <- glm(\n  disclose_donations ~ public_sector_corruption + I(public_sector_corruption^2) + \n    polyarchy + log_gdp_percapita + public_sector_corruption * region,\n  family = binomial(link = \"logit\"),\n  data = corruption\n)\n```\n:::\n\n\nHere are the average marginal effects (AME) (again, each original row is plugged into the model, a slope is calculated for each, and then they're all averaged together):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  avg_slopes()\n## \n##                      Term                                                           Contrast Estimate Std. Error      z Pr(>|z|)   2.5 %   97.5 %\n##  log_gdp_percapita        dY/dX                                                               0.00960    0.03784  0.254  0.79977 -0.0646  0.08377\n##  polyarchy                dY/dX                                                               0.00226    0.00172  1.318  0.18757 -0.0011  0.00563\n##  public_sector_corruption dY/dX                                                              -0.00653    0.00198 -3.303  < 0.001 -0.0104 -0.00266\n##  region                   Asia and Pacific - Eastern Europe and Central Asia                 -0.20418    0.09156 -2.230  0.02575 -0.3836 -0.02473\n##  region                   Latin America and the Caribbean - Eastern Europe and Central Asia  -0.26174    0.10447 -2.505  0.01223 -0.4665 -0.05699\n##  region                   Middle East and North Africa - Eastern Europe and Central Asia     -0.20647    0.10545 -1.958  0.05023 -0.4132  0.00021\n##  region                   Sub-Saharan Africa - Eastern Europe and Central Asia               -0.24986    0.11238 -2.223  0.02619 -0.4701 -0.02960\n##  region                   Western Europe and North America - Eastern Europe and Central Asia -0.29109    0.09378 -3.104  0.00191 -0.4749 -0.10728\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n```\n:::\n\n\nAnd here are the marginal effects at the mean (MEM) (again, the average values for each covariate are plugged into the model). Using `emtrends()` results in a note about interactions, so we'll use `avg_slopes(..., newdata = \"mean\")` instead:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  emtrends(~ public_sector_corruption, \n           var = \"public_sector_corruption\", \n           regrid = \"response\")\n## NOTE: Results may be misleading due to involvement in interactions\n##  public_sector_corruption public_sector_corruption.trend      SE  df asymp.LCL asymp.UCL\n##                      45.8                       -0.00955 0.00301 Inf   -0.0155  -0.00366\n## \n## Results are averaged over the levels of: region \n## Confidence level used: 0.95\n\n# This uses avg_slopes() to find the MEM instead\nmodel_logit_fancy |> \n  avg_slopes(newdata = \"mean\")\n## \n##                      Term                                                           Contrast Estimate Std. Error      z Pr(>|z|)    2.5 %   97.5 %\n##  log_gdp_percapita        dY/dX                                                               0.00919    0.03944  0.233  0.81579 -0.06811  0.08649\n##  polyarchy                dY/dX                                                               0.00217    0.00222  0.976  0.32915 -0.00219  0.00652\n##  public_sector_corruption dY/dX                                                              -0.01004    0.00607 -1.654  0.09816 -0.02194  0.00186\n##  region                   Asia and Pacific - Eastern Europe and Central Asia                 -0.53122    0.19300 -2.752  0.00592 -0.90950 -0.15294\n##  region                   Latin America and the Caribbean - Eastern Europe and Central Asia  -0.37448    0.16569 -2.260  0.02381 -0.69921 -0.04974\n##  region                   Middle East and North Africa - Eastern Europe and Central Asia     -0.57907    0.20653 -2.804  0.00505 -0.98387 -0.17427\n##  region                   Sub-Saharan Africa - Eastern Europe and Central Asia               -0.56591    0.16579 -3.413  < 0.001 -0.89086 -0.24097\n##  region                   Western Europe and North America - Eastern Europe and Central Asia -0.65700    0.16341 -4.021  < 0.001 -0.97728 -0.33672\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n```\n:::\n\n\nNow that we're working with multiple covariates, we have instantaneous marginal effects for each regression term, which is neat. We only care about corruption here, so let's extract the slopes and plot them:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_ame_fancy <- model_logit_fancy |> \n  avg_slopes()\n\nplot_mem_fancy <- model_logit_fancy |> \n  avg_slopes(newdata = \"mean\")\n\n# Combine the two tidy data frames for plotting\nplot_effects <- bind_rows(\"AME\" = plot_ame_fancy, \"MEM\" = plot_mem_fancy, .id = \"type\") |> \n  filter(term == \"public_sector_corruption\") |> \n  mutate(nice_slope = nice_number(estimate * 100))\n\nggplot(plot_effects, aes(x = estimate * 100, y = fct_rev(type), color = type)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[1]) +\n  geom_pointrange(aes(xmin = conf.low * 100, xmax = conf.high * 100)) +\n  geom_label(aes(label = nice_slope), nudge_y = 0.3) +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL) +\n  scale_color_manual(values = c(clrs[2], clrs[5]), guide = \"none\") +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-mfx-ame-mem-fancy-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\nYikes! The AME is statistically significant (p < 0.001) with a narrower confidence interval, but the MEM includes zero in its confidence interval and isn't significant (p = 0.098).\n\nThe choice of marginal effect averaging thus matters a lot!\n\n\n## Other marginal slope things\n\nTo make life even more exciting, we're not limited to just average marginal effects (AMEs) or marginal effects at the mean (MEMs). Additionally, if we think back to the slider/switch/mixing board analogy, all we've really done so far with our logistic regression model is move one slider (`public_sector_corruption`) up and down. What happens if we move other switches and sliders at the same time? (i.e. the marginal effect of corruption at specific values of corruption, or across different regions, or at different levels of GDP per capita and polyarchy)\n\nWe can use both `slopes()` and `emtrends()`/`emmeans()` to play with our model's full mixing board. We'll continue to use the logistic regression model as an example since it's sensitive to the order of averaging.\n\n\n### Group average marginal effects\n\nIf we have categorical covariates in our model like `region`, we can find the average marginal effect (AME) of continuous predictors across those different groups. This is fairly straightforward when working with `slopes()` because of its approach to averaging. Remember that with the AME, each original row gets its own fitted value and each individual slope, which we can then average and collapse into a single row. Group characteristics like region are maintained after calculating predictions, so we can calculate group averages of the individual slopes. This outlines the process:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/flow-game@3x.png){fig-align='center' width=100%}\n:::\n:::\n\n\nBecause we're working with the AME, we have an `estimate` column with instantaneous slopes for each row in the original data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# We'll specify variables = \"public_sector_corruption\" here to filter the\n# marginal effects results. If we don't we'll get dozens of separate marginal\n# effects later when using summary() for each of the coefficients, interactions,\n# and cross-region contrasts\nmfx_logit_fancy <- model_logit_fancy |> \n  slopes(variables = \"public_sector_corruption\")\n\n# Original data frame + estimated slope for each row\nhead(mfx_logit_fancy)\n## \n##                      Term  Estimate Std. Error      z Pr(>|z|)    2.5 %  97.5 %\n##  public_sector_corruption -0.008825    0.00653 -1.352    0.176 -0.02162 0.00397\n##  public_sector_corruption -0.000897    0.00788 -0.114    0.909 -0.01634 0.01454\n##  public_sector_corruption -0.006759    0.00511 -1.324    0.186 -0.01677 0.00325\n##  public_sector_corruption -0.006727    0.00546 -1.231    0.218 -0.01744 0.00398\n##  public_sector_corruption -0.002460    0.00307 -0.802    0.423 -0.00847 0.00355\n##  public_sector_corruption -0.005864    0.00557 -1.052    0.293 -0.01679 0.00506\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, disclose_donations, public_sector_corruption, polyarchy, log_gdp_percapita, region\n```\n:::\n\n\nAll the original columns are still there, which means we can collapse the results however we want. For instance, here's the average marginal effect across each region:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmfx_logit_fancy |> \n  group_by(region) |> \n  summarize(region_ame = mean(estimate))\n## # A tibble: 6 × 2\n##   region                           region_ame\n##   <fct>                                 <dbl>\n## 1 Eastern Europe and Central Asia    -0.00751\n## 2 Latin America and the Caribbean    -0.00326\n## 3 Middle East and North Africa       -0.00629\n## 4 Sub-Saharan Africa                 -0.00435\n## 5 Western Europe and North America   -0.0112 \n## 6 Asia and Pacific                   -0.00830\n```\n:::\n\n\nWe can also use summarizing methods built in to **marginaleffects** by using the `by` argument in `slopes()`. This is the better option, since it does some tricky standard error calculations behind the scenes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  slopes(variables = \"public_sector_corruption\",\n         by = \"region\")\n## \n##                      Term    Contrast                           region Estimate Std. Error      z Pr(>|z|)   2.5 %   97.5 %\n##  public_sector_corruption mean(dY/dX) Eastern Europe and Central Asia  -0.00751    0.00222 -3.376  < 0.001 -0.0119 -0.00315\n##  public_sector_corruption mean(dY/dX) Latin America and the Caribbean  -0.00326    0.00395 -0.825  0.40957 -0.0110  0.00448\n##  public_sector_corruption mean(dY/dX) Middle East and North Africa     -0.00629    0.00193 -3.264  0.00110 -0.0101 -0.00251\n##  public_sector_corruption mean(dY/dX) Sub-Saharan Africa               -0.00435    0.00156 -2.788  0.00530 -0.0074 -0.00129\n##  public_sector_corruption mean(dY/dX) Western Europe and North America -0.01123    0.00911 -1.233  0.21749 -0.0291  0.00662\n##  public_sector_corruption mean(dY/dX) Asia and Pacific                 -0.00830    0.00285 -2.916  0.00354 -0.0139 -0.00272\n## \n## Columns: term, contrast, region, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n```\n:::\n\n\n\n\nThese are on the percentage point scale, not the logit scale, so we can interpret them directly. In Western Europe, the AME of corruption is −0.0033, so a one-point increase in the public sector corruption index there is associated with a −0.33 percentage point decrease in the probability of having a campaign finance disclosure law, on average (though it's not actually significant (p = 0.410)). In the Middle East, on the other hand, corruption seems to matter less for disclosure laws—an increase in the corruption index there is associated with a −0.83 percentage point decrease in the probability of having a laws, on average (and that *is* significant (p = 0.004)).\n\nWe can use `emtrends()` to get region-specific slopes, but we'll get different results because of the order of averaging. **emmeans** creates averages and then plugs them in; **marginaleffects** plugs all the values in and then creates averages:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  emtrends(~ public_sector_corruption + region,\n           var = \"public_sector_corruption\", regrid = \"response\")\n##  public_sector_corruption region                           public_sector_corruption.trend      SE  df asymp.LCL asymp.UCL\n##                      45.8 Eastern Europe and Central Asia                        -0.01119 0.00554 Inf   -0.0221  -0.00033\n##                      45.8 Latin America and the Caribbean                        -0.00734 0.00611 Inf   -0.0193   0.00463\n##                      45.8 Middle East and North Africa                           -0.01172 0.01105 Inf   -0.0334   0.00993\n##                      45.8 Sub-Saharan Africa                                     -0.01001 0.00607 Inf   -0.0219   0.00188\n##                      45.8 Western Europe and North America                       -0.00335 0.00769 Inf   -0.0184   0.01172\n##                      45.8 Asia and Pacific                                       -0.01371 0.00667 Inf   -0.0268  -0.00063\n## \n## Confidence level used: 0.95\n```\n:::\n\n\nWe can replicate the results from `emtrends()` with `avg_slopes()` if we plug in average or representative values (more on that in the next section), since that follows the same averaging order as **emmeans** (i.e. plugging averages into the model)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  avg_slopes(variables = \"public_sector_corruption\",\n             newdata = datagrid(region = levels(corruption$region)),\n             by = \"region\")\n## \n##                      Term    Contrast                           region Estimate Std. Error      z Pr(>|z|)   2.5 %    97.5 %\n##  public_sector_corruption mean(dY/dX) Eastern Europe and Central Asia  -0.01117    0.00554 -2.016   0.0437 -0.0220 -0.000313\n##  public_sector_corruption mean(dY/dX) Latin America and the Caribbean  -0.00733    0.00611 -1.200   0.2301 -0.0193  0.004641\n##  public_sector_corruption mean(dY/dX) Middle East and North Africa     -0.01177    0.01106 -1.064   0.2873 -0.0334  0.009907\n##  public_sector_corruption mean(dY/dX) Sub-Saharan Africa               -0.01004    0.00607 -1.654   0.0982 -0.0219  0.001859\n##  public_sector_corruption mean(dY/dX) Western Europe and North America -0.00336    0.00771 -0.436   0.6627 -0.0185  0.011756\n##  public_sector_corruption mean(dY/dX) Asia and Pacific                 -0.01375    0.00667 -2.059   0.0395 -0.0268 -0.000664\n## \n## Columns: rowid, term, contrast, region, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n```\n:::\n\n\n\n### Marginal effects at user-specified or representative values\n\nIf we want to unlock the full potential of our regression mixing board, we can feed the model any values we want. In general, we'll (1) make a little dataset with covariate values set to either specific values that we care about, or typical or average values, (2) plug that little dataset into the the model and get fitted values, and (3) work with the results. There are a bunch of different names for this little fake dataset like \"data grid\" and \"reference grid\", but they're all the same idea. Here's an overview of the approach:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/flow-mer@3x.png){fig-align='center' width=100%}\n:::\n:::\n\n\n#### Creating representative values\n\nBefore plugging anything in, it's helpful to look at different ways of creating data grids with R. For all these examples, we'll make a dataset with public sector corruption set to 20 and 80 across Western Europe, Latin America, and the Middle East, with all other variables in the model set to their means. We'll make a little list of these regions to save typing time:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nregions_to_use <- c(\"Western Europe and North America\", \n                    \"Latin America and the Caribbean\",\n                    \"Middle East and North Africa\")\n```\n:::\n\n\n\nFirst, we can do it all manually with the `expand_grid()` function from **tidyr** (or `expand.grid()` from base R). This creates a data frame from all combinations of the vectors and single values we feed it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexpand_grid(public_sector_corruption = c(20, 80),\n            region = regions_to_use,\n            polyarchy = mean(corruption$polyarchy),\n            log_gdp_percapita = mean(corruption$log_gdp_percapita))\n## # A tibble: 6 × 4\n##   public_sector_corruption region                           polyarchy log_gdp_percapita\n##                      <dbl> <chr>                                <dbl>             <dbl>\n## 1                       20 Western Europe and North America      52.8              8.57\n## 2                       20 Latin America and the Caribbean       52.8              8.57\n## 3                       20 Middle East and North Africa          52.8              8.57\n## 4                       80 Western Europe and North America      52.8              8.57\n## 5                       80 Latin America and the Caribbean       52.8              8.57\n## 6                       80 Middle East and North Africa          52.8              8.57\n```\n:::\n\n\nA disadvantage of using `expand_grid()` like this is that the averages we calculated aren't necessarily the same averages of the data that gets used in the model. If any rows are dropped in the model because of missing values, that won't be reflected here. We could get around that by doing `model.frame(model_logit_fancy)$polyarchy`, but that's starting to get unwieldy. Instead, we can use a function that takes information about the model into account.\n\nSecond, we can use [`data_grid()`](https://modelr.tidyverse.org/reference/data_grid.html) from [**modelr**](https://modelr.tidyverse.org/), which is part of the really neat [**tidymodels** ecosystem](https://www.tidymodels.org/). An advantage of doing this is that it will handle the typical value part automatically—it will calculate the mean for continuous predictors and the mode for categorical predictors.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodelr::data_grid(corruption,\n                  public_sector_corruption = c(20, 80),\n                  region = regions_to_use,\n                  .model = model_logit_fancy)\n## # A tibble: 6 × 4\n##   public_sector_corruption region                           polyarchy log_gdp_percapita\n##                      <dbl> <chr>                                <dbl>             <dbl>\n## 1                       20 Latin America and the Caribbean       54.2              8.52\n## 2                       20 Middle East and North Africa          54.2              8.52\n## 3                       20 Western Europe and North America      54.2              8.52\n## 4                       80 Latin America and the Caribbean       54.2              8.52\n## 5                       80 Middle East and North Africa          54.2              8.52\n## 6                       80 Western Europe and North America      54.2              8.52\n```\n:::\n\n\nThird, we can use **marginaleffects**'s `datagrid()`, which will also calculate typical values for any covariates we don't specify:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndatagrid(model = model_logit_fancy,\n         public_sector_corruption = c(20, 80),\n         region = regions_to_use)\n##   disclose_donations polyarchy log_gdp_percapita public_sector_corruption                           region\n## 1              FALSE      52.8              8.57                       20 Western Europe and North America\n## 2              FALSE      52.8              8.57                       20  Latin America and the Caribbean\n## 3              FALSE      52.8              8.57                       20     Middle East and North Africa\n## 4              FALSE      52.8              8.57                       80 Western Europe and North America\n## 5              FALSE      52.8              8.57                       80  Latin America and the Caribbean\n## 6              FALSE      52.8              8.57                       80     Middle East and North Africa\n```\n:::\n\n\nAnd finally, we can use **emmeans**'s `ref_grid()`, which will *also* automatically create typical values. This doesn't return a data frame—it's some sort of special `ref_grid` object, but all the important information is still there:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nref_grid(model_logit_fancy,\n         at = list(public_sector_corruption = c(20, 80),\n                   region = regions_to_use))\n## 'emmGrid' object with variables:\n##     public_sector_corruption = 20, 80\n##     polyarchy = 52.79\n##     log_gdp_percapita = 8.5674\n##     region = Western Europe and North America, Latin America and the Caribbean, Middle East and North Africa\n## Transformation: \"logit\"\n```\n:::\n\n\n#### Working with representative values\n\nNow that we have a hypothetical data grid of sliders and switches set to specific values, we can plug it into the model and generate fitted values. Importantly, doing this provides us with results that are analogous to the marginal effects at the mean (MEM) that we found earlier, and *not* the average marginal effect (AME), since we're not feeding the entire original dataset to the model. None of these hypothetical rows exist in real life—there is no country with any of these exact combinations of corruption, polyarchy/democracy, GDP per capita, or region. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  slopes(variables = \"public_sector_corruption\",\n         newdata = datagrid(public_sector_corruption = c(20, 80),\n                            region = regions_to_use))\n## \n##                      Term  Estimate Std. Error       z Pr(>|z|)     2.5 %   97.5 % polyarchy log_gdp_percapita public_sector_corruption                           region\n##  public_sector_corruption -2.49e-02   1.56e-02 -1.5982    0.110 -0.055495 0.005643      52.8              8.57                       20 Western Europe and North America\n##  public_sector_corruption  8.27e-04   8.59e-03  0.0962    0.923 -0.016018 0.017672      52.8              8.57                       20 Latin America and the Caribbean \n##  public_sector_corruption -2.04e-02   1.43e-02 -1.4240    0.154 -0.048490 0.007680      52.8              8.57                       20 Middle East and North Africa    \n##  public_sector_corruption -1.49e-05   8.59e-05 -0.1734    0.862 -0.000183 0.000153      52.8              8.57                       80 Western Europe and North America\n##  public_sector_corruption -4.36e-03   3.53e-03 -1.2356    0.217 -0.011289 0.002559      52.8              8.57                       80 Latin America and the Caribbean \n##  public_sector_corruption -1.06e-04   3.83e-04 -0.2762    0.782 -0.000857 0.000646      52.8              8.57                       80 Middle East and North Africa    \n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, disclose_donations, polyarchy, log_gdp_percapita, public_sector_corruption, region\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  emtrends(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n           at = list(public_sector_corruption = c(20, 80),\n                     region = regions_to_use),\n           regrid = \"response\", delta.var = 0.001) \n##  public_sector_corruption region                           public_sector_corruption.trend      SE  df asymp.LCL asymp.UCL\n##                        20 Western Europe and North America                       -0.02493 0.01560 Inf   -0.0555   0.00565\n##                        80 Western Europe and North America                       -0.00001 0.00009 Inf   -0.0002   0.00015\n##                        20 Latin America and the Caribbean                         0.00083 0.00860 Inf   -0.0160   0.01768\n##                        80 Latin America and the Caribbean                        -0.00437 0.00353 Inf   -0.0113   0.00256\n##                        20 Middle East and North Africa                           -0.02040 0.01433 Inf   -0.0485   0.00768\n##                        80 Middle East and North Africa                           -0.00011 0.00038 Inf   -0.0009   0.00065\n## \n## Confidence level used: 0.95\n```\n:::\n\n\nWe have a ton of marginal effects here, but this is all starting to get really complicated. These are slopes, but slopes for which lines? What do these marginal effects actually look like?\n\nPlotting these regression lines is tricky because we're no longer working with a single variable on the x-axis. Instead, we need to generate predicted values of the regression outcome across a range of one $x$ while holding all the other variables constant. This is exactly what we've been doing to get marginal effects, only now instead of getting slopes as the output, we want fitted values. Both **marginaleffects** and **emmeans** make this easy.\n\nIn the **marginaleffects** world, we can use `predictions()`. The `estimate` column now shows the fitted value instead of the slope:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  predictions(newdata = datagrid(public_sector_corruption = c(20, 80),\n                                 region = regions_to_use))\n## \n##  Estimate Pr(>|z|)    2.5 % 97.5 % polyarchy log_gdp_percapita public_sector_corruption                           region\n##  3.81e-01   0.7001 4.93e-02  0.879      52.8              8.57                       20 Western Europe and North America\n##  3.97e-01   0.5855 1.29e-01  0.746      52.8              8.57                       20 Latin America and the Caribbean \n##  6.58e-01   0.5566 1.78e-01  0.945      52.8              8.57                       20 Middle East and North Africa    \n##  7.69e-05   0.1363 2.98e-10  0.952      52.8              8.57                       80 Western Europe and North America\n##  5.45e-02   0.0579 3.01e-03  0.524      52.8              8.57                       80 Latin America and the Caribbean \n##  5.93e-04   0.0708 1.87e-07  0.653      52.8              8.57                       80 Middle East and North Africa    \n## \n## Columns: rowid, estimate, p.value, conf.low, conf.high, disclose_donations, polyarchy, log_gdp_percapita, public_sector_corruption, region\n```\n:::\n\n\nIn the **emmeans** world, we can use `emmeans()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  emmeans(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n          at = list(public_sector_corruption = c(20, 80),\n                    region = regions_to_use),\n          regrid = \"response\") \n##  public_sector_corruption region                            prob     SE  df asymp.LCL asymp.UCL\n##                        20 Western Europe and North America 0.381 0.2975 Inf   -0.2021     0.964\n##                        80 Western Europe and North America 0.000 0.0005 Inf   -0.0009     0.001\n##                        20 Latin America and the Caribbean  0.397 0.1827 Inf    0.0395     0.755\n##                        80 Latin America and the Caribbean  0.055 0.0776 Inf   -0.0975     0.207\n##                        20 Middle East and North Africa     0.658 0.2505 Inf    0.1671     1.149\n##                        80 Middle East and North Africa     0.001 0.0024 Inf   -0.0042     0.005\n## \n## Confidence level used: 0.95\n```\n:::\n\n\nThe results from the two packages are identical because we're using a data grid—in both cases we're averaging *before* plugging stuff into the model.\n\nInstead of setting corruption to 20 and 80, we'll use a whole range of values so we can plot it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit_predictions <- model_logit_fancy |> \n  emmeans(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n          at = list(public_sector_corruption = seq(0, 90, 1)),\n          regrid = \"response\") |> \n  as_tibble()\n\nggplot(logit_predictions, aes(x = public_sector_corruption, y = prob, color = region)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Public sector corruption\", y = \"Predicted probability of having\\na campaign finance disclosure law\", color = NULL) +\n  scale_y_continuous(labels = percent_format()) +\n  scale_color_manual(values = c(clrs, \"grey30\")) +\n  theme_mfx() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-logit-predictions-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n(Alternatively, you can use **marginaleffects**'s built-in `plot_predictions()` to make this plot with one line of code):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_predictions(model_logit_fancy, condition = c(\"public_sector_corruption\", \"region\"))\n```\n:::\n\n\nThat's such a cool plot! Each region has a different shape of predicted probabilities across public sector corruption. \n\nEarlier we calculated a bunch of instantaneous slopes when corruption was set to 20 and 80 in a few different regions, so let's put those slopes and their tangent lines on the plot:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit_slopes <- model_logit_fancy |> \n  emtrends(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n           at = list(public_sector_corruption = c(20, 80),\n                     region = regions_to_use),\n           regrid = \"response\", delta.var = 0.001) |> \n  as_tibble() |> \n  mutate(panel = glue(\"Corruption set to {public_sector_corruption}\"))\n\nslopes_to_plot <- logit_predictions |> \n  filter(public_sector_corruption %in% c(20, 80),\n         region %in% regions_to_use) |> \n  left_join(select(logit_slopes, public_sector_corruption, region, public_sector_corruption.trend, panel),\n            by = c(\"public_sector_corruption\", \"region\")) |> \n  mutate(intercept = find_intercept(public_sector_corruption, prob, public_sector_corruption.trend)) |> \n  mutate(round_slope = label_number(accuracy = 0.001, style_negative = \"minus\")(public_sector_corruption.trend * 100),\n         nice_slope = glue(\"Slope: {round_slope} pct pts\"))\n\nggplot(logit_predictions, aes(x = public_sector_corruption, y = prob, color = region)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = slopes_to_plot, size = 2, show.legend = FALSE) +\n  geom_abline(data = slopes_to_plot, \n              aes(slope = public_sector_corruption.trend, intercept = intercept, color = region), \n              linewidth = 0.5, linetype = \"21\", show.legend = FALSE) +\n  geom_label_repel(data = slopes_to_plot, aes(label = nice_slope),\n                   fontface = \"bold\", seed = 123, show.legend = FALSE,\n                   size = 3, direction = \"y\") +\n  labs(x = \"Public sector corruption\", \n       y = \"Predicted probability of having\\na campaign finance disclosure law\", \n       color = NULL) +\n  scale_y_continuous(labels = percent_format()) +\n  scale_color_manual(values = c(\"grey30\", clrs)) +\n  facet_wrap(vars(panel)) +\n  theme_mfx() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-logit-predictions-slopes-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nAHH this is delightful! This helps us understand and visualize all these marginal effects. Let's interpret them:\n\n- In both the Middle East and Western Europe/North America, an increase in public sector corruption in countries with low levels of corruption (20) is associated with a −2.040 and −2.493 percentage point decrease in the probability of seeing a disclosure law, while in low-corruption countries in Latin America, an increase in public sector corruption doesn't do much to the probability (it *increases* it slightly by 0.083 percentage points)\n- In countries with high levels of corruption (80), on the other hand, a small increase in corruption doesn't do much to the probability of having a disclosure law in the Middle East (−0.011 percentage point decrease) or Western Europe (−0.001 percentage point decrease). In Latin America, though, a small increase in corruption is associated with a −0.437 percentage point decrease in the probability of having a disclosure law.\n\n**MAJOR CAVEAT**: None of these marginal effects are statistically significant, so there's a good chance that they're possibly zero, or positive, or more negative, or whatever. We can plot just these marginal slopes to show this:\n\n\n::: {.cell layout-align=\"center\" height='3.5'}\n\n```{.r .cell-code}\nggplot(logit_slopes, aes(x = public_sector_corruption.trend * 100, y = region, color = region)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[5]) +\n  geom_pointrange(aes(xmin = asymp.LCL * 100, xmax = asymp.UCL * 100)) +\n  scale_color_manual(values = c(clrs[4], clrs[1], clrs[2]), guide = \"none\") +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL) +\n  facet_wrap(vars(panel), ncol = 1) +\n  theme_mfx()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-logit-mfx-coef-plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n### Average marginal effects at counterfactual user-specified values\n\n::: {.callout-note}\n### Counterfactual grids in emmeans\n\nWhen I first wrote this in 2022, **emmeans** did not have support for counterfactual grids, but **marginaleffects** did, so here I only show this example with **marginaleffects**. Later in 2022, however, [**emmeans** added a `counterfactuals` argument to `ref_grid()`](https://cran.r-project.org/web/packages/emmeans/vignettes/messy-data.html#counterfact), so you can actually calculate these same average marginal effects at counterfactual user-specified values with both **marginaleffects** and **emmeans** now.\n:::\n\nCalculating marginal effects at representative values is useful and widespread—plugging different values into the model while holding others constant is the best way to see how all the different moving parts of a model work, especially when there interactions, exponents, or non-linear outcomes. We're using the full mixing panel here!\n\nHowever, creating a hypothetical data or reference grid creates hypothetical observations that might never exist in real life. This was the main difference behind the average marginal effect (AME) and the marginal effect at the mean (MEM) that we looked at earlier. Passing average covariate values into a model creates average predictions, but those averages might not reflect reality.\n\nFor example, we used this data grid to look at the effect of corruption on the probability of having a campaign finance disclosure law across different regions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndatagrid(model = model_logit_fancy,\n         public_sector_corruption = c(20, 80),\n         region = regions_to_use)\n##   disclose_donations polyarchy log_gdp_percapita public_sector_corruption                           region\n## 1              FALSE      52.8              8.57                       20 Western Europe and North America\n## 2              FALSE      52.8              8.57                       20  Latin America and the Caribbean\n## 3              FALSE      52.8              8.57                       20     Middle East and North Africa\n## 4              FALSE      52.8              8.57                       80 Western Europe and North America\n## 5              FALSE      52.8              8.57                       80  Latin America and the Caribbean\n## 6              FALSE      52.8              8.57                       80     Middle East and North Africa\n```\n:::\n\n\nPolyarchy (democracy) and GDP per capita here are set at their dataset-level means, but that's not how the world actually works. Levels of democracy and personal wealth vary *a lot* by region:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorruption |> \n  filter(region %in% regions_to_use) |> \n  group_by(region) |> \n  summarize(avg_polyarchy = mean(polyarchy),\n            avg_log_gdp_percapita = mean(log_gdp_percapita))\n## # A tibble: 3 × 3\n##   region                           avg_polyarchy avg_log_gdp_percapita\n##   <fct>                                    <dbl>                 <dbl>\n## 1 Latin America and the Caribbean           62.9                  8.77\n## 2 Middle East and North Africa              27.4                  9.01\n## 3 Western Europe and North America          86.5                 10.7\n```\n:::\n\n\n\n\nWestern Europe is far more democratic (average polyarchy = 86.50) than the Middle East (average polyarchy = 27.44). But in our calculations for finding region-specific marginal effects, we've been using a polyarchy value of 52.79 for all the regions.\n\nFortunately we can do something neat to work with observed covariate values and thus create an AME-flavored marginal effect at representative values instead of the current MEM-flavored marginal effect at representative values. Here's the general process:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/flow-counterfactual@3x.png){fig-align='center' width=100%}\n:::\n:::\n\n\nInstead of creating a data or reference grid, we create multiple copies of our original dataset. In each copy we change the columns that we want to set to specific values and we leave all the other columns at their original values. We then feed all the copies of the dataset into the model and generate a ton of fitted values, which we *then* collapse into average effects.\n\nThat sounds really complex, but it's only a matter of adding one argument to `marginaleffects::datagrid()`. We'll take `region` out of `datagrid` here so that we keep all the original regions—we'll take the average across those regions after the fact.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncfct_data <- datagrid(model = model_logit_fancy,\n                      public_sector_corruption = c(20, 80),\n                      grid_type = \"counterfactual\")\n```\n:::\n\n\nThis new data grid has twice the number of rows that we have in the original data, since there are now two copies of the data stacked together:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(corruption)\n## [1] 168\nnrow(cfct_data)\n## [1] 336\n```\n:::\n\n\nTo verify, lets look at the first 5 rows in each of the copies:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncfct_data[c(1:5, nrow(corruption) + 1:5), ]\n##     rowidcf disclose_donations polyarchy log_gdp_percapita                           region public_sector_corruption\n## 1         1               TRUE      64.7              9.10  Latin America and the Caribbean                       20\n## 2         2              FALSE      76.1              8.93  Latin America and the Caribbean                       20\n## 3         3              FALSE      90.8             10.85 Western Europe and North America                       20\n## 4         4              FALSE      89.4             11.36 Western Europe and North America                       20\n## 5         5              FALSE      72.0              7.61               Sub-Saharan Africa                       20\n## 169       1               TRUE      64.7              9.10  Latin America and the Caribbean                       80\n## 170       2              FALSE      76.1              8.93  Latin America and the Caribbean                       80\n## 171       3              FALSE      90.8             10.85 Western Europe and North America                       80\n## 172       4              FALSE      89.4             11.36 Western Europe and North America                       80\n## 173       5              FALSE      72.0              7.61               Sub-Saharan Africa                       80\n```\n:::\n\n\nThat's neat! These 5 countries all have their original values of polyarchy, GDP per capita, and region, but have their public sector corruption indexes set to 20 (in the first copy) and 80 (in the second copy).\n\nWe can feed this stacked data to `slopes()` to get an instantaneous slope (`estimate`) for each row:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Specify variables = \"public_sector_corruption\" so that it doesn't calculate\n# slopes and contrasts for all the other covariates\nmfx_cfct <- model_logit_fancy |> \n  slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                            grid_type = \"counterfactual\"),\n         variables = \"public_sector_corruption\")\n\nhead(mfx_cfct)\n## \n##  rowidcf                     Term  Estimate Std. Error       z Pr(>|z|)   2.5 %  97.5 % polyarchy log_gdp_percapita                           region public_sector_corruption\n##        1 public_sector_corruption  0.000860    0.00898  0.0958    0.924 -0.0167 0.01846      64.7              9.10 Latin America and the Caribbean                        20\n##        2 public_sector_corruption  0.000861    0.00900  0.0956    0.924 -0.0168 0.01851      76.1              8.93 Latin America and the Caribbean                        20\n##        3 public_sector_corruption -0.024669    0.02425 -1.0174    0.309 -0.0722 0.02285      90.8             10.85 Western Europe and North America                       20\n##        4 public_sector_corruption -0.024566    0.02460 -0.9986    0.318 -0.0728 0.02365      89.4             11.36 Western Europe and North America                       20\n##        5 public_sector_corruption -0.014743    0.01034 -1.4264    0.154 -0.0350 0.00551      72.0              7.61 Sub-Saharan Africa                                     20\n##        6 public_sector_corruption -0.014592    0.01034 -1.4107    0.158 -0.0349 0.00568      70.3              8.64 Sub-Saharan Africa                                     20\n## \n## Columns: rowid, rowidcf, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, disclose_donations, polyarchy, log_gdp_percapita, region, public_sector_corruption\n```\n:::\n\n\nFinally we can calculate group averages for each of the levels of `public_sector_corruption` to get AME-flavored effects:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmfx_cfct |> \n  group_by(public_sector_corruption) |> \n  summarize(avg_slope = mean(estimate))\n## # A tibble: 2 × 2\n##   public_sector_corruption avg_slope\n##                      <dbl>     <dbl>\n## 1                       20  -0.0128 \n## 2                       80  -0.00307\n```\n:::\n\n\nOr we can let **marginaleffects** deal with the averaging so that we can get standard errors and confidence intervals:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  avg_slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                                grid_type = \"counterfactual\"),\n             variables = \"public_sector_corruption\",\n             by = \"public_sector_corruption\")\n## \n##                      Term    Contrast public_sector_corruption Estimate Std. Error     z Pr(>|z|)    2.5 %    97.5 %\n##  public_sector_corruption mean(dY/dX)                       20 -0.01277    0.00659 -1.94   0.0527 -0.02568  0.000149\n##  public_sector_corruption mean(dY/dX)                       80 -0.00307    0.00123 -2.50   0.0125 -0.00547 -0.000661\n## \n## Columns: term, contrast, public_sector_corruption, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n```\n:::\n\n\nWe can also calculate group averages across each region to get region-specific AME-flavored effects:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmfx_cfct |> \n  filter(region %in% regions_to_use) |> \n  group_by(public_sector_corruption, region) |> \n  summarize(avg_slope = mean(estimate))\n## `summarise()` has grouped output by 'public_sector_corruption'. You can override using the `.groups` argument.\n## # A tibble: 6 × 3\n## # Groups:   public_sector_corruption [2]\n##   public_sector_corruption region                            avg_slope\n##                      <dbl> <fct>                                 <dbl>\n## 1                       20 Latin America and the Caribbean   0.000816 \n## 2                       20 Middle East and North Africa     -0.0218   \n## 3                       20 Western Europe and North America -0.0252   \n## 4                       80 Latin America and the Caribbean  -0.00570  \n## 5                       80 Middle East and North Africa     -0.0000711\n## 6                       80 Western Europe and North America -0.0000370\n```\n:::\n\n\nOr again, we can get standard errors and confidence intervals:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_logit_fancy |> \n  avg_slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                                grid_type = \"counterfactual\"),\n             variables = \"public_sector_corruption\",\n             by = c(\"public_sector_corruption\", \"region\"))\n## \n##                      Term    Contrast public_sector_corruption                           region  Estimate Std. Error      z Pr(>|z|)     2.5 %    97.5 %\n##  public_sector_corruption mean(dY/dX)                       20 Eastern Europe and Central Asia  -1.84e-03   0.005413 -0.340   0.7337 -0.012452  0.008769\n##  public_sector_corruption mean(dY/dX)                       20 Latin America and the Caribbean   8.16e-04   0.008500  0.096   0.9235 -0.015844  0.017477\n##  public_sector_corruption mean(dY/dX)                       20 Middle East and North Africa     -2.18e-02   0.016457 -1.323   0.1860 -0.054021  0.010490\n##  public_sector_corruption mean(dY/dX)                       20 Sub-Saharan Africa               -1.43e-02   0.011347 -1.262   0.2071 -0.036554  0.007925\n##  public_sector_corruption mean(dY/dX)                       20 Western Europe and North America -2.52e-02   0.023444 -1.077   0.2815 -0.071197  0.020703\n##  public_sector_corruption mean(dY/dX)                       20 Asia and Pacific                 -1.62e-02   0.010530 -1.536   0.1246 -0.036811  0.004467\n##  public_sector_corruption mean(dY/dX)                       80 Eastern Europe and Central Asia  -1.28e-02   0.006090 -2.104   0.0353 -0.024751 -0.000879\n##  public_sector_corruption mean(dY/dX)                       80 Latin America and the Caribbean  -5.70e-03   0.004395 -1.296   0.1951 -0.014309  0.002919\n##  public_sector_corruption mean(dY/dX)                       80 Middle East and North Africa     -7.11e-05   0.000254 -0.279   0.7799 -0.000570  0.000427\n##  public_sector_corruption mean(dY/dX)                       80 Sub-Saharan Africa               -2.21e-04   0.000454 -0.487   0.6261 -0.001112  0.000669\n##  public_sector_corruption mean(dY/dX)                       80 Western Europe and North America -3.70e-05   0.000217 -0.171   0.8646 -0.000463  0.000389\n##  public_sector_corruption mean(dY/dX)                       80 Asia and Pacific                 -2.73e-04   0.000682 -0.401   0.6884 -0.001610  0.001063\n## \n## Columns: term, contrast, public_sector_corruption, region, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n```\n:::\n\n\nLet's compare these counterfactual marginal effects with the region-specific marginal effects at representative values that we calculated earlier:\n\n\n::: {.cell layout-align=\"center\" height='3.5'}\n\n```{.r .cell-code}\name_flavored <- model_logit_fancy |> \n  slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                            grid_type = \"counterfactual\"),\n         variables = \"public_sector_corruption\",\n         by = c(\"public_sector_corruption\", \"region\")) |> \n  filter(region %in% regions_to_use)\n\nmem_flavored <- model_logit_fancy |> \n  slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                            region = regions_to_use),\n         variables = \"public_sector_corruption\",\n         by = c(\"public_sector_corruption\", \"region\"))\n\nmfx_to_plot <- bind_rows(`Counterfactual stacked data` = ame_flavored, \n                         `Average values` = mem_flavored, \n                         .id = \"approach\") |> \n  mutate(panel = glue(\"Corruption set to {public_sector_corruption}\"))\n\nggplot(mfx_to_plot, aes(x = estimate * 100, y = region, color = region, \n                        linetype = approach, shape = approach)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[5]) +\n  geom_pointrange(aes(xmin = conf.low * 100, xmax = conf.high * 100),\n                  position = position_dodge(width = -0.6)) +\n  scale_color_manual(values = c(clrs[4], clrs[1], clrs[2]), guide = \"none\") +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL, linetype = NULL, shape = NULL) +\n  facet_wrap(vars(panel), ncol = 1) +\n  theme_mfx() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-cfx-coef-plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIn this case there are no huge differences 🤷. BUT STILL this is really neat!\n\n\n## Categorical contrasts as statistical/marginal effects\n\nConfusingly, people sometimes also use the term \"marginal effect\" to talk about group averages or predicted values ([I myself am guilty of this!](https://datavizs21.classes.andrewheiss.com/example/07-example/#marginal-effects-plots)). Technically speaking, a marginal effect is only a partial derivative, or a slope—not a predicted value or a difference in group means.\n\nBut regression lends itself well to group means, and predictions and fitted values are fundamental to calculating instantaneous slopes, so both **marginaleffects** and **emmeans** are used for adjusted predictions and marginal means and contrasts. They also use different approaches for calculating these averages, either averaging before putting values in the model (**emmeans**) or averaging after (**marginaleffects**'s default setting).\n\nWe've already seen two different functions for generating predictions when we plotted the predicted probabilities of having a disclosure law for each region: `marginaleffects::predictions()` and `emmeans::emmeans()`.\n\nI won't go into a ton of detail here about the differences between the two approaches to predictions and contrasts, mostly because pretty much everything we've looked at so far applies to both. Instead, you should look at Vincent's excellent vignettes for **marginaleffects**:\n\n- [Predictions](https://marginaleffects.com/articles/predictions.html)\n- [Comparisons](https://marginaleffects.com/articles/comparisons.html)\n\nAnd the equally excellent vignettes for **emmeans**:\n\n- [Prediction in emmeans](https://cran.r-project.org/web/packages/emmeans/vignettes/predictions.html)\n- [Comparisons and contrasts in emmeans](https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html)\n\nYou should also check out [this Twitter thread tutorial](https://twitter.com/alexpghayes/status/1282869973006909441) by [Alex Hayes](https://www.alexpghayes.com/) on categorical contrasts and means—it's a fantastic illustration of this same process.\n\nIn general, the two packages follow the same overall approach that we've seen with `slopes()` and `emtrends()`:\n\n- Prediction and comparison functions in **marginaleffects** try to calculate predictions and averages for each row, then collapses them to single average values (either globally or for specific groups). This approach is AME-flavored (though **marginaleffects** can also do MEM-flavored operations and average first).\n- Prediction and contrast functions in **emmeans** collapse values into averages first, then feeds those average values into the model to generate average predictions and means (either globally or for specific groups). This approach is MEM-flavored.\n\n\n## tl;dr: Overall summary of all these marginal effects approaches\n\n***PHEW*** we just did a lot of marginal work. This is important stuff. Unless you're working with a linear OLS model without any fancy extra things like interactions, polynomials, logs, and so on, **don't try to talk about marginal effects based on just the output of a regression table—it's not possible unless you do a lot of manual math!**\n\nBoth **marginaleffects** and **emmeans** provide all sorts of neat and powerful ways to calculate marginal effects without needing to resort to calculus, but as we've seen here, there are some subtle and extremely important differences in how they calculate their different effects.\n\nThe main takeaway from this whole post is this: **If you take the average *before* plugging values into the model, you compute average marginal effects for a combination of covariates that might not actually exist in reality. If you take the average *after* plugging values into the model, each original observation reflects combinations of covariates that definitely exist in reality, so the average marginal effect reflects that reality.**\n\nTo remember all these differences, here's a table summarizing all their different approaches:\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table style=\"font-size:80%; margin-left: auto; margin-right: auto;\" class=\"table\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Type </th>\n   <th style=\"text-align:left;\"> Process </th>\n   <th style=\"text-align:left;\"> marginaleffects </th>\n   <th style=\"text-align:left;\"> emmeans </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 15%; \"> Average marginal effects (AME) </td>\n   <td style=\"text-align:left;width: 25%; \"> Generate predictions for each row of the original data, then collapse to averages </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>avg_slopes(model)</code></pre> </td>\n   <td style=\"text-align:left;width: 30%; \"> Not supported </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15%; \"> Group average marginal effects (G-AME) </td>\n   <td style=\"text-align:left;width: 25%; \"> Generate predictions for each row of the original data, then collapse to grouped averages </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>avg_slopes(model, by = \"some_group\")</code></pre> </td>\n   <td style=\"text-align:left;width: 30%; \"> Not supported </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15%; \"> Marginal effects at the mean (MEM) </td>\n   <td style=\"text-align:left;width: 25%; \"> Collapse data to averages, then generate predictions using those averages </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>avg_slopes(model, newdata = \"mean\")</code></pre> </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>emtrends(model, ...)</code></pre> </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15%; \"> Marginal effects at user-specified or representative values (MER) </td>\n   <td style=\"text-align:left;width: 25%; \"> Create a grid of specific and average or typical values, then generate predictions </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>slopes(\n  model,\n  newdata = datagrid(some_x = c(10, 20))\n)</code></pre> </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>emtrends(\n  model, ...,\n  at = list(some_x = c(10, 20))\n)</code></pre> </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15%; \"> Average marginal effects at counterfactual user-specified values </td>\n   <td style=\"text-align:left;width: 25%; \"> Create multiple copies of the original data with some columns set to specific values, then generate predictions for each row of each copy of the original data, then collapse to averages </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>slopes(\n  model,\n  newdata = datagrid(\n    some_x = c(10, 20),\n    grid_type = \"counterfactual\"\n  )\n)</code></pre> </td>\n   <td style=\"text-align:left;width: 30%; \"> <pre class=\"r smaller\"><code>emtrends(\n  model, \"some_x\",\n  counterfactuals = \"some_x\"\n)</code></pre> </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAnd here's an image with all five of the diagrams at the same time:\n\n::: {.callout-tip}\n### Diagrams!\n\nYou can download PDF, SVG, and PNG versions of the marginal effects diagrams in this guide, as well as the original Adobe Illustrator file, here:\n\n- [PDFs, SVGs, and PNGs](http://files.andrewheiss.com/marginal-effects-diagrams/marginal-effects-output.zip)\n- [Illustrator .ai file](http://files.andrewheiss.com/marginal-effects-diagrams/marginal-effects.ai)\n\nDo whatever you want with them! They're licensed under [Creative Commons Attribution-ShareAlike (BY-SA 4.0)](http://creativecommons.org/licenses/by-sa/4.0/).\n:::\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/flow-everything@3x.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Which approach is best?\n\nWho even knows. \n\nBoth kinds of averaging approaches are pretty widespread. The [**tidymodels** ecosystem](https://www.tidymodels.org/) encourages the use of `modelr::data_grid()` and plugging various combinations of specific and typical variables into models to look at slopes and group contrasts. That's marginal effects at the mean (MEM) and marginal effects at representative values (MER), which both use average values before putting them in model. And it's fine—**tidymodels** is used in data science production pipelines around the world.\n\n**emmeans** is also incredibly popular in data science and academia. I use it in a few of my blog post guides ([like this one](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/) where I talk about average marginal effects the whole time even though technically *none* of the effects there are true AMEs—they're all MEMs!). **emmeans** just calculates MEMs and MERs.\n\nThe idea of average marginal effects (AMEs)—calculating averages *after* plugging values into models—is incredibly popular in the social sciences. **marginaleffects**, its predecessor [**margins**](https://github.com/leeper/margins), and [its Stata counterpart **margins**](https://www.stata.com/features/overview/marginal-analysis/) are all used in research in political science, public policy, economics, and other fields. \n\nI'm sure there are super smart people in the world who know when AMEs or MEMs are most appropriate ([like this article here!](https://doi.org/10.1111/j.1540-5907.2012.00602.x)), and people who have even better and robust ways to account for the typicalness and/or uncertainty of the original data ([see here for an averaging approach using a Bayesian bootstrap](https://arelbundock.com/bayesian_bootstrap.html), for instance), but I'm not one of those super smart people.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}