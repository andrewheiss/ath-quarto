{
  "hash": "fb568a2e30ef7b81d44691a426832aca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly\"\ndate: 2025-01-27\ndescription: \"Download CPS demographic data from IPUMS and use R and {brms} to calculate differences between sample and national proportions with Bayesian ROPE-based inference\"\n\nimage: index_files/figure-html/fig-prop-test-bayes-diffs-1.png\ntwitter-card: \n    image: \"index_files/figure-html/fig-prop-test-bayes-diffs-1.png\"\nopen-graph: \n    image: \"index_files/figure-html/fig-prop-test-bayes-diffs-1.png\"\n\ncategories:\n  - r\n  - tidyverse\n  - us census\n  - ggplot\n  - bayes\n  - brms\n  - surveys\n\nformat:\n  html:\n    shift-heading-level-by: 1\n    reference-location: margin\n    include-in-header:\n      - text: |\n          <style type=\"text/css\">\n          hr.dinkus {\n              width: 50px;\n              margin: 2em auto 2em;\n              border-top: 5px dotted #454545;\n          }\n\n          div.column-margin+hr.dinkus {\n              margin: 1em auto 2em;\n          }\n\n          .quarto-float-tbl figcaption {\n              text-align: left !important;\n          }\n          </style>\n\nresources:\n  - \"synthetic_data.rds\"\n  - \"synthetic_data.csv\"\n\nbibliography: references.json\ncsl: chicago-author-date.csl\n\ndoi: 10.59350/8ws3f-1fd56\ncitation: true\n---\n\n\n\n\n\nLast week I was making some final revisions [to a paper](https://stats.andrewheiss.com/silent-skywalk/) where we used a neat conjoint experiment to test the effect of a bunch of different treatments on nonprofit donor preference. \n\nOne of the peer reviewers asked us to compare the characteristics of our experimental sample with the general population so that we could speak a little to the experiment's generalizability. This is a super common thing to do with survey research, and one of the main reasons survey researchers include demographic questions in their surveys.\n\nThanks to the wonders of the R community‚Äîand thanks to publicly accessible data‚ÄîI was able to grab nationally representative demographic data, clean it up and summarize it, run some statistical tests, and make a table to meet the reviewer's request, [all in like 45 minutes](https://bsky.app/profile/andrew.heiss.phd/post/3lgc5loazd22y).\n\n![](img/00-bsky-post.png){width=\"80%\" fig-align=\"center\"}\n\nIt was a magically quick and easy process, so I figured I'd make a guide about it so that the rest of the world (but mostly future me) can see how to do it.\n\n\n# Nationally representative demographic data\n\nFinding nationally representative demographic data (in the US, at least) is pretty easy, and there are two common sources for it:\n\n- The US Census's [American Community Survey](https://www.census.gov/programs-surveys/acs) (ACS) is a rolling monthly survey of ‚âà3.5 million (!!!) US *households* that's compiled into an annual dataset.\n- The US Census's [Current Population Survey](https://www.census.gov/programs-surveys/cps.html) (CPS) is a monthly survey of ‚âà100,000 US *individuals*. A more comprehensive annual version‚Äîthe Annual Social and Economic Supplement (ASEC)‚Äîis published every March.\n\nThe two surveys serve different purposes, and [the Census has an FAQ fact sheet explaining the difference between the ACS and CPS](https://www.census.gov/topics/income-poverty/poverty/guidance/data-sources/acs-vs-cps.html). Notably, the ACS only surveys *households*, and it uses a shorter 8-question survey, while the CPS tries to reach the entire civilian noninstitutionalized population and uses a longer, more detailed survey.\n\nResearchers use both surveys‚ÄîI've used both in my own work. [According to the Census](https://www.census.gov/topics/income-poverty/poverty/guidance/data-sources/acs-vs-cps.html), due to its detailed questionnaire and staff experience and regular frequency, the CPS ASEC is a \"high quality source of information used to produce the official annual estimate of poverty, and estimates of a number of other socioeconomic and demographic characteristics\". The ACS also has demographic details and people use those instead too. \n\nI'm not entirely sure which one is best‚Äîto me they're both great ü§∑‚Äç‚ôÇÔ∏è. Smarter people than me know and care about the difference.\n\n# Accessing US Census data\n\nGetting data from the Census is a surprisingly complex process! There are websites and R packages that make it easier though.\n\n## ACS\n\nFor the ACS, [the {tidycensus} R package](https://walker-data.com/tidycensus/) provides an interface to the Census's API, and [its documentation is great and thorough](https://walker-data.com/tidycensus/). Working with the results is tricky though, and involves a lot of pivoting and reshaping and combining variables. [I have a whole notebook](https://stats.andrewheiss.com/zany-zebra/notebook/acs-data.html) showing how I access the ACS and create a bunch of variables, with [little notes](https://stats.andrewheiss.com/zany-zebra/notebook/acs-data.html#high-school-education) reminding myself how I constructed everything:\n\n![Explanation of how I calculated the proportion of households in a block group with a high school education](img/00-zany-zebra-example.png){.lightbox .border width=\"80%\" fig-align=\"center\"}\n\n## CPS (and others!)\n\n{tidycensus} doesn't provide Census API access to CPS data. Instead, [IPUMS](https://www.ipums.org/)‚Äîa project housed at the University of Minnesota and supported by a consortium of other institutions and companies‚Äîprovides easy access to all sorts of census and survey data, both through its website and through an API. It's wild how much data they have. In addition to the [CPS](https://cps.ipums.org/cps/), they have the [ACS](https://usa.ipums.org/), [census microtata for 100+ countries](https://international.ipums.org/international/), [historical GIS shapefiles](https://www.nhgis.org/), [time use surveys](https://timeuse.ipums.org/), and a ton of other things. It's an incredible project.\n\n# Getting started\n\n::: {.callout-note}\n\n### Who this guide is for\n\nHere's what I assume you know:\n\n- You're familiar with [R](https://www.r-project.org/) and the [tidyverse](https://www.tidyverse.org/) (particularly [{dplyr}](https://dplyr.tidyverse.org/) and [{ggplot2}](https://ggplot2.tidyverse.org/)).\n- You're familiar with [{brms}](https://paul-buerkner.github.io/brms/) for running Bayesian regression models and [{tidybayes}](https://mjskay.github.io/tidybayes/) and [{ggdist}](https://mjskay.github.io/ggdist/) for manipulating and plotting posterior draws.\n\n:::\n\nIn this guide, we'll use IPUMS to get [CPS](https://cps.ipums.org/cps/) data (monthly and ASEC) with R. Because their data explorer website takes a little while to get used to, I'll show a bunch of step-by-step screenshots of how to navigate it. It's possible to [access the IPUMS API](https://tech.popdata.org/ipumsr/articles/ipums-api.html) with the {ipumsr} package, and I also [show how to do that in this guide](#more-reproducible-alternative-using-the-ipums-api). But in order to use the API, you still need to know how to use the website‚Äîyou have to find variable names and figure out which samples include which variables. So the screenshots below are still important even if you're using the API.\n\nI'll then show how to answer the question of whether a survey proportion is equivalent to a population proportion in a couple diffferent ways: \n\n1. Frequentist/classical proportion tests with null hypothesis significance testing, and\n2. Bayesian proportion tests and inference based on regions of practical equivalence, or ROPEs\n\nBefore getting started, let's load all the packages we need and create some helpful functions and variables.\n\nThe experimental survey data here comes from @ChaudhryDotsonHeiss:2024. Since we haven't published it yet (though it's close‚Äîit's under review post R&R now!), the original data isn't quite public yet. So I used [the {synthpop} R package](https://synthpop.org.uk/get-started.html) to create a synthetic version of part of our data that has the same relationships and distributions as the real results, but is all fake. You can see [the R code I used for that process here](https://github.com/andrewheiss/ath-quarto/blob/main/blog/2025/01/27/ipums-cps-proportions-bayes/make_synthetic_data.R).\n\nIf you want to follow along, you can download this synthetic data here:\n\n- [{{< fa brands r-project >}}&ensp;`synthetic_data.rds`](synthetic_data.rds): RDS version with all the variable attributes (i.e. factor levels and ordering) included\n- [{{< fa file-csv >}}&ensp;`synthetic_data.csv`](synthetic_data.csv): Plain-text version (but without variable attributes)\n\nTo reflect the fact that this is all public, government-created data, I'm using the [Public Sans font](https://public-sans.digital.gov/), an open source font developed as part of the General Services Administration's [USWDS](https://designsystem.digital.gov/) (US Web Design System) for making accessible federal government websites. I'm also using the [USWDS's basic color palette](https://designsystem.digital.gov/utilities/color/#basic-palette-2), developed by [18F](https://18f.gsa.gov/2020/04/21/a-token-of-our-affection-uswds-2/).\n\nLet's get started!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)   # {ggplot2}, {dplyr}, and friends\nlibrary(tinytable)   # Nice tables\nlibrary(brms)        # Best way to run Stan models\nlibrary(tidybayes)   # Manipulate Stan objects and draws\nlibrary(broom)       # Convert model objects to data frames\nlibrary(glue)        # Easier string construction\nlibrary(scales)      # Nicer labels\nlibrary(ggdist)      # Plot posterior distributions\nlibrary(ggforce)     # Extra ggplot things like facet_col()\nlibrary(patchwork)   # Combine ggplot plots\n\n# Load the synthetic survey results\nresults <- readRDS(\"synthetic_data.rds\")\n\n# Use the cmdstanr backend for brms because it's faster and more modern than the\n# default rstan backend. You need to install the cmdstanr package first\n# (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n# install cmdstan on your computer.\noptions(\n  mc.cores = 4,\n  brms.backend = \"cmdstanr\"\n)\n\n# Set some global Stan options\nCHAINS <- 4\nITER <- 2000\nWARMUP <- 1000\nBAYES_SEED <- 1234\n\n# Nice ggplot theme\ntheme_public <- function() {\n  theme_minimal(base_family = \"Public Sans\") +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(family = \"Public Sans\", face = \"bold\", size = rel(1.25)),\n      plot.subtitle = element_text(family = \"Public Sans Light\", face = \"plain\"),\n      plot.caption = element_text(family = \"Public Sans Light\", face = \"plain\"),\n      axis.title = element_text(family = \"Public Sans Semibold\", size = rel(0.8)),\n      axis.title.x = element_text(hjust = 0),\n      axis.title.y = element_text(hjust = 1),\n      strip.text = element_text(\n        family = \"Public Sans Semibold\", face = \"plain\",\n        size = rel(0.8), hjust = 0\n      ),\n      strip.background = element_rect(fill = \"grey90\", color = NA),\n      legend.title = element_text(family = \"Public Sans Semibold\", size = rel(0.8)),\n      legend.text = element_text(size = rel(0.8)),\n      legend.position = \"bottom\",\n      legend.justification = \"left\",\n      legend.title.position = \"top\",\n      legend.margin = margin(l = 0, t = 0)\n    )\n}\n\ntheme_set(theme_public())\nupdate_geom_defaults(\"text\", list(family = \"Public Sans\"))\nupdate_geom_defaults(\"label\", list(family = \"Public Sans\"))\n\n# USWDS basic palette\n# https://designsystem.digital.gov/utilities/color/#basic-palette-2\nclrs <- c(\n  \"#e52207\", # .bg-red\n  \"#e66f0e\", # .bg-orange\n  \"#ffbe2e\", # .bg-gold\n  \"#fee685\", # .bg-yellow\n  \"#538200\", # .bg-green\n  \"#04c585\", # .bg-mint\n  \"#009ec1\", # .bg-cyan\n  \"#0076d6\", # .bg-blue\n  \"#676cc8\", # .bg-indigo\n  \"#8168b3\", # .bg-violet\n  \"#d72d79\" # .bg-magenta\n)\n\n# Some functions for creating percentage point labels\nlabel_pp <- label_number(\n  accuracy = 1, scale = 100, suffix = \" pp.\", style_negative = \"minus\"\n)\n\nlabel_pp_01 <- label_number(\n  accuracy = 0.1, scale = 100, suffix = \" pp.\", style_negative = \"minus\"\n)\n```\n:::\n\n\n\n# Getting CPS data from the IPUMS website\n\nGo to the [IPUMS CPS website](https://cps.ipums.org/cps/) and create an account if you don't already have one.\n\nOnce you're logged in, go the [\"Select Data\" page](https://cps.ipums.org/cps-action/variables/group), where IPUMS tells you to do two things: \n\n1. **Select samples**, or specific verisons of different surveys\n2. **Select variables**, or specific columns in different surveys\n\n![Initial data extract page](img/01-data-extract.png){.lightbox}\n\nVisually it looks like you should select samples first, but **I actually find it easier to poke around for different variables first**, since not all variables are recorded in every sample.\n\n## Finding variables\n\nSo first let's look at a few variables to get a feel for the IPUMS data extract website. Click on the little \"SEARCH üîç\" button in the \"SELECT VARIABLES\" section. We could do a bunch of fancy advanced search options, but for now, just search for \"age\"\n\n![Searching for age-related variables](img/02-age-search.png){.lightbox}\n\nThere are 200+(!) age-related variables in the CPS data:\n\n![Search results related to age](img/02-age-results.png){.lightbox}\n\nThis big list shows some useful information already. The first variable in the search results is `AGE`, and it's the one we care about. It gets recorded in every CPS survey: each monthly one and in the annual ASEC one. Not all the age variables do this‚Äînotice that `WHYSS1` only appears in the annual ASEC.\n\nIf you click on `AGE` in the \"Variable\" column, you can see detailed information about the variable, like how it's coded, a description, and its availability. For example, prior to 1976, age was only available in the annual ASEC; starting in January 1976, it became a monthly thing.\n\n![AGE availability](img/02-age-availability.png){.lightbox}\n\nSince we know we want this variable, we can add it to our \"Data Cart\". IPUMS ues a shopping metaphor for building a data extract‚Äîwe can add different variables and samples to a cart and then check it out (for free) once we've found everything we're looking for. \n\nClick on \"Add to cart\" to add it, then go back to the search page to look for more variables. You can also add variables to the cart without going to the variable details page‚Äîthere's a plus sign in the search results page next to each result that will add the variable for you.\n\n![Add AGE to cart](img/02-age-add-cart.png){.lightbox}\n\nNow that we have age, we need to hunt around for other variables we care about, like sex, marital status, voting history, and so on. To speed things up, you can search for their official variable names and add each one to the cart:\n\n- Sex = `SEX`\n- Marital status = `MARST`\n- Education = `EDUC`\n- Donating = `VLDONATE`\n- Volunteering = `VLSTATUS`\n- Volunteering Supplement weight = `VLSUPPWT`\n- Voting = `VOTED`\n- Voter Supplement weight = `VOSUPPWT`\n\n### Pay attention to the details!\n\nLooking at the details for these variables is helpful since they're all categorical variables, unlike age. For instance, marital status has 9 different levels:\n\n![Nine different marital status (MARST) codes](img/03-marst-codes.png){.lightbox}\n\nAlso, it's important to check the variable details to check for availability. While basic demographic variables like age, sex, marital status, etc. are available in both the monthly surveys and in the annual ASEC, more specialized variables are not.\n\nVariables related to philanthropy and volunteering are only available in September (since they're part of a [special CPS Volunteer Supplement](https://cps.ipums.org/cps/volunteer_sample_notes.shtml)), and only in some years:\n\n![Volunteer status (VLSTATUS) only available in September, and only in some years](img/03-vlstatus-availability.png){.lightbox}\n\nVariables related to voting are only available in November in even-numbered years (since they're part the [CPS's Voting and Registration Supplement](https://cps.ipums.org/cps/voter_sample_notes.shtml))\n\n![Voting status (VOTED) only available in even-numbered years in November](img/03-voted-availability.png){.lightbox}\n\n## Selecting samples\n\nGreat! If you check the cart, you'll see all the variables we added at the bottom, along with a bunch of other pre-selected columns:\n\n![All variables added to the cart, but no samples are selected](img/04-cart-no-samples.png){.lightbox}\n\nWe can't download any data yet, though. We've selected the variables‚Äînow we need to select the samples. Go back to the \"Select data\" page and click on \"Select samples\" (or click on the \"Add more samples\" button at the top of the data cart page).\n\nBy default, IPUMS will have a bunch of different samples pre-checked. In my case, it grabbed all annual ASEC surveys from 2010‚Äì2024, and all monthly surveys from 2021‚Äì2024.\n\n::: {.panel-tabset}\n### Pre-selected ASEC samples\n\n![Pre-selected ASEC samples](img/05-default-samples-1.png){.lightbox}\n\n### Pre-selected monthly samples\n\n![Pre-selected monthly samples](img/05-default-samples-2.png){.lightbox}\n:::\n\nIncluding all these samples would be useful if we were doing some sort of analysis of CPS trends over time, comparing changes in age or education or volunteering or whatever. But that doesn't matter here‚Äîall we want to know is what age (and everything else) looked like at the time the survey was administered. That means we really just need one year.\n\nHowever, we can't just choose one sample. Things like demographics are availble in all annual and monthly samples, but volunteering is only available in September in specific years, and voting is only available in November in specific years.\n\nThis survey was administered in mid-2019, so we'll choose samples that are as close to that as possible. Though demographics are available both monthly and annually, I like to use the annual versions because ASEC data is typically used to stand in for annual information‚Äîlike if you were building a state-year panel dataset, you'd use ASEC data for each year. The ASEC occurs in March and actually overlaps with the monthly March data ([IPUMS has a note about that](https://cps.ipums.org/cps/basic_asec.shtml)), so the data *technically* is for March 2019, but whatever. We don't have to be super precise here.\n\nWe'll use the September 2019 sample for volunteering, even though that's after the survey was administered. The next earliest volunteering data is the September 2017 sample, which is like 2 years before the survey. Things don't line up precisely, but again, that's fine.\n\nFinally, we'll use the November 2018 sample for voting. That's before the survey, but it's the closest we can get‚Äîthe next alternative is November 2020, which is a year after the survey. Once again, nothing lines up exactly, but it's fine.\n\nIn summary, here are the variables we want and the samples we'll get them from:\n\n- Age (`AGE`): 2019 ASEC\n- Sex (`SEX`): 2019 ASEC\n- Marital status (`MARST`): 2019 ASEC\n- Education (`EDUC`): 2019 ASEC\n- Donating (`VLDONATE`): 2019-09 Monthly\n- Volunteering (`VLSTATUS`): 2019-09 Monthly\n- Volunteering Supplement weight (`VLSUPPWT`): 2019-09 Monthly\n- Voting (`VOTED`): 2018-11 Monthly\n- Voter Supplement weight (`VOSUPPWT`): 2018-11 Monthly\n\nSelect those three samples (2019 ASEC, September 2019, and November 2018) and click on \"Submit sample selections\" to add them to the cart.\n\n::: {.panel-tabset}\n### Specific ASEC sample\n\n![March 2019 ASEC sample](img/05-asec-real.png){.lightbox}\n\n### Specific monthly samples\n\n![September 2019 and November 2018 monthly samples](img/05-monthly-real.png){.lightbox}\n:::\n\nThe cart should now have 9 variables and 3 samples. Conveniently, it has a little summary table showing which samples have which variables, where we can confirm that age, sex, marital status, and education are in all three, volunteering and donating are only in September 2019, and voting is only in November 2018.\n\n![All variables and samples selected and ready to go](img/05-everything-selected.png){.lightbox}\n\n## Downloading the data\n\nNow that we have all the variables and samples we care about in the data cart, we can create a data extract and download this stuff.\n\nClick on the \"Create data extract\" button at the top of the data cart page, which will take you to the official Extract Request page. There are a bunch of extra options here, and you can optionally add a description to the extract, but we'll ignore all those. Click on the \"Submit Abstract\" button and wait for the IPUMS server to compile it all.\n\n![Extract submission page](img/06-extract-page.png){.lightbox}\n\nOnce it's ready, it'll appear at your \"My Data\" page, which will have a list of all your past extracts.\n\nTo download the data, we actually need to download two things:\n\n1. **The data itself.** Click on the big green \"Download .DAT\" button.\n\n   ![Button for download a `.dat` version of the data](img/07-data-page-download-dat.png){.lightbox}\n\n::: {.callout-note}\n### `.dat` vs `.dat.gz`\n\nDepending on your browser, the downloaded file will either end in `.dat` or `.dat.gz`. If it ends in `.gz`, it'll be compressed and zipped (the compressed version of this extract is 7.6 MB); if it ends in `.dat`, it'll be uncompressed and huge (‚âà55 MB in this case). Chrome and Firefox will keep the compressed `.gz` version; Safari will automatically unzip it and throw away the `.gz` version, which is annoying.\n\nTry to keep the compressed version. You don't even need to extract/unzip it‚Äîthe {ipumsr} data loading functions will handle unzipping for you automatically behind the scenes.\n:::\n\n2. The machine-readable XML codebook, or **the DDI file**. R uses to clean and relabel the raw data when you load it. If you click on the DDI link in the Codebook column, your browser will likely open a plain text XML file, which isn't really what you want. Instead, right click on the DDI link and choose \"Save file as‚Ä¶\" or \"Download file as‚Ä¶\" or whatever your browser calls it. This will let you save the XML file to your computer.\n\n   ![Context menu for the DDI codebook link](img/07-data-page-download-ddi.png){.lightbox width=\"50%\"}\n\nThere are some other helpful links there too:\n\n- If you click on the R link, it'll give you a barebones R script for loading the data. It'll look something like this:\n\n  ```{.r}\n  # NOTE: To load data, you must download both the extract's data and the DDI\n  # and also set the working directory to the folder with these files (or change the path below).\n\n  if (!require(\"ipumsr\")) stop(\"Reading IPUMS data into R requires the ipumsr package. It can be installed using the following command: install.packages('ipumsr')\")\n\n  ddi <- read_ipums_ddi(\"cps_00001.xml\")\n  data <- read_ipums_micro(ddi)\n  ```\n\n- If you click on the basic codebook, you'll get a short plain text version of the codebook, which I find really helpful for remembering which variables show up where and how each variable is coded.\n\nMove the newly downloaded `.dat` and the `.ddi` files to the same folder somewhere on your computer (preferably in an RStudio Project or a Positron project/folder or wherever your R working directory is). I put mine in a folder named `raw_data`; you can put it wherever.\n\nWe're finally ready to load this CPS data into R!\n\n# More reproducible alternative: using the IPUMS API\n\nAlternatively, it's possible to use the {ipumsr} package to access the IPUMS API directly and not need to manually download the data extract from the IPUMS website.\n\nThe {ipumsr} API functions essentially let you programmatically create and download a data extract cart. Unless you know the IPUMS CPS data *really well*, **you'll still likely need to hunt around the website for specific variables and their availabilities**, so the whole previous section is still relevant.\n\n[The {ipumsr} vignette for working with the API](https://tech.popdata.org/ipumsr/articles/ipums-api.html) is nice and complete‚Äîsee that for full details. \n\nHere's an abbreviated example of how to get the same data extract we collected manually from the website:\n\n1. Go to your IPUMS dashboard and [create an API key](https://account.ipums.org/api_keys). This needs to be stored as an environment variable named `IPUMS_API_KEY`. You can manually add this to your `.Renviron` file, or you can run this to make {ipumsr} do it for you:\n\n   ```{.r}\n   ipumsr::set_ipums_api_key(\"BLAH\", save = TRUE)\n   ```\n\n2. Build a data extract with `define_extract_micro()`. This is the equivalent of adding stuff to your data cart on the IPUMS website. You'll need to know two things:\n\n   - The variable names, which you can find by searching the IPUMS website\n\n   - The sample IDs, which you can find by running `get_sample_info()`:\n\n     ```{.r}\n     library(tidyverse)\n     library(ipumsr)\n\n     all_cps_samples <- get_sample_info(collection = \"cps\")\n\n     # Find the names for 2019 samples\n     all_cps_samples |> \n       filter(str_detect(description, \"ASEC 2019\"))\n     #> # A tibble: 13 √ó 2\n     #>    name        description              \n     #>    <chr>       <chr>                    \n     #>  1 cps2019_01s IPUMS-CPS, January 2019  \n     #>  2 cps2019_02s IPUMS-CPS, February 2019 \n     #>  3 cps2019_03b IPUMS-CPS, March 2019    \n     #>  4 cps2019_04b IPUMS-CPS, April 2019    \n     #>  5 cps2019_05s IPUMS-CPS, May 2019      \n     #>  6 cps2019_06s IPUMS-CPS, June 2019     \n     #>  7 cps2019_03s IPUMS-CPS, ASEC 2019  \n     #>  ...\n     ```\n\n   The code for creating the extract will look like this:\n\n   ```{.r}\n   cps_extract_definition <- define_extract_micro(\n     collection = \"cps\",\n     description = \"API extract for blog post\",\n     samples = c(\n       \"cps2019_03s\",  # ASEC, March 2019\n       \"cps2019_09s\",  # CPS, September 2019\n       \"cps2018_11s\"   # CPS, November 2018\n     ),\n     variables = c(\n       \"AGE\", \"SEX\", \"MARST\", \"EDUC\",\n       \"VLDONATE\", \"VLSTATUS\", \"VLSUPPWT\",\n       \"VOTED\", \"VOSUPPWT\"\n     )\n   )\n   ```\n\n   This is identical to what we had in the cart on the the website: 9 variables and 3 samples:\n\n   ```{.r}\n   cps_extract_definition\n   #> Unsubmitted IPUMS CPS extract \n   #> Description: API extract for blog post\n   #> \n   #> Samples: (3 total) cps2019_03s, cps2019_09s, cps2018_11s\n   #> Variables: (9 total) AGE, SEX, MARST, EDUC, VLDONATE, VLSTATUS, VLSU...\n   ```\n\n3. Submit the request to the server to generate the extract. This is equivalent to checking out your data cart on the website.\n\n   ```{.r}\n   cps_extract <- submit_extract(cps_extract_definition)\n   #> Successfully submitted IPUMS CPS extract number ZZZZ\n   ```\n\n4. Download the extract. The extract won't be downloadable immediately‚Äîyou need to wait for e-mail confirmation. Once it's ready, you can download it with `download_extract()`, which will download both the `.dat.gz` data and the `.xml` codebook to your computer:\n\n   ```{.r}\n   cps_downloaded <- download_extract(cps_extract, download_dir = \"raw_data\")\n   #>  |==================================================| 100%\n   #>  |==================================================| 100%\n   #> DDI codebook file saved to ~/blah/raw_data/cps_ZZZZ.xml\n   #> Data file saved to ~/blah/raw_data/cps_ZZZZ.dat.gz\n   ```\n\n5. Once it's on your computer, you can load it with the standard {ipumsr} process shown in the next section. \n\n   ```{.r}\n   cps_data <- read_ipums_micro(cps_downloaded)\n   ```\n\n::: {.callout-important}\n### The IPUMS API and literate programming\n\nIf you're using a literate programming document Quarto or R Markdown, **don't include this API extraction process in your document**. It will rerun every time you render your document and create a new IPUMS extract each time, which is excessive. It's best to run this process in a separate R script or function (perhaps orchestrated with something like [{targets}](https://books.ropensci.org/targets/)), and then load the DDI `.xml` and `.dat` data in the document.\n:::\n\n\n# Loading CPS data\n\nGetting this data into R is easy thanks to [the {ipumsr} package](https://tech.popdata.org/ipumsr/). We feed the XML DDI codebook into `read_ipums_ddi()` and then feed that into `read_ipums_micro()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ipumsr)\n\nddi <- read_ipums_ddi(\"raw_data/cps_00001.xml\")\ncps_data <- read_ipums_micro(ddi, data_file = \"raw_data/cps_00001.dat.gz\", verbose = FALSE)\n\nglimpse(cps_data)\n## Rows: 421,402\n## Columns: 21\n## $ YEAR     <dbl> 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018,‚Ä¶\n## $ SERIAL   <dbl> 1, 1, 3, 4, 4, 4, 4, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9, 9, 10, 10, 11, 12, 13, 13, 14, 14, 14, 15, 15, 16, 17, 19, 20, 20, 21, 21, 22, 23, 23, 23, 23, 24, 25, 26, 26, 26, 26, 26, 28, 28, 28, 28, 29, 30, 30, 30, 32, 36, 36, 37, 37, 37, 37, 39, 39, 39, 39, 39, 40, 40, 41, 41, 41, 41, 42,‚Ä¶\n## $ MONTH    <int+lbl> 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1‚Ä¶\n## $ HWTFINL  <dbl> 1704, 1704, 1957, 1688, 1688, 1688, 1688, 2090, 2090, 1832, 1832, 1779, 1779, 1779, 1853, 1853, 2077, 2077, 1427, 1427, 1611, 2044, 1738, 1738, 1690, 1690, 1690, 3135, 3135, 2679, 2253, 1639, 1615, 1615, 1515, 1515, 2254, 1459, 1459, 1459, 1459, 1960, 1942, 1701, 1701, 1701, 1701,‚Ä¶\n## $ CPSID    <dbl> 2.017e+13, 2.017e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e‚Ä¶\n## $ ASECFLAG <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n## $ ASECWTH  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n## $ PERNUM   <dbl> 1, 2, 1, 1, 2, 3, 4, 1, 2, 1, 2, 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 1, 2, 3, 1, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 1, 1, 1, 2, 1, 1, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4,‚Ä¶\n## $ WTFINL   <dbl> 1704, 1845, 1957, 1688, 2780, 2780, 2679, 2090, 2090, 1832, 2679, 1754, 1779, 2452, 1853, 1870, 2151, 2077, 2003, 1427, 1611, 2044, 1738, 1738, 1690, 2102, 2537, 3135, 4172, 2679, 2253, 1639, 1615, 2609, 1900, 1515, 2254, 1459, 1546, 1912, 1752, 1960, 1942, 1527, 1701, 2231, 2104,‚Ä¶\n## $ CPSIDP   <dbl> 2.017e+13, 2.017e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e‚Ä¶\n## $ CPSIDV   <dbl> 2.017e+14, 2.017e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.018e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.018e+14, 2.018e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.018e+14, 2.018e+14, 2.018e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e‚Ä¶\n## $ ASECWT   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n## $ AGE      <int+lbl> 26, 26, 48, 53, 16, 16, 20, 22, 23, 57, 23, 61, 62, 39, 74, 49, 54, 52, 69, 76, 41, 56, 64, 62, 53, 13, 21, 28, 28, 40, 51, 78, 64, 40, 59, 36, 27, 35, 36, 5, 8, 76, 80, 36, 33, 3, 6, 11, 62, 80, 61, 61, 57, 24, 22, 26, 61, 24, 0, 37, 3, 5, 32, 33, 8, 9, 10, 15, 55, 61, 29, 29‚Ä¶\n## $ SEX      <int+lbl> 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2‚Ä¶\n## $ MARST    <int+lbl> 6, 6, 4, 4, 6, 6, 6, 6, 6, 4, 6, 1, 1, 6, 5, 6, 1, 1, 1, 1, 6, 4, 6, 6, 5, 9, 6, 1, 1, 6, 3, 5, 6, 6, 1, 1, 6, 1, 1, 9, 9, 5, 4, 1, 1, 9, 9, 9, 6, 5, 4, 3, 4, 6, 6, 6, 3, 6, 9, 3, 9, 9, 6, 6, 9, 9, 9, 6, 1, 1, 1, 1, 9, 9, 6, 4, 1, 1, 4, 1, 1, 9, 1, 1, 1, 1, 9, 9, 1, 1, 4, 1, 5‚Ä¶\n## $ EDUC     <int+lbl> 111, 123, 73, 81, 50, 50, 81, 81, 81, 111, 81, 81, 81, 92, 81, 81, 123, 111, 81, 60, 111, 73, 73, 73, 81, 1, 81, 81, 81, 91, 111, 60, 73, 73, 125, 81, 92, 124, 111, 1, 1, 60, 73, 123, 125, 1, 1, 1, 73, 30, 73, 73, 124, 81, 92, 73, 73, 73, 1, 20, 1, 1, 20, 73, 1, 1, 1, 30, 111,‚Ä¶\n## $ VLSTATUS <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n## $ VLDONATE <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n## $ VLSUPPWT <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n## $ VOTED    <int+lbl> 98, 98, 2, 2, 99, 99, 2, 99, 99, 98, 98, 2, 2, 2, 1, 97, 2, 2, 1, 1, 2, 98, 2, 1, 2, 99, 1, 1, 2, 98, 98, 2, 98, 98, 98, 98, 2, 2, 2, 99, 99, 1, 2, 1, 1, 99, 99, 99, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 99, 98, 99, 99, 98, 2, 99, 99, 99, 99, 1, 1, 2, 2, 99, 99, 2, 98, 2, 2, 96, 2, 2,‚Ä¶\n## $ VOSUPPWT <dbl> 1704, 1845, 1957, 1688, 2780, 2780, 2679, 2090, 2090, 1832, 2679, 1754, 1779, 2452, 1853, 1870, 2151, 2077, 2003, 1427, 1611, 2044, 1738, 1738, 1690, 2102, 2537, 3135, 4172, 2679, 2253, 1639, 1615, 2609, 1900, 1515, 2254, 1459, 1546, 1912, 1752, 1960, 1942, 1527, 1701, 2231, 2104,‚Ä¶\n```\n:::\n\n\n\nholy moly we have nearly half a million rows. That's because we have three samples (2019 ASEC, September 2019, and November 2018) and they're all stacked on top of each other in this data. We need to filter this huge data to extract the three samples. We'll also remove rows with missing data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncps_demographics <- cps_data |>\n  # Only look at the 2019 ASEC data\n  filter(YEAR == 2019, MONTH == 03, ASECFLAG == 1) |>\n  # Remove rows that are missing or are \"not in universe\"\n  mutate(\n    SEX = ifelse(SEX == 9, NA, SEX),\n    MARST = ifelse(MARST == 9, NA, MARST),\n    EDUC = ifelse(EDUC < 1 | EDUC == 999, NA, EDUC)\n  )\n\ncps_volunteer <- cps_data |> \n  filter(YEAR == 2019, MONTH == 09) |> \n  # Remove rows that are missing or are \"not in universe\"\n  mutate(across(c(VLSTATUS, VLDONATE), \\(x) ifelse(x == 99, NA, x)))\n\ncps_voting <- cps_data |> \n  filter(YEAR == 2018, MONTH == 11) |>\n  # Remove rows that are missing or are \"not in universe\"\n  mutate(VOTED = ifelse(VOTED == 99, NA, VOTED))\n```\n:::\n\n\n\nThese counts are more reasonable (but still huge!)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(cps_demographics)\n## [1] 180101\nnrow(cps_volunteer)\n## [1] 118557\nnrow(cps_voting)\n## [1] 122744\n```\n:::\n\n\n\n# Summarizing CPS data\n\nUltimately, our goal is to find the population-level average of a bunch of characteristics and see if our sample plausibly matches population averages.\n\nThings get a little tricky and loosey-goosey here. The different levels measured by the CPS don't always match what's in the survey. For example, the CPS measures sex and provides only 2 levels (1 = male; 2 = female); the experiment called this construct gender and included male, female,^[We should have called these \"man\" and \"woman\" since gender ‚â† sex.] transgender, prefer not to say, and other. \n\nTo make the survey question reasonably match what the CPS is capturing, I find that it's easiest to collapse both the survey data and the CPS data to simpler constructs. Before we collapse things, though, we need to look at one statistical issue: weighting.\n\n## Weighting\n\nIn an effort to make the CPS nationally representative, every row is [weighted](https://cps.ipums.org/cps-action/faq#7)‚Äî[each individual does not represent the same number of persons in the population](https://cps.ipums.org/cps-action/faq#ques21). The Census oversamples some subpopulations and shifts weights up and down to give individuals more or less statistical influence in the sample so that the survey results better approximate the characteristics of the general population. Any analysis we do with CPS data needs to take those weights into account.\n\nThe weights for ASEC variables are included in the `ASECWT` column; the weights for volunteering and voting variables are in the `VLSUPPWT` and `VOSUPPWT` columns\n\nIf we're calculating basic averages, we can use `weighted.mean()` instead of `mean()`. Note the difference in average when we don't weight!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncps_demographics |> \n  summarize(\n    avg_age_weighted = weighted.mean(AGE, w = ASECWT),  # BAD\n    avg_age_unweighted = mean(AGE)  # GOOD\n  )\n## # A tibble: 1 √ó 2\n##   avg_age_weighted avg_age_unweighted\n##              <dbl>              <dbl>\n## 1             38.8               37.3\n```\n:::\n\n\n\nIf we're doing stuff with models, we can use the `weights` argument:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# BAD: Non-weighted intercept-only model\nlm(AGE ~ 1, data = cps_demographics) |> \n  tidy(conf.int = TRUE)\n## # A tibble: 1 √ó 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n## 1 (Intercept)     37.3    0.0540      690.       0     37.2      37.4\n\n# GOOD: Weighted intercept-only model\nlm(AGE ~ 1, data = cps_demographics, weights = ASECWT) |> \n  tidy(conf.int = TRUE)\n## # A tibble: 1 √ó 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n## 1 (Intercept)     38.8    0.0542      715.       0     38.7      38.9\n```\n:::\n\n\n\nBase R only really has `weighted.mean()`. If we want other things, like a weighted variance, or weighted rank, or weighted table/crosstabs, we can use a bunch of different functions in the {Hmisc} package:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Some Hmisc::wtd.*() things:\ncps_demographics |> \n  summarize(\n    avg_age = Hmisc::wtd.mean(AGE, weights = ASECWT),\n    var_age = Hmisc::wtd.var(AGE, weights = ASECWT),\n    sd_age = sqrt(var_age)\n  )\n## # A tibble: 1 √ó 3\n##   avg_age var_age sd_age\n##     <dbl>   <dbl>  <dbl>\n## 1    38.8    529.   23.0\n```\n:::\n\n\n\n## Calculating population-level proportions\n\nWe'll collapse these population-level CPS values into binary versions of each question so that we can look at things like the proportion of women, the proportion of people who volunteer, and so on. We'll also collapse age into a binary above/below the median age‚Äîthis isn't necessary, and we could totally work with numeric age instead of proportions, but in our anonymized survey data, our age column is an indicator representing being above/below 36 (the median age at the time of the survey).\n\nWe'll do some basic summarizing with `weighted.mean()` and calculate all these national proportions, along with the weighted standard deviations (which will be important for the Bayesian analysis later in this post).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnational_demographics <- cps_demographics |> \n  summarize(\n    # AGE is already numeric\n    age = weighted.mean(AGE >= 36, ASECWT), \n    age_sd = sqrt(Hmisc::wtd.var(AGE >= 36, weights = ASECWT)),\n\n    # 1 = Female\n    female = weighted.mean(SEX == 2, ASECWT),\n    female_sd = sqrt(Hmisc::wtd.var(SEX == 2, weights = ASECWT)),\n\n    # 1 = Married, spouse present\n    # 2 = Married, spouse absent\n    married = weighted.mean(MARST %in% 1:2, na.rm = TRUE),\n    married_sd = sqrt(Hmisc::wtd.var(MARST %in% 1:2, weights = ASECWT)),\n\n    # 111 = Bachelor's degree\n    college = weighted.mean(EDUC >= 111, ASECWT, na.rm = TRUE),\n    college_sd = sqrt(Hmisc::wtd.var(EDUC >= 111, weights = ASECWT))\n)\n\nnational_volunteer <- cps_volunteer |> \n  summarize(\n    # 1 = Volunteer\n    volunteering = weighted.mean(VLSTATUS == 1, VLSUPPWT, na.rm = TRUE),\n    volunteering_sd = sqrt(Hmisc::wtd.var(VLSTATUS == 1, weights = VLSUPPWT)),\n\n    # 2 = Yes, made a donation to charity in the past 12 months\n    donating = weighted.mean(VLDONATE == 2, VLSUPPWT, na.rm = TRUE),\n    donating_sd = sqrt(Hmisc::wtd.var(VLDONATE == 2, weights = VLSUPPWT))\n  )\n\nnational_voting <- cps_voting |>\n  summarize(\n    # 2 = Voted in the most recent November election\n    voting = weighted.mean(VOTED == 2, VOSUPPWT, na.rm = TRUE),\n    voting_sd = sqrt(Hmisc::wtd.var(VOTED == 2, weights = VOSUPPWT))\n  )\n```\n:::\n\n\n\nI like to store these in a little one-row data frame so that it's easy to access invidiual values:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnational_values <- bind_cols(\n  national_demographics, national_volunteer, national_voting\n)\nnational_values\n## # A tibble: 1 √ó 14\n##     age age_sd female female_sd married married_sd college college_sd volunteering volunteering_sd donating donating_sd voting voting_sd\n##   <dbl>  <dbl>  <dbl>     <dbl>   <dbl>      <dbl>   <dbl>      <dbl>        <dbl>           <dbl>    <dbl>       <dbl>  <dbl>     <dbl>\n## 1 0.530  0.499  0.510     0.500   0.410      0.492   0.258      0.437        0.300           0.458    0.474       0.499  0.534     0.499\n\n# Proportion of women\nnational_values$female\n## [1] 0.5097\n\n# Proportion that voted\nnational_values$voting\n## [1] 0.5344\n```\n:::\n\n\n\n# Summarizing sample proportions\n\nWe're almost done! All that's left is testing whether the demographic characteristics of the survey experiment respondents reasonably match their corresponding population proportions.\n\nFirst, though, we need to make binary versions of the survey responses. To make life easier, we'll use the same names as the CPS data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresults_to_test <- results |> \n  mutate(\n    age = age == \"More than median\",\n    female = gender == \"Female\",\n    married = marital_status == \"Married\",\n    college = education %in% c(\n      \"4 year degree\", \n      \"Graduate or professional degree\", \n      \"Doctorate\"\n    ),\n    volunteering = volunteer_frequency != \"Haven't volunteered in past 12 months\",\n    donating = donate_frequency == \"More than once a month, less than once a year\",\n    voting = voted == \"Yes\"\n  ) |> \n  select(female, age, married, college, volunteering, donating, voting)\n\nglimpse(results_to_test)\n## Rows: 1,300\n## Columns: 7\n## $ female       <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, ‚Ä¶\n## $ age          <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, F‚Ä¶\n## $ married      <lgl> TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRU‚Ä¶\n## $ college      <lgl> TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, ‚Ä¶\n## $ volunteering <lgl> TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TR‚Ä¶\n## $ donating     <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n## $ voting       <lgl> TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE‚Ä¶\n```\n:::\n\n\n\n# Testing sample vs. population proportions frequentist-ly\n\n## One-sample proportion test for age\n\nAs a quick and easy check, we can run a one-sample proportion test to see if the proportion of a variable is significantly different from a null value. We can do this with `prop.test()`, which works a bunch of different ways‚Äîwith matrices, with vectors, and with single values ([see this blog post](https://www.andrewheiss.com/blog/2023/05/15/fancy-bayes-diffs-props/index.html#classically) for some other examples of `prop.test()`).\n\nLet's look at age first. 50.77% of people in the sample are older than 36; 53% of people in the population are older than 36:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Proportion of sample older than 36\nmean(results_to_test$age)\n## [1] 0.5077\n\n# CPS proportion older than 36\nnational_values$age\n## [1] 0.5303\n```\n:::\n\n\n\nIs that an issue? Is the sample significantly younger than the rest of the country?\n\nFor this one-sample test, we need to feed `prop.test()` three things: (1) the number of \"successes\", or the count of rows where the respondent is older than 36 (or where `age` is `TRUE`), (2) the number of rows in the sample, and (3) the null value, or the population-level CPS proportion:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprop_test_freq_age <- prop.test(\n  x = sum(results_to_test$age),  # Number of \"successes\" (rows where age == TRUE)\n  n = nrow(results_to_test),     # Sample size\n  p = national_values$age        # Population-level proportion from the CPS\n)\n\ntidy(prop_test_freq_age)\n## # A tibble: 1 √ó 8\n##   estimate statistic p.value parameter conf.low conf.high method                                               alternative\n##      <dbl>     <dbl>   <dbl>     <int>    <dbl>     <dbl> <chr>                                                <chr>      \n## 1    0.508      2.59   0.108         1    0.480     0.535 1-sample proportions test with continuity correction two.sided\n```\n:::\n\n\n\nFor fun, we can plot this too:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this plot ‚Üì\"}\nprop_test_freq_age |> \n  tidy() |> \n  mutate(\n    prop_label = glue(\n      \"Sample proportion\\n{prop} [{low}, {high}]\",\n      prop = label_percent(accuracy = 0.01)(estimate),\n      low = label_percent(accuracy = 0.01)(conf.low),\n      high = label_percent(accuracy = 0.01)(conf.high)\n    )\n  ) |> \n  ggplot(aes(x = estimate, y = \"Age\")) +\n  geom_vline(\n    xintercept = national_values$age, color = clrs[2]\n  ) +\n  annotate(\n    geom = \"label\", x = national_values$age, y = I(1.3), \n    label = glue(\n      \"CPS proportion\\n{x}\", \n      x = label_percent(accuracy = 0.01)(national_values$age)\n    ),\n    fill = clrs[2], color = \"white\", size = 8, size.unit = \"pt\"\n  ) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_label(aes(label = prop_label), nudge_y = -0.3, size = 8, size.unit = \"pt\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Proportion older than 36\", y = NULL)\n```\n\n::: {.cell-output-display}\n![Proportion of survey respondents older than 36 compared to the national proportion](index_files/figure-html/fig-age-sample-prop-freq-1.png){#fig-age-sample-prop-freq fig-align='center' width=90%}\n:::\n:::\n\n\n\nBased on this, the 95% confidence interval for the proportion in the sample is 0.48‚Äì0.54, and the null/population value is 0.53, which fits safely in that confidence interval. The corresponding *p*-value is 0.108, which means that the sample proportion isn't significantly different from the national proportion. \n\nWe can't be certain that the sample doesn't generally match the population, age-wise. \n\nThat's a horribly convoluted sentence‚Äîwelcome to the world of frequentist null hypothesis testing! Technically we can't really say that the sample matches the population, and we can only say that we don't know if it doesn't match. Stay tuned for the Bayesian analysis section for a way to get an answer that we do care about. For now, as a kind of cheat-y shorthand, we can (semi-illegally) say that since the CPS proportion is in the sample confidence interval, there probably isn't a significant difference between the two.\n\n## One-sample proportion test for volunteering\n\nLet's do another one: volunteering. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprop_test_freq_vol <- prop.test(\n  x = sum(results_to_test$volunteering),\n  n = nrow(results_to_test),\n  p = national_values$volunteering\n)\n\ntidy(prop_test_freq_vol)\n## # A tibble: 1 √ó 8\n##   estimate statistic   p.value parameter conf.low conf.high method                                               alternative\n##      <dbl>     <dbl>     <dbl>     <int>    <dbl>     <dbl> <chr>                                                <chr>      \n## 1    0.598      546. 8.79e-121         1    0.570     0.624 1-sample proportions test with continuity correction two.sided\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this plot ‚Üì\"}\nprop_test_freq_vol |> \n  tidy() |> \n  mutate(\n    prop_label = glue(\n      \"Sample proportion\\n{prop} [{low}, {high}]\",\n      prop = label_percent(accuracy = 0.01)(estimate),\n      low = label_percent(accuracy = 0.01)(conf.low),\n      high = label_percent(accuracy = 0.01)(conf.high)\n    )\n  ) |> \n  ggplot(aes(x = estimate, y = \"Volunteering\")) +\n  geom_vline(\n    xintercept = national_values$volunteering, color = clrs[2]\n  ) +\n  annotate(\n    geom = \"label\", x = national_values$volunteering, y = I(1.3), \n    label = glue(\n      \"CPS proportion\\n{x}\", \n      x = label_percent(accuracy = 0.01)(national_values$volunteering)\n    ),\n    fill = clrs[2], color = \"white\", size = 8, size.unit = \"pt\"\n  ) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_label(aes(label = prop_label), nudge_y = -0.3, size = 8, size.unit = \"pt\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Proportion that volunteers regularly\", y = NULL) +\n  expand_limits(x = c(0.26, 0.7))\n```\n\n::: {.cell-output-display}\n![Proportion of survey respondents that volunteer compared to the national proportion](index_files/figure-html/fig-age-sample-prop-vol-1.png){#fig-age-sample-prop-vol fig-align='center' width=90%}\n:::\n:::\n\n\n\nPhew, this one is *way* off. 30% of the general population has volunteered in the last year; 60% of the sample has volunteered. The sample proportion is most definitely significantly different from the general population, and we can reject the null hypothesis that it's the same.\n\nIn this case, that's fine. In our experiment, we only wanted to test our different treatments on people who donate to charity on at least an annual basis, so we screened out respondents who hadn't donated in the past year. Volunteer behavior and donation behavior are closely correlated, so we have way more volunteers in our sample.\n\n## Proportion tests and differences for everything all at once\n\nWe can repeat this one-sample proportion test for all the different characteristics we care about. Instead of repeating the same code over and over, we'll make a little wrapper function for it. We'll also calculate the difference between the sample and population proportions:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"`prop_test_freq()` wrapper function\"}\n#' Perform a basic one-sample proportion test\n#'\n#' @param sample_column A numeric vector representing the sample data (0s and 1s).\n#' @param cps_prop A numeric value representing the proportion to compare against.\n#' @return A tibble containing the test results and the differences between the\n#'         sample estimate and the specified proportion.\n#' @examples\n#' sample_data <- c(1, 0, 1, 1, 0, 1, 0, 1, 1, 0)\n#' cps_prop <- 0.5\n#' prop_test_freq(sample_data, cps_prop)\nprop_test_freq <- function(sample_column, cps_prop) {\n  n_yes <- sum(sample_column)\n  n_total = length(sample_column)\n\n  sample_prop_test <- prop.test(\n    x = n_yes,\n    n = n_total,\n    p = cps_prop\n  )\n\n  out_df <- tidy(sample_prop_test) |> \n    mutate(\n      diff = estimate - cps_prop,\n      diff_low = conf.low - cps_prop,\n      diff_high = conf.high - cps_prop\n    )\n\n  return(tibble(test = list(sample_prop_test), out_df))\n}\n```\n:::\n\n\n\nWe'll then create a little summary dataset and plug each row of it into our new `prop_test_freq()` function with the magic of `purrr::map()`, which will store the results from the hypothesis test in a list column named `prop_test_results`, which we'll finally unnest so that we can access the results as columns:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Create `sample_cps_props_freq` and use `prop_test_freq()` on each row\"}\nsample_cps_props_freq <- tribble(\n  ~category, ~variable, ~sample_value, ~national_value,\n  \"Demographics\", \"Age (% 36+)\", results_to_test$age, national_values$age,\n  \"Demographics\", \"Female (%)\", results_to_test$female, national_values$female,\n  \"Demographics\", \"Married (%)\", results_to_test$married, national_values$married,\n  \"Demographics\", \"Education (% BA+)\", results_to_test$college, national_values$college,\n  \"Philanthropy\", \"Donated in past year (%)\", results_to_test$donating, national_values$donating,\n  \"Philanthropy\", \"Volunteered in past year (%)\", results_to_test$volunteering, national_values$volunteering,\n  \"Voting\", \"Voted in last November election (%)\", results_to_test$voting, national_values$voting\n) |> \n  mutate(prop_test_results = pmap(\n    list(sample_value, national_value), \\(x, y) prop_test_freq(x, y)\n  )) |>\n  unnest_wider(prop_test_results)\n```\n:::\n\n\n\nWe can use this new `sample_cps_props_freq` data frame for plotting:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this plot ‚Üì\"}\nsample_cps_props_freq |>\n  mutate(across(c(variable, category), \\(x) fct_inorder(x))) |>\n  ggplot(aes(x = estimate, y = fct_rev(variable))) +\n  # It would be nicer to use geom_segment() to add vertical lines here, but it \n  # doesn't play nicely with categorical y-axis breaks, so we can cheat and use \n  # geom_point() instead with `shape = \"|\"` to use the | character\n  geom_point(aes(x = national_value, color = \"CPS proportion\"), shape = \"|\", size = 6) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high, color = \"Sample proportion\")) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(\"CPS proportion\" = clrs[2], \"Sample proportion\" = \"black\")) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  labs(x = \"Average proportion\", y = NULL, color = NULL)\n```\n\n::: {.cell-output-display}\n![Respondent demographic proportions compared to national proportions](index_files/figure-html/fig-prop-test-freq-props-1.png){#fig-prop-test-freq-props fig-align='center' width=90%}\n:::\n:::\n\n\n\nThat looks super neat and it's helpful to visualize all these differences. In general, the sample looks like the population in terms of age, gender, marital status, and education, but the sample is way more socially and civically oriented than the rest of the country (again, by design).\n\nThe only issue with this plot is that it's a little hard to read with the CPS proportion moving around in each variable. We can center it at 0 and look at differences from the CPS proportion:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this plot ‚Üì\"}\nsample_cps_props_freq |> \n  mutate(across(c(variable, category), \\(x) fct_inorder(x))) |> \n  ggplot(aes(x = diff, y = fct_rev(variable))) +\n  geom_vline(aes(xintercept = 0), color = clrs[2]) +\n  geom_pointrange(\n    aes(xmin = diff_low, xmax = diff_high, color = \"Sample proportion ‚àí CPS proportion\")\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(\"Sample proportion ‚àí CPS proportion\" = \"black\")) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  labs(x = \"Difference in proportion from CPS\", y = NULL, color = NULL)\n```\n\n::: {.cell-output-display}\n![Differences between demographic proportions and national proportions](index_files/figure-html/fig-prop-test-freq-diffs-1.png){#fig-prop-test-freq-diffs fig-align='center' width=90%}\n:::\n:::\n\n\n\nAnd we can also use `sample_cps_props_freq` to make a pretty table. Here's a table with [{tinytable}](https://vincentarelbundock.github.io/tinytable/) (though you could do this with any of R's tablemaking packages, like [{gt}](https://gt.rstudio.com/) or [{kableExtra}](https://haozhu233.github.io/kableExtra/) or whatever)\n\n\n\n::: {#tbl-prop-test-freq .cell .column-body-outset-right layout-align=\"center\" tbl-cap='Sample characteristics compared to nationally representative Current Population Survey (CPS) estimates'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this table ‚Üì\"}\nnotes <- list(\n  \"*\" = \"Sample proportion significantly different from CPS (p < 0.05)\",\n  \"a\" = list(i = 1:4, j = 1, text = \"Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019\"),\n  \"b\" = list(i = 5:6, j = 1, text = \"Monthly CPS, September 2019\"),\n  \"c\" = list(i = 7, j = 1, text = \"Monthly CPS, November 2018\")\n)\n\nsample_cps_props_freq |> \n  mutate(significant = ifelse(p.value < 0.05, \"*\", \"\")) |> \n  mutate(sample_nice = glue(\n    \"{estimate}{significant}<br>[{conf.low}, {conf.high}]\",\n    estimate = label_percent(accuracy = 0.1)(estimate),\n    conf.low = label_percent(accuracy = 0.1)(conf.low),\n    conf.high = label_percent(accuracy = 0.1)(conf.high)\n  )) |> \n  mutate(diff_nice = glue(\n    \"{diff}{significant}<br>[{diff_low}, {diff_high}]\",\n    diff = label_pp_01(diff),\n    diff_low = label_number(accuracy = 0.1, scale = 100)(diff_low),\n    diff_high = label_number(accuracy = 0.1, scale = 100)(diff_high)\n  )) |> \n  select(\n    Variable = variable, \n    National = national_value, \n    Sample = sample_nice,\n    `‚àÜ` = diff_nice) |> \n  tt(width = c(0.3, 0.2, 0.3, 0.2), notes = notes) |> \n  group_tt(i = sample_cps_props_freq$category) |> \n  format_tt(j = 2, fn = label_percent(accuracy = 0.1)) |> \n  style_tt(i = c(1, 6, 9), bold = TRUE, background = \"#e6e6e6\") |> \n  style_tt(\n    bootstrap_class = \"table\",\n    bootstrap_css_rule = \".table tfoot { text-align: left; } .table { font-size: 0.85rem; }\"\n  ) |> \n  style_tt(j = 1, align = \"l\") |> \n  style_tt(j = 2:4, align = \"c\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_ld6qh6salal4ubr4rmdj(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_ld6qh6salal4ubr4rmdj\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow_66yl1bwz5sa262mezhrr(i, colspan, content) {\n        var table = document.getElementById('tinytable_ld6qh6salal4ubr4rmdj');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_ld6qh6salal4ubr4rmdj(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_ld6qh6salal4ubr4rmdj\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\nwindow.addEventListener('load', function () { insertSpanRow_66yl1bwz5sa262mezhrr(7, 4, 'Voting') });\nwindow.addEventListener('load', function () { insertSpanRow_66yl1bwz5sa262mezhrr(5, 4, 'Philanthropy') });\nwindow.addEventListener('load', function () { insertSpanRow_66yl1bwz5sa262mezhrr(1, 4, 'Demographics') });\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 10, j: 2 },  ], css_id: 'tinytable_css_w8r5j06dldozutkr0oib',}, \n          { positions: [ { i: 1, j: 0 }, { i: 6, j: 0 }, { i: 9, j: 0 },  ], css_id: 'tinytable_css_s37bt60jk7x1i3wdrl8l',}, \n          { positions: [ { i: 6, j: 3 }, { i: 1, j: 1 }, { i: 6, j: 1 }, { i: 9, j: 3 }, { i: 1, j: 3 }, { i: 9, j: 1 },  ], css_id: 'tinytable_css_ph8hne8x40wi8lva29qf',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_ov1w4cbja1z30ech1y7p',}, \n          { positions: [ { i: 3, j: 1 }, { i: 4, j: 1 }, { i: 7, j: 1 }, { i: 8, j: 1 }, { i: 2, j: 3 }, { i: 3, j: 3 }, { i: 4, j: 3 }, { i: 2, j: 1 }, { i: 7, j: 3 }, { i: 8, j: 3 }, { i: 5, j: 3 }, { i: 5, j: 1 },  ], css_id: 'tinytable_css_lnwhtekyfgwdhc3p8bc4',}, \n          { positions: [ { i: 10, j: 3 }, { i: 10, j: 1 },  ], css_id: 'tinytable_css_jlzq19oodbfh1n09fepz',}, \n          { positions: [ { i: 10, j: 0 },  ], css_id: 'tinytable_css_cyjkn1lwjswb6oakr1mk',}, \n          { positions: [ { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 2, j: 0 }, { i: 7, j: 0 }, { i: 8, j: 0 }, { i: 5, j: 0 },  ], css_id: 'tinytable_css_9w0xuixan42vkjya2u15',}, \n          { positions: [ { i: 0, j: 2 },  ], css_id: 'tinytable_css_93a8bxxxps4v26q6ixky',}, \n          { positions: [ { i: 1, j: 2 }, { i: 9, j: 2 }, { i: 6, j: 2 },  ], css_id: 'tinytable_css_296h63pemb3gbg1ddr2y',}, \n          { positions: [ { i: 0, j: 1 }, { i: 0, j: 3 },  ], css_id: 'tinytable_css_1iw7xz5nv8i699p9bnmx',}, \n          { positions: [ { i: 5, j: 2 }, { i: 3, j: 2 }, { i: 4, j: 2 }, { i: 2, j: 2 }, { i: 7, j: 2 }, { i: 8, j: 2 },  ], css_id: 'tinytable_css_1bln1l6w4n205be5mtd6',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_ld6qh6salal4ubr4rmdj(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_w8r5j06dldozutkr0oib, .table th.tinytable_css_w8r5j06dldozutkr0oib { text-align: center; border-bottom: solid #d3d8dc 0.1em; width: 30%; }\n      .table td.tinytable_css_s37bt60jk7x1i3wdrl8l, .table th.tinytable_css_s37bt60jk7x1i3wdrl8l { font-weight: bold; background-color: #e6e6e6; text-align: left; width: 30%; }\n      .table td.tinytable_css_ph8hne8x40wi8lva29qf, .table th.tinytable_css_ph8hne8x40wi8lva29qf { font-weight: bold; background-color: #e6e6e6; text-align: center; width: 20%; }\n      .table td.tinytable_css_ov1w4cbja1z30ech1y7p, .table th.tinytable_css_ov1w4cbja1z30ech1y7p { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; width: 30%; }\n      .table td.tinytable_css_lnwhtekyfgwdhc3p8bc4, .table th.tinytable_css_lnwhtekyfgwdhc3p8bc4 { text-align: center; width: 20%; }\n      .table td.tinytable_css_jlzq19oodbfh1n09fepz, .table th.tinytable_css_jlzq19oodbfh1n09fepz { text-align: center; border-bottom: solid #d3d8dc 0.1em; width: 20%; }\n      .table td.tinytable_css_cyjkn1lwjswb6oakr1mk, .table th.tinytable_css_cyjkn1lwjswb6oakr1mk { padding-left: 1em; text-align: left; border-bottom: solid #d3d8dc 0.1em; width: 30%; }\n      .table td.tinytable_css_9w0xuixan42vkjya2u15, .table th.tinytable_css_9w0xuixan42vkjya2u15 { padding-left: 1em; text-align: left; width: 30%; }\n      .table td.tinytable_css_93a8bxxxps4v26q6ixky, .table th.tinytable_css_93a8bxxxps4v26q6ixky { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; width: 30%; }\n      .table td.tinytable_css_296h63pemb3gbg1ddr2y, .table th.tinytable_css_296h63pemb3gbg1ddr2y { font-weight: bold; background-color: #e6e6e6; text-align: center; width: 30%; }\n      .table td.tinytable_css_1iw7xz5nv8i699p9bnmx, .table th.tinytable_css_1iw7xz5nv8i699p9bnmx { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; width: 20%; }\n      .table td.tinytable_css_1bln1l6w4n205be5mtd6, .table th.tinytable_css_1bln1l6w4n205be5mtd6 { text-align: center; width: 30%; }\n.table tfoot { text-align: left; } .table { font-size: 0.85rem; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table\" id=\"tinytable_ld6qh6salal4ubr4rmdj\" style=\"table-layout: fixed; width: 100% !important; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">Variable</th>\n                <th scope=\"col\">National</th>\n                <th scope=\"col\">Sample</th>\n                <th scope=\"col\">‚àÜ</th>\n              </tr>\n        </thead>\n        <tfoot><tr><td colspan='4'><sup>*</sup> Sample proportion significantly different from CPS (p < 0.05)</td></tr>\n<tr><td colspan='4'><sup>a</sup> Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019</td></tr>\n<tr><td colspan='4'><sup>b</sup> Monthly CPS, September 2019</td></tr>\n<tr><td colspan='4'><sup>c</sup> Monthly CPS, November 2018</td></tr></tfoot>\n        <tbody>\n                <tr>\n                  <td>Age (% 36+)<sup>a</sup></td>\n                  <td>53.0%</td>\n                  <td>50.8%<br>[48.0%, 53.5%]</td>\n                  <td>‚àí2.3 pp.<br>[-5.0, 0.5]</td>\n                </tr>\n                <tr>\n                  <td>Female (%)<sup>a</sup></td>\n                  <td>51.0%</td>\n                  <td>47.2%*<br>[44.4%, 49.9%]</td>\n                  <td>‚àí3.8 pp.*<br>[-6.6, -1.1]</td>\n                </tr>\n                <tr>\n                  <td>Married (%)<sup>a</sup></td>\n                  <td>41.0%</td>\n                  <td>40.6%<br>[37.9%, 43.3%]</td>\n                  <td>‚àí0.4 pp.<br>[-3.1, 2.3]</td>\n                </tr>\n                <tr>\n                  <td>Education (% BA+)<sup>a</sup></td>\n                  <td>25.8%</td>\n                  <td>28.5%*<br>[26.1%, 31.1%]</td>\n                  <td>2.8 pp.*<br>[0.4, 5.3]</td>\n                </tr>\n                <tr>\n                  <td>Donated in past year (%)<sup>b</sup></td>\n                  <td>47.4%</td>\n                  <td>56.9%*<br>[54.2%, 59.6%]</td>\n                  <td>9.5 pp.*<br>[6.8, 12.2]</td>\n                </tr>\n                <tr>\n                  <td>Volunteered in past year (%)<sup>b</sup></td>\n                  <td>30.0%</td>\n                  <td>59.8%*<br>[57.0%, 62.4%]</td>\n                  <td>29.7 pp.*<br>[27.0, 32.4]</td>\n                </tr>\n                <tr>\n                  <td>Voted in last November election (%)<sup>c</sup></td>\n                  <td>53.4%</td>\n                  <td>71.5%*<br>[69.0%, 74.0%]</td>\n                  <td>18.1 pp.*<br>[15.5, 20.5]</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n# Testing sample vs. population proportions Bayesian-ly\n\n## ew null hypothesis significance testing\n\nThat's all well and good, but I'm actually not 100% sure if that's the right proportion test to use. [As I mention here](https://www.andrewheiss.com/blog/2023/05/15/fancy-bayes-diffs-props/index.html#classically), every classical statistical test has a bunch of \"flavors\" for different situations and assumptions (e.g., do the two samples have equal or unequal variances? do we need to correct the sample size? make a continuity correction? is it tuesday?). There are [all sorts of flowcharts](https://www.google.com/search?q=statistical+test+flow+chart) you can follow to choose the right version.\n\nTechnically, the results from all those one-sample proportion tests above tell us the answer to this question:\n\n> In a hypothetical world where the difference between the sample and population proportions is 0 (or where the sample proportion is equal to the population proportion), what's the probability that this one-time collection of data fits in that world‚Äîand if the probability is low, is there enough evidence (i.e. is the probability less than 0.05?) to confidently reject that hypothetical world of no difference?\n\nThat's a mouthful and it's a weird question. With age, there's a 2.3 percentage point difference between the sample and the national proportions, with a *p*-value of 0.108. This means that there's a 10.8% chance of seeing a 2.3 percentage point difference in a world where the difference is actually 0. That's less than 5%, so we cannot confidently declare that there's not *not* a difference. Or in other words, we can't reject the possibility that we're in the hypothetical null world.\n\nAs a kind of shorthand, we then handwavily concluded that the sample and population proportions are probably about the same, but technically that's wrong. All we really concluded is that we don't have enough evidence that the hypothetical world of no difference is wrong. It could be right; it could be wrong. Who knows.\n\nWith volunteering, there's a 29.7 percentage point difference between the sample and the national proportions, with a tiny tiny *p*-value of 8.79¬†√ó¬†10^‚àí121^. This means that there's basicaly a 0% chance that we'd see that 29.7 percentage point difference in a world where the difference is actually 0. That makes it statistically significant‚Äîwe have enough evidence to safely declare that we're not in the hypothetical null world.\n\nI really really don't like this logic of null hypothesis testing. It doesn't really answer the question we want to know. Here's what we're really actually interested in:\n\n> Given this data, what's the probability that there's no difference between the sample and population proportions?\n\nWe can answer this question with Bayesian statistics and avoid all this null hypothesis stuff. And as an added bonus, we don't need to think about which flavor of which context-specific statistical test we need to use. We can instead model the data-generating process more directly and then work with the simulated posterior distribution of that process.\n\n## Modeling proportions with a binomial distribution\n\nThe actual process for generating the age column (and all the other variables in the sample) involved asking each survey respondent their age. If someone is older than 36, it's counted as a \"success\"; if they are younger than 36, it's not a success. It's a binary choice^[Kind of‚Äîtechnically they selected their actual age, but we can pretend that it was just a binary choice.] that is repeated across hundreds of other respondents (or \"trials\"). There's some underlying probability for being older than 36 that corresponds to the proportion of people that select that answer.\n\nThis data-generating process involves a bunch of independent trials (or respondents) with some probability of success (or being older than 36), which makes it a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution).\n\nFormally, it has three parameters:\n\n$$\ny \\sim \\operatorname{Binomial}(n, \\pi)\n$$\n\n1. $y$, or the number of successes (the number of people older than 36)\n2. $n$, or the number of trials (the total number of respondents)\n3. $\\pi$, or the probability of success (the probability that someone is older than 36)\n\nWe know $y$ and $n$ from our data:\n\n- $y$ = `sum(results_to_test$age)` = 660\n- $n$ = `nrow(results_to_test)` = 1300\n\nWe want to find out $\\pi$ so we can find the difference between $\\pi$ and the population proportion to see if it's 0 or not.\n\nWe can estimate $\\pi$ with a Bayesian beta-binomial regression model using {brms}. We'll use a vague Beta(1, 1) prior, which is 50% with a wide range‚Äî[see here for more about Beta distributions and priors](https://www.andrewheiss.com/blog/2023/05/15/fancy-bayes-diffs-props/#formal-model). Note that this is an intercept-only model with no other covariates. That's because we just want to know the underlying proportion of age‚Äîwe're not conditioning that proportion on anything else.\n\n$$\n\\begin{aligned}\ny_{(n \\text{ age > 36})} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total respondents}}, \\pi_{\\text{age > 36}}) \\\\\n\\pi_{\\text{age > 36}} =&\\ \\beta_0 \\\\[10pt]\n\\beta_0 =&\\ \\operatorname{Beta}(1, 1)\n\\end{aligned}\n$$\n\n{brms} likes working with data frames, so we'll put our $y$ and $n$ into a little one-row dataset and then use `brm()` to fit a model:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nage_binomial_df <- tibble(\n  n_yes = sum(results_to_test$age),\n  n_total = nrow(results_to_test)\n)\n\n# lol tiny data\nage_binomial_df\n## # A tibble: 1 √ó 2\n##   n_yes n_total\n##   <int>   <int>\n## 1   660    1300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_age_binomial <- brm(\n  bf(n_yes | trials(n_total) ~ 1),\n  data = age_binomial_df,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(1, 1), class = \"Intercept\", lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0,\n  file = \"models/model_age_binomial\"\n)\n```\n:::\n\n\n\n## Working with the posterior\n\nSince this is just a regression model, it behaves like any normal {brms} model. The coefficient for the intercept represents the estimated proportion of people older than 36 in the sample:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(model_age_binomial)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n_yes | trials(n_total) ~ 1 \n##    Data: age_binomial_df (Number of observations: 1) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.51      0.01     0.48     0.54 1.00     1537     1828\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\n\nSince we're in The Land of Bayes, we can work with the full posterior and calculate estimands directly, like the posterior difference between the sample proportion and the national proportion:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nage_draws <- model_age_binomial |> \n  spread_draws(b_Intercept) |> \n  mutate(diff = b_Intercept - national_values$age)\n```\n:::\n\n\n\nAnd we can plot these estimands:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\np1 <- ggplot(age_draws, aes(x = b_Intercept, y = \"Age (% 36+)\")) + \n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = national_values$age, color = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Proportion older than 36\", y = NULL)\n\np2 <- ggplot(age_draws, aes(x = diff, y = \"Age (% 36+)\")) + \n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  scale_x_continuous(labels = label_pp) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Difference from CPS\", y = NULL)\n\np1 / p2\n```\n\n::: {.cell-output-display}\n![Posterior proportion of survey respondents older than 36 compared to the national proportion, and posterior differences between sample and national proportions (like @fig-age-sample-prop-freq, but Bayesian)](index_files/figure-html/fig-age-posterior-only-1.png){#fig-age-posterior-only fig-align='center' width=90%}\n:::\n:::\n\n\n\nThe population value is in the sample posterior, which means that the posterior difference between the sample and population includes 0, but that doesn't tell us much about how likely that is. We could calculate the probability that the difference isn't equal to 0, but that's relatively useless‚Äîzero is a single point, and the probability that the posterior is different from that one point is infinite.\n\n## The region of practical equivalence (ROPE)\n\nAnother approach is to think of a range of values around 0 that all have \"practically no effect.\" I like to think of this as a sort of \"dead zone.\" If the difference between the sample and the population is 0, we can safely conclude that there's no difference between the two. If the measured difference were 0.3 percentage points, or ‚àí0.6 percentage points, or even 2 percentage points, I'd still feel pretty confident that that's basically the same. Bayesians call this the [region of practical equivalence](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html), or ROPE [@Kruschke:2010a; @Kruschke:2015; @KruschkeAguinisJoo:2012; @KruschkeLiddell:2018].\n\nThere are lots of ways to define this ROPE or dead zone. You can base it on experience with the phenomenon, or you can base it on data that you have. @KruschkeLiddell:2018 suggest looking at a tenth of the variable's standard deviation above and below the main null value, or\n\n$$\n[-0.1 \\times SD_y, 0.1 \\times SD_y]\n$$\n\nTo illustrate, let's find the ROPE for the proportion of people older than 36 with ¬±0.1 √ó standard deviation:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nage_rope <- tibble(\n  avg_age = national_values$age,\n  sd_age = national_values$age_sd\n) |> \n  mutate(\n    rope_low = -0.1 * sd_age,\n    rope_avg_low = avg_age + rope_low,\n    rope_high = 0.1 * sd_age,\n    rope_avg_high = avg_age + rope_high\n  )\nage_rope\n## # A tibble: 1 √ó 6\n##   avg_age sd_age rope_low rope_avg_low rope_high rope_avg_high\n##     <dbl>  <dbl>    <dbl>        <dbl>     <dbl>         <dbl>\n## 1   0.530  0.499  -0.0499        0.480    0.0499         0.580\n```\n:::\n\n\n\nFollowing this rule, we shouldn't care about sample/population differences between ¬±4.99 percentage points. For all intents and purposes, any differences in that range‚Äîor any sample proportions between 53% ¬± 4.99 (or 48%‚Äì58%)‚Äîare equivalent.\n\nHere's what the ROPE for age looks like, both for the full proportion and for the difference:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\np1 <- ggplot(age_draws, aes(x = b_Intercept, y = \"Age (% 36+)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = age_rope$rope_avg_low, \n    xmax = age_rope$rope_avg_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = age_rope$avg_age, color = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Proportion older than 36\", y = NULL)\n\np2 <- ggplot(age_draws, aes(x = diff, y = \"Age (% 36+)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = age_rope$rope_low, \n    xmax = age_rope$rope_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  scale_x_continuous(labels = label_pp) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Difference from CPS\", y = NULL)\n\np1 / p2\n```\n\n::: {.cell-output-display}\n![Posterior proportion of survey respondents older than 36 compared to the national proportion, and posterior differences between sample and national proportions, overlaid on a region of practical equivalence (ROPE)](index_files/figure-html/fig-age-rope-1.png){#fig-age-rope fig-align='center' width=90%}\n:::\n:::\n\n\n\nA huge chunk of that posterior distribution is inside the ROPE dead zone. We can calculate the exact proportion:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprop_in_rope <- age_draws |> \n  mutate(\n    is_below_rope = diff < age_rope$rope_low,\n    is_above_rope = diff > age_rope$rope_high\n  ) |> \n  summarize(is_inside_rope = 1 - mean(is_below_rope | is_above_rope))\nprop_in_rope$is_inside_rope\n## [1] 0.9762\n```\n:::\n\n\n\n97.6% of the posterior is inside the ROPE, which means that there's a 97.6% probability that the sample matches the population. \n\nThat's *way* cooler and *way* more interpretable than null hypothesis testing.\n\n<hr class=\"dinkus\">\n\n::: {.callout-tip collapse=\"true\"}\n### BONUS: Faster, more automatic ROPE calculations\n\nWe just went through a lot of work ‚Üë up there to calculate the bounds of the ROPE and then find how much of the posterior is in it.\n\nThere's a faster and easier way! The {bayestestR} package has a `rope()` function that can do it automatically.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_age_binomial |> \n  bayestestR::rope(ci = 1)\n## # Proportion of samples inside the ROPE [-45.25, 45.25]:\n## \n## Parameter | inside ROPE\n## -----------------------\n## Intercept |    100.00 %\n```\n:::\n\n\n\nIt doesn't give exactly the same result as before because the automatic ROPE limits (`range` here) aren't based on the weighted standard deviation, but we can define that range ourselves:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_age_binomial |> \n  bayestestR::rope(\n    ci = 1, \n    range = c(age_rope$rope_avg_low, age_rope$rope_avg_high)\n  )\n## # Proportion of samples inside the ROPE [0.48, 0.58]:\n## \n## Parameter | inside ROPE\n## -----------------------\n## Intercept |     97.62 %\n```\n:::\n\n\n\n`bayestestR::rope()` is incorporated in the {parameters} package too, so you can do this:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_age_binomial |> \n  parameters::model_parameters(\n    test = \"rope\",\n    rope_ci = 1, \n    rope_range = c(age_rope$rope_avg_low, age_rope$rope_avg_high),\n    verbose = FALSE\n  )\n## # Fixed Effects\n## \n## Parameter   | Median |       95% CI | % in ROPE |  Rhat |     ESS\n## -----------------------------------------------------------------\n## (Intercept) |   0.51 | [0.48, 0.54] |    97.62% | 1.001 | 1528.00\n```\n:::\n\n\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### BONUS: How much of the posterior should we count?\n\n[There are big debates in the ROPE world](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html#credible-interval-in-rope-vs-full-posterior-in-rope) about how much of the posterior we should look at when working with ROPEs. We just looked at the 100% of the sample posterior. Some people [@Kruschke:2015; @McElreath:2020] say to look instead at how much of the 95% (or 89%) highest density interval (HDI) falls within the ROPE.\n\nI have no strong preferences either way, and it's fairly straightforward to calculate the proportion of the HDI in the ROPE. Here's an example with a Richard McElreath-style 89% HDI:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Find the 89% HDI\nage_hdi <- age_draws |> \n  median_hdi(diff, .width = 0.89)\nage_hdi\n## # A tibble: 1 √ó 6\n##      diff  .lower   .upper .width .point .interval\n##     <dbl>   <dbl>    <dbl>  <dbl> <chr>  <chr>    \n## 1 -0.0227 -0.0450 -0.00108   0.89 median hdi\n\n# Find how much of the the age HDI is in the ROPE\nage_draws |> \n  filter(diff >= age_hdi$.lower & diff <= age_hdi$.upper) |> \n  mutate(\n    is_below_rope = diff < age_rope$rope_low,\n    is_above_rope = diff > age_rope$rope_high\n  ) |> \n  summarize(is_inside_rope = 1 - mean(is_below_rope | is_above_rope))\n## # A tibble: 1 √ó 1\n##   is_inside_rope\n##            <dbl>\n## 1              1\n```\n:::\n\n\n\nOr with the more automatic `bayestestR::rope()`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_age_binomial |> \n  bayestestR::rope(\n    ci = 0.89, \n    range = c(age_rope$rope_avg_low, age_rope$rope_avg_high)\n  )\n## # Proportion of samples inside the ROPE [0.48, 0.58]:\n## \n## Parameter | inside ROPE\n## -----------------------\n## Intercept |    100.00 %\n```\n:::\n\n\n\nIn this case, 100% of the HDI is in the ROPE, so we can say that the age proportion is equivalent to the population proportion.\n:::\n\n## Bayesian proportion test for volunteering\n\nLet's look at volunteer status by itself next, to keep things parallel with [the frequentist section earlier](#one-sample-proportion-test-for-volunteering).\n\nWe'll make a little one-row dataset:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvol_binomial_df <- tibble(\n  n_yes = sum(results_to_test$volunteering),\n  n_total = nrow(results_to_test)\n)\nvol_binomial_df\n## # A tibble: 1 √ó 2\n##   n_yes n_total\n##   <int>   <int>\n## 1   777    1300\n```\n:::\n\n\n\nThen we'll use it to model the proportion/probability of volunteering:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_vol_binomial <- brm(\n  bf(n_yes | trials(n_total) ~ 1),\n  data = vol_binomial_df,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(1, 1), class = \"Intercept\", lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0,\n  file = \"models/model_vol_binomial\"\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvol_draws <- model_vol_binomial |> \n  spread_draws(b_Intercept) |> \n  mutate(diff = b_Intercept - national_values$volunteering)\n```\n:::\n\n\n\nWe can calculate the ROPE for the proportion volunteering, or ¬±0.1 √ó SD~volunteering~\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvol_rope <- tibble(\n  avg_vol = national_values$volunteering,\n  sd_vol = national_values$volunteering_sd\n) |> \n  mutate(\n    rope_low = -0.1 * sd_vol,\n    rope_avg_low = avg_vol + rope_low,\n    rope_high = 0.1 * sd_vol,\n    rope_avg_high = avg_vol + rope_high\n  )\nvol_rope\n## # A tibble: 1 √ó 6\n##   avg_vol sd_vol rope_low rope_avg_low rope_high rope_avg_high\n##     <dbl>  <dbl>    <dbl>        <dbl>     <dbl>         <dbl>\n## 1   0.300  0.458  -0.0458        0.254    0.0458         0.346\n```\n:::\n\n\n\nAnd finally we can visualize things and calculate how much of the posterior fits inside the ROPE:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\np1 <- ggplot(vol_draws, aes(x = b_Intercept, y = \"Volunteered in past year (%)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = vol_rope$rope_avg_low, \n    xmax = vol_rope$rope_avg_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[8]) +\n  geom_vline(xintercept = vol_rope$avg_vol, color = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Proportion\", y = NULL)\n\np2 <- ggplot(vol_draws, aes(x = diff, y = \"Volunteered in past year (%)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = vol_rope$rope_low, \n    xmax = vol_rope$rope_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[8]) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  scale_x_continuous(labels = label_pp) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Difference from CPS\", y = NULL)\n\np1 / p2\n```\n\n::: {.cell-output-display}\n![Posterior proportion of survey respondents that volunteer to the national proportion, and posterior differences between sample and national proportions, overlaid on a region of practical equivalence (ROPE); like @fig-age-sample-prop-vol, but Bayesian](index_files/figure-html/fig-vol-rope-1.png){#fig-vol-rope fig-align='center' width=90%}\n:::\n:::\n\n\n\nAs we saw with the frequentist version of this analysis, the proportion of people who volunteer is *substantially* different from the CPS‚Äî‚âà30 percentage points higher! The probability that the sample is equivalent to the CPS population is 0:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvol_draws |> \n  mutate(\n    is_below_rope = b_Intercept < vol_rope$rope_avg_low,\n    is_above_rope = b_Intercept > vol_rope$rope_avg_high\n  ) |> \n  summarize(is_inside_rope = 1 - mean(is_below_rope | is_above_rope))\n## # A tibble: 1 √ó 1\n##   is_inside_rope\n##            <dbl>\n## 1              0\n```\n:::\n\n\n\n## Posterior proportions, differences, and ROPEs for everything all at once\n\n[Like we did before with the frequentist approach](#proportion-tests-and-differences-for-everything-all-at-once), we'll make a little wrapper function for this process. It's a little more complex, and we'll make it return a list of lots of things: the original {brms} model, a long data frame of all the MCMC draws, a data frame with the ROPE bounds, and the probability that the full posterior is in the ROPE.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"`prop_test_bayes()` wrapper function\"}\n#' Perform a Bayesian proportion test using a beta-binomial model\n#'\n#' @param short_name A character string representing a short name for the model file.\n#' @param sample_column A numeric vector representing the sample data (0/1 or logical).\n#' @param cps_prop A numeric value representing the population proportion to compare against.\n#' @param cps_sd A numeric value representing the standard deviation of the population proportion.\n#' @return A list containing the model, MCMC draws, ROPE details, and the proportion of the posterior\n#'         inside the ROPE.\n#' @examples\n#' sample_data <- c(1, 0, 1, 1, 0, 1, 0, 1, 1, 0)\n#' cps_prop <- 0.5\n#' cps_sd <- 0.1\n#' prop_test_bayes(\"example\", sample_data, cps_prop, cps_sd)\nprop_test_bayes <- function(short_name, sample_column, cps_prop, cps_sd) {\n  # Little one-row data frame\n  df <- tibble(\n    n_yes = sum(sample_column),\n    n_total = length(sample_column)\n  )\n\n  # Intercept-only beta-binomial model\n  model <- brm(\n    bf(n_yes | trials(n_total) ~ 1),\n    data = df,\n    family = binomial(link = \"identity\"),\n    prior = c(prior(beta(1, 1), class = \"Intercept\", lb = 0, ub = 1)),\n    chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n    refresh = 0,\n    file = glue(\"models/model_{short_name}_binomial\")\n  )\n\n  # ROPE details\n  rope <- tibble(\n    cps_avg = cps_prop,\n    cps_sd = cps_sd\n  ) |> \n    mutate(\n      rope_low = -0.1 * cps_sd,\n      rope_avg_low = cps_avg + rope_low,\n      rope_high = 0.1 * cps_sd,\n      rope_avg_high = cps_avg + rope_high\n    )\n\n  # MCMC draws of intercept and difference from population \n  draws <- model |> \n    spread_draws(b_Intercept) |> \n    mutate(diff = b_Intercept - cps_prop)\n\n  # Proportion of posterior inside the ROPE\n  prop_in_rope <- draws |> \n    mutate(\n      is_below_rope_full = b_Intercept < rope$rope_avg_low,\n      is_above_rope_full = b_Intercept > rope$rope_avg_high\n    ) |> \n    summarize(is_inside_rope_full = 1 - mean(is_below_rope_full | is_above_rope_full))\n\n  return(lst(model, draws, rope, prop_in_rope))\n}\n```\n:::\n\n\n\nAnd as before, we'll make a little data frame with each of the variables we want to compare.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Create `sample_cps_props_bayes` and use `prop_test_bayes()` on each row\"}\nsample_cps_props_bayes <- tribble(\n  ~category, ~short_name, ~variable, ~sample_value, ~national_value, ~national_sd,\n  \"Demographics\", \"age\", \"Age (% 36+)\", results_to_test$age, national_values$age, national_values$age_sd,\n  \"Demographics\", \"female\", \"Female (%)\", results_to_test$female, national_values$female, national_values$female_sd,\n  \"Demographics\", \"married\", \"Married (%)\", results_to_test$married, national_values$married, national_values$married_sd,\n  \"Demographics\", \"college\", \"Education (% BA+)\", results_to_test$college, national_values$college, national_values$college_sd,\n  \"Philanthropy\", \"donating\", \"Donated in past year (%)\", results_to_test$donating, national_values$donating, national_values$donating_sd,\n  \"Philanthropy\", \"volunteering\", \"Volunteered in past year (%)\", results_to_test$volunteering, national_values$volunteering, national_values$volunteering_sd,\n  \"Voting\", \"voting\", \"Voted in last November election (%)\", results_to_test$voting, national_values$voting, national_values$voting_sd\n) |>\n  mutate(prop_test_results = pmap(\n    list(short_name, sample_value, national_value, national_sd),\n    \\(short_name, sample_value, national_value, national_sd) prop_test_bayes(short_name, sample_value, national_value, national_sd)\n  ))\n```\n:::\n\n\n\nWe'll then extract the MCMC draws and ROPE details from the little data frame so that we can make some plots and tables.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Extract draws and ROPE from `sample_cps_props_bayes` results\"}\nrope_details <- sample_cps_props_bayes |>\n  mutate(prop_in_rope = map(prop_test_results, \\(x) x$prop_in_rope)) |>\n  mutate(rope = map(prop_test_results, \\(x) x$rope)) |>\n  mutate(sample_median = map_dbl(prop_test_results, \\(x) median(x$draws$b_Intercept))) |>\n  unnest_wider(c(rope, prop_in_rope)) |>\n  mutate(\n    variable = fct_rev(fct_inorder(variable)),\n    category = fct_inorder(category)\n  ) |>\n  mutate(p_rope = case_when(\n    row_number() == 1 ~ glue(\"p(Sample = CPS) = {x}\", x = label_percent(accuracy = 0.1)(is_inside_rope_full)),\n    TRUE ~ glue(\"{x}\", x = label_percent(accuracy = 0.1)(is_inside_rope_full))\n  )) |> \n  group_by(category) |>\n  mutate(variable_numeric = as.numeric(fct_drop(variable)))\n\nsample_cps_props_draws <- sample_cps_props_bayes |>\n  mutate(draws = map(prop_test_results, \\(x) x$draws)) |>\n  unnest(draws) |>\n  mutate(\n    variable = fct_rev(fct_inorder(variable)),\n    category = fct_inorder(category)\n  )\n```\n:::\n\n\n\nAnd finally, we can make a plot of all these fancy results!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this plot ‚Üì\"}\nsample_cps_props_draws |>\n  ggplot(aes(x = b_Intercept, y = variable)) +\n  geom_rect(\n    data = rope_details,\n    aes(\n      xmin = rope_avg_low, xmax = rope_avg_high,\n      ymin = variable_numeric,\n      ymax = variable_numeric + 0.9,\n      fill = \"ROPE (¬± 0.1 SD)\"\n    ),\n    alpha = 0.2, inherit.aes = FALSE\n  ) +\n  stat_halfeye(aes(slab_fill = category)) +\n  geom_segment(\n    data = rope_details,\n    aes(\n      x = national_value, xend = national_value,\n      y = variable_numeric, yend = variable_numeric + 0.9,\n      color = \"CPS proportion\"\n    ),\n    key_glyph = \"vline\"\n  ) +\n  geom_label(\n    data = rope_details, \n    aes(x = 0.9, y = variable, label = p_rope), \n    inherit.aes = FALSE,\n    hjust = 1, vjust = -0.1, size = 8, size.unit = \"pt\", label.size = 0, fill = \"grey95\"\n  ) +\n  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = clrs[c(5, 8, 11)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"Proportion\", y = NULL, color = NULL, fill = NULL) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  theme(\n    legend.key.width = unit(2, \"lines\"),\n    legend.key.height = unit(0.9, \"lines\"),\n    axis.text.y = element_text(vjust = 0)\n  )\n```\n\n::: {.cell-output-display}\n![Respondent demographic proportions compared to national proportions, overlaid on variable-specific ROPEs; like @fig-prop-test-freq-props, but Bayesian](index_files/figure-html/fig-prop-test-bayes-props-1.png){#fig-prop-test-bayes-props fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this plot ‚Üì\"}\nsample_cps_props_draws |>\n  ggplot(aes(x = diff, y = variable)) +\n  geom_rect(\n    data = rope_details,\n    aes(\n      xmin = rope_low, xmax = rope_high,\n      ymin = variable_numeric,\n      ymax = variable_numeric + 0.9,\n      fill = \"ROPE (¬± 0.1 SD)\"\n    ),\n    alpha = 0.2, inherit.aes = FALSE\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"CPS proportion\")) +\n  stat_halfeye(aes(slab_fill = category)) +\n  geom_label(\n    data = rope_details, \n    aes(x = 0.4, y = variable, label = p_rope), \n    inherit.aes = FALSE,\n    hjust = 1, vjust = -0.1, size = 8, size.unit = \"pt\", label.size = 0, fill = \"grey95\"\n  ) +\n  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = clrs[c(5, 8, 11)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"Difference from CPS\", y = NULL, color = NULL, fill = NULL) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  theme(\n    legend.key.width = unit(2, \"lines\"),\n    legend.key.height = unit(0.9, \"lines\"),\n    axis.text.y = element_text(vjust = 0)\n  )\n```\n\n::: {.cell-output-display}\n![Differences between demographic proportions and national proportions, overlaid on variable-specific ROPEs; like @fig-prop-test-freq-diffs, but Bayesian](index_files/figure-html/fig-prop-test-bayes-diffs-1.png){#fig-prop-test-bayes-diffs fig-align='center' width=90%}\n:::\n:::\n\n\n\nAnd as before, we can make pretty tables with [{tinytable}](https://vincentarelbundock.github.io/tinytable/):\n\n\n\n::: {#tbl-prop-test-bayes .cell .column-page-inset-right layout-align=\"center\" tbl-cap='Sample characteristics compared to nationally representative Current Population Survey (CPS) estimates, with probability that the sample is equivalent to the national proportion; like @tbl-prop-test-freq, but Bayesian'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for making this table ‚Üì\"}\nnotes <- list(\n  \"‚Ä†\" = list(i = 0, j = 5, text = \"Proportion of the complete sample posterior that falls outside of the region of practical equivalence (ROPE) around the national proportion, or [‚àí0.1 √ó SD, 0.1 √ó SD]. This essentially respresents the probabilty that the sample posterior is equivalent to the national proportion.\"),\n  \"a\" = list(i = 1:4, j = 1, text = \"Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019\"),\n  \"b\" = list(i = 5:6, j = 1, text = \"Monthly CPS, September 2019\"),\n  \"c\" = list(i = 7, j = 1, text = \"Monthly CPS, November 2018\")\n)\n\ntbl_bayes <- sample_cps_props_bayes |>\n  mutate(prop_in_rope = map(prop_test_results, \\(x) x$prop_in_rope)) |>\n  mutate(sample_median_ci = map(prop_test_results, \\(x) x$draws |> median_qi(b_Intercept, diff))) |> \n  unnest_wider(c(prop_in_rope, sample_median_ci))\n\ntbl_bayes |> \n  mutate(sample_nice = glue(\n    \"{value}<br>[{lower}, {upper}]\",\n    value = label_percent(accuracy = 0.1)(b_Intercept),\n    lower = label_percent(accuracy = 0.1)(b_Intercept.lower),\n    upper = label_percent(accuracy = 0.1)(b_Intercept.upper)\n  )) |> \n  mutate(diff_nice = glue::glue(\n    \"{diff}<br>[{lower}, {upper}]\",\n    diff = label_pp_01(diff),\n    lower = label_number(accuracy = 0.1, scale = 100)(diff.lower),\n    upper = label_number(accuracy = 0.1, scale = 100)(diff.upper)\n  )) |> \n  mutate(p_in_rope = label_percent(accuracy = 0.1, scale = 100)(is_inside_rope_full)) |> \n  select(\n    Variable = variable,\n    National = national_value,\n    Sample = sample_nice,\n    `‚àÜ` = diff_nice,\n    `p(Sample = CPS)` = p_in_rope\n  ) |> \n  tt(width = c(0.3, 0.1, 0.2, 0.2, 0.2), notes = notes) |> \n  group_tt(i = tbl_bayes$category) |> \n  format_tt(j = 2, fn = label_percent(accuracy = 0.1)) |> \n  style_tt(i = c(1, 6, 9), bold = TRUE, background = \"#e6e6e6\") |> \n  style_tt(\n    bootstrap_class = \"table\",\n    bootstrap_css_rule = \".table tfoot { text-align: left; } .table { font-size: 0.85rem; }\"\n  ) |> \n  style_tt(j = 1, align = \"l\") |> \n  style_tt(j = 2:5, align = \"c\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_ms43054bhimhmv9c76po(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_ms43054bhimhmv9c76po\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow_o8mhpj00ubn4atozcgij(i, colspan, content) {\n        var table = document.getElementById('tinytable_ms43054bhimhmv9c76po');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_ms43054bhimhmv9c76po(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_ms43054bhimhmv9c76po\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\nwindow.addEventListener('load', function () { insertSpanRow_o8mhpj00ubn4atozcgij(7, 5, 'Voting') });\nwindow.addEventListener('load', function () { insertSpanRow_o8mhpj00ubn4atozcgij(5, 5, 'Philanthropy') });\nwindow.addEventListener('load', function () { insertSpanRow_o8mhpj00ubn4atozcgij(1, 5, 'Demographics') });\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 10, j: 0 },  ], css_id: 'tinytable_css_yovxku58v0bjx6jc0qkz',}, \n          { positions: [ { i: 0, j: 2 }, { i: 0, j: 4 }, { i: 0, j: 3 },  ], css_id: 'tinytable_css_n64iujdx2r8u733mg4hl',}, \n          { positions: [ { i: 10, j: 1 },  ], css_id: 'tinytable_css_mxxq8s459gf8eeekadyl',}, \n          { positions: [ { i: 3, j: 2 }, { i: 8, j: 2 }, { i: 2, j: 3 }, { i: 2, j: 2 }, { i: 5, j: 3 }, { i: 4, j: 2 }, { i: 3, j: 3 }, { i: 8, j: 3 }, { i: 7, j: 2 }, { i: 3, j: 4 }, { i: 5, j: 2 }, { i: 5, j: 4 }, { i: 2, j: 4 }, { i: 7, j: 4 }, { i: 4, j: 4 }, { i: 7, j: 3 }, { i: 4, j: 3 }, { i: 8, j: 4 },  ], css_id: 'tinytable_css_l0lsc21ndng4xj5gdaem',}, \n          { positions: [ { i: 6, j: 4 }, { i: 6, j: 3 }, { i: 1, j: 2 }, { i: 6, j: 2 }, { i: 9, j: 3 }, { i: 1, j: 3 }, { i: 9, j: 4 }, { i: 1, j: 4 }, { i: 9, j: 2 },  ], css_id: 'tinytable_css_kwvl4def6mbofzyd7wip',}, \n          { positions: [ { i: 10, j: 3 }, { i: 10, j: 2 }, { i: 10, j: 4 },  ], css_id: 'tinytable_css_c2bvxsmutc4dpjn73sd8',}, \n          { positions: [ { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 2, j: 0 }, { i: 7, j: 0 }, { i: 8, j: 0 }, { i: 5, j: 0 },  ], css_id: 'tinytable_css_as197ai34n64j3k85z8j',}, \n          { positions: [ { i: 3, j: 1 }, { i: 4, j: 1 }, { i: 2, j: 1 }, { i: 7, j: 1 }, { i: 8, j: 1 }, { i: 5, j: 1 },  ], css_id: 'tinytable_css_57e6t6absi9ldka4hb7f',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_4qogn9jo72vqe52excn0',}, \n          { positions: [ { i: 1, j: 0 }, { i: 6, j: 0 }, { i: 9, j: 0 },  ], css_id: 'tinytable_css_1ijbhgq20zatxfa3paf3',}, \n          { positions: [ { i: 1, j: 1 }, { i: 6, j: 1 }, { i: 9, j: 1 },  ], css_id: 'tinytable_css_1goilph8rur79xfdjw04',}, \n          { positions: [ { i: 0, j: 1 },  ], css_id: 'tinytable_css_03v7ob43ggstz9l3t130',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_ms43054bhimhmv9c76po(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_yovxku58v0bjx6jc0qkz, .table th.tinytable_css_yovxku58v0bjx6jc0qkz { padding-left: 1em; text-align: left; border-bottom: solid #d3d8dc 0.1em; width: 30%; }\n      .table td.tinytable_css_n64iujdx2r8u733mg4hl, .table th.tinytable_css_n64iujdx2r8u733mg4hl { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; width: 20%; }\n      .table td.tinytable_css_mxxq8s459gf8eeekadyl, .table th.tinytable_css_mxxq8s459gf8eeekadyl { text-align: center; border-bottom: solid #d3d8dc 0.1em; width: 10%; }\n      .table td.tinytable_css_l0lsc21ndng4xj5gdaem, .table th.tinytable_css_l0lsc21ndng4xj5gdaem { text-align: center; width: 20%; }\n      .table td.tinytable_css_kwvl4def6mbofzyd7wip, .table th.tinytable_css_kwvl4def6mbofzyd7wip { font-weight: bold; background-color: #e6e6e6; text-align: center; width: 20%; }\n      .table td.tinytable_css_c2bvxsmutc4dpjn73sd8, .table th.tinytable_css_c2bvxsmutc4dpjn73sd8 { text-align: center; border-bottom: solid #d3d8dc 0.1em; width: 20%; }\n      .table td.tinytable_css_as197ai34n64j3k85z8j, .table th.tinytable_css_as197ai34n64j3k85z8j { padding-left: 1em; text-align: left; width: 30%; }\n      .table td.tinytable_css_57e6t6absi9ldka4hb7f, .table th.tinytable_css_57e6t6absi9ldka4hb7f { text-align: center; width: 10%; }\n      .table td.tinytable_css_4qogn9jo72vqe52excn0, .table th.tinytable_css_4qogn9jo72vqe52excn0 { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; width: 30%; }\n      .table td.tinytable_css_1ijbhgq20zatxfa3paf3, .table th.tinytable_css_1ijbhgq20zatxfa3paf3 { font-weight: bold; background-color: #e6e6e6; text-align: left; width: 30%; }\n      .table td.tinytable_css_1goilph8rur79xfdjw04, .table th.tinytable_css_1goilph8rur79xfdjw04 { font-weight: bold; background-color: #e6e6e6; text-align: center; width: 10%; }\n      .table td.tinytable_css_03v7ob43ggstz9l3t130, .table th.tinytable_css_03v7ob43ggstz9l3t130 { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; width: 10%; }\n.table tfoot { text-align: left; } .table { font-size: 0.85rem; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table\" id=\"tinytable_ms43054bhimhmv9c76po\" style=\"table-layout: fixed; width: 100% !important; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">Variable</th>\n                <th scope=\"col\">National</th>\n                <th scope=\"col\">Sample</th>\n                <th scope=\"col\">‚àÜ</th>\n                <th scope=\"col\">p(Sample = CPS)<sup>‚Ä†</sup></th>\n              </tr>\n        </thead>\n        <tfoot><tr><td colspan='5'><sup>‚Ä†</sup> Proportion of the complete sample posterior that falls outside of the region of practical equivalence (ROPE) around the national proportion, or [‚àí0.1 √ó SD, 0.1 √ó SD]. This essentially respresents the probabilty that the sample posterior is equivalent to the national proportion.</td></tr>\n<tr><td colspan='5'><sup>a</sup> Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019</td></tr>\n<tr><td colspan='5'><sup>b</sup> Monthly CPS, September 2019</td></tr>\n<tr><td colspan='5'><sup>c</sup> Monthly CPS, November 2018</td></tr></tfoot>\n        <tbody>\n                <tr>\n                  <td>Age (% 36+)<sup>a</sup></td>\n                  <td>53.0%</td>\n                  <td>50.8%<br>[48.1%, 53.5%]</td>\n                  <td>‚àí2.3 pp.<br>[-5.0, 0.5]</td>\n                  <td>97.6%</td>\n                </tr>\n                <tr>\n                  <td>Female (%)<sup>a</sup></td>\n                  <td>51.0%</td>\n                  <td>47.2%<br>[44.4%, 49.9%]</td>\n                  <td>‚àí3.8 pp.<br>[-6.5, -1.1]</td>\n                  <td>80.9%</td>\n                </tr>\n                <tr>\n                  <td>Married (%)<sup>a</sup></td>\n                  <td>41.0%</td>\n                  <td>40.6%<br>[37.9%, 43.3%]</td>\n                  <td>‚àí0.4 pp.<br>[-3.1, 2.3]</td>\n                  <td>99.9%</td>\n                </tr>\n                <tr>\n                  <td>Education (% BA+)<sup>a</sup></td>\n                  <td>25.8%</td>\n                  <td>28.6%<br>[26.0%, 31.1%]</td>\n                  <td>2.8 pp.<br>[0.3, 5.3]</td>\n                  <td>89.8%</td>\n                </tr>\n                <tr>\n                  <td>Donated in past year (%)<sup>b</sup></td>\n                  <td>47.4%</td>\n                  <td>56.9%<br>[54.3%, 59.7%]</td>\n                  <td>9.5 pp.<br>[6.9, 12.3]</td>\n                  <td>0.1%</td>\n                </tr>\n                <tr>\n                  <td>Volunteered in past year (%)<sup>b</sup></td>\n                  <td>30.0%</td>\n                  <td>59.7%<br>[57.1%, 62.4%]</td>\n                  <td>29.7 pp.<br>[27.1, 32.3]</td>\n                  <td>0.0%</td>\n                </tr>\n                <tr>\n                  <td>Voted in last November election (%)<sup>c</sup></td>\n                  <td>53.4%</td>\n                  <td>71.5%<br>[68.9%, 73.9%]</td>\n                  <td>18.0 pp.<br>[15.5, 20.5]</td>\n                  <td>0.0%</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}